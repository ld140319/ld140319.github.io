<!DOCTYPE html>



  


<html class="theme-next muse use-motion" lang="zh-Hans">
<head><meta name="generator" content="Hexo 3.8.0">
  <meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform">
<meta http-equiv="Cache-Control" content="no-siteapp">
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css">







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css">

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="Hexo, NexT">





  <link rel="alternate" href="/atom.xml" title="搬运工 + 践行者" type="application/atom+xml">






<meta name="description" content="做一个懂业务的程序员">
<meta property="og:type" content="website">
<meta property="og:title" content="搬运工 + 践行者">
<meta property="og:url" content="http://blog.com/page/25/index.html">
<meta property="og:site_name" content="搬运工 + 践行者">
<meta property="og:description" content="做一个懂业务的程序员">
<meta property="og:locale" content="zh-Hans">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="搬运工 + 践行者">
<meta name="twitter:description" content="做一个懂业务的程序员">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Muse',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://blog.com/page/25/">





  <title>搬运工 + 践行者</title>
  








</head>

<body itemscope="" itemtype="http://schema.org/WebPage" lang="zh-Hans">

  
  
    
  

  <div class="container sidebar-position-left 
  page-home">
    <div class="headband"></div>

    <header id="header" class="header" itemscope="" itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">搬运工 + 践行者</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle">记录学习的技能和遇到的问题</p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br>
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br>
            
            标签
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br>
            
            分类
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br>
            
            归档
          </a>
        </li>
      

      
        <li class="menu-item menu-item-search">
          
            <a href="javascript:;" class="popup-trigger">
          
            
              <i class="menu-item-icon fa fa-search fa-fw"></i> <br>
            
            搜索
          </a>
        </li>
      
    </ul>
  

  
    <div class="site-search">
      
  <div class="popup search-popup local-search-popup">
  <div class="local-search-header clearfix">
    <span class="search-icon">
      <i class="fa fa-search"></i>
    </span>
    <span class="popup-btn-close">
      <i class="fa fa-times-circle"></i>
    </span>
    <div class="local-search-input-wrapper">
      <input autocomplete="off" placeholder="搜索..." spellcheck="false" type="text" id="local-search-input">
    </div>
  </div>
  <div id="local-search-result"></div>
</div>



    </div>
  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            
  <section id="posts" class="posts-expand">
    
      

  

  
  
  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://blog.com/2019/07/18/消息中间件选型分析/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="刘泽明">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="搬运工 + 践行者">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/07/18/消息中间件选型分析/" itemprop="url">消息中间件选型分析</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-07-18T12:12:57+08:00">
                2019-07-18
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing">
                  <a href="/categories/架构/" itemprop="url" rel="index">
                    <span itemprop="name">架构</span>
                  </a>
                </span>

                
                
                  ， 
                
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing">
                  <a href="/categories/架构/消息队列/" itemprop="url" rel="index">
                    <span itemprop="name">消息队列</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="消息中间件选型分析"><a href="#消息中间件选型分析" class="headerlink" title="消息中间件选型分析"></a>消息中间件选型分析</h1><p><br></p>
<blockquote>
<p>原文地址：<a href="https://mp.weixin.qq.com/s/ad7jibTb5nTzh3nDQYKFeg" target="_blank" rel="noopener">https://mp.weixin.qq.com/s/ad7jibTb5nTzh3nDQYKFeg</a></p>
</blockquote>
<p><br><br><img src="//blog.com/2019/07/18/消息中间件选型分析/1.jpg" alt="img"></p>
<p>​    ——从Kafka与RabbitMQ的对比来看全局</p>
<blockquote>
<p>有很多网友留言：公司要做消息中间件选型，该如何选？你觉得哪个比较好？消息选型的确是一个大论题，实则说来话长的事情又如何长话短说。对此笔者专门撰稿一篇内功心法：<a href="http://mp.weixin.qq.com/s?__biz=MzU0MzQ5MDA0Mw==&amp;mid=2247483866&amp;idx=1&amp;sn=905732d14b1271b2604dddfbec02481c&amp;chksm=fb0beb4ecc7c62585cd5e223c271c45f2a99f5b66c7eb62bec1a6bb493f32ed552457b32b814&amp;scene=21#wechat_redirect" target="_blank" rel="noopener">如何看待消息中间件的选型</a>，不过这篇只表其意未表其行，为了弥补这种缺陷，笔者最近特意重新撰稿一篇，以供参考。温馨提示：本文一万多字，建议先马（关注）后看。</p>
</blockquote>
<h2 id="一、前言"><a href="#一、前言" class="headerlink" title="一、前言"></a>一、前言</h2><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;消息队列中间件（简称消息中间件）是指利用高效可靠的消息传递机制进行与平台无关的数据交流，并基于数据通信来进行分布式系统的集成。通过提供消息传递和消息排队模型，它可以在分布式环境下提供应用解耦、弹性伸缩、冗余存储、流量削峰、异步通信、数据同步等等功能，其作为分布式系统架构中的一个重要组件，有着举足轻重的地位。</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;目前开源的消息中间件可谓是琳琅满目，能让大家耳熟能详的就有很多，比如<code>ActiveMQ</code>、<code>RabbitMQ</code>、<code>Kafka</code>、<code>RocketMQ</code>、<code>ZeroMQ</code>等。不管选择其中的哪一款，都会有用的不趁手的地方，毕竟不是为你量身定制的。有些大厂在长期的使用过程中积累了一定的经验，其消息队列的使用场景也相对稳定固化，或者目前市面上的消息中间件无法满足自身需求，并且也具备足够的精力和人力而选择自研来为自己量身打造一款消息中间件。但是绝大多数公司还是不会选择重复造轮子，那么选择一款合适自己的消息中间件显得尤为重要。就算是前者，那么在自研出稳定且可靠的相关产品之前还是会经历这样一个选型过程。</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;在整体架构中引入消息中间件，势必要考虑很多因素，比如成本及收益问题，怎么样才能达到最优的性价比？虽然消息中间件种类繁多，但是各自都有各自的侧重点，选择合适自己、扬长避短无疑是最好的方式。如果你对此感到无所适从，本文或许可以参考一二。</p>
<h2 id="二、各类消息队列简述"><a href="#二、各类消息队列简述" class="headerlink" title="二、各类消息队列简述"></a>二、各类消息队列简述</h2><p><strong>ActiveMQ</strong>是<code>Apache</code>出品的、采用Java语言编写的完全基于<code>JMS1.1</code>规范的面向消息的中间件，为应用程序提供高效的、可扩展的、稳定的和安全的企业级消息通信。不过由于历史原因包袱太重，目前市场份额没有后面三种消息中间件多，其最新架构被命名为<code>Apollo</code>，号称下一代<code>ActiveMQ</code>，有兴趣的同学可行了解。</p>
<p><strong>RabbitMQ</strong>是采用<code>Erlang</code>语言实现的<code>AMQP</code>协议的消息中间件，最初起源于金融系统，用于在分布式系统中存储转发消息。<code>RabbitMQ</code>发展到今天，被越来越多的人认可，这和它在可靠性、可用性、扩展性、功能丰富等方面的卓越表现是分不开的。</p>
<p><strong>Kafka</strong>起初是由<code>LinkedIn</code>公司采用<code>Scala</code>语言开发的一个分布式、多分区、多副本且基于<code>zookeeper</code>协调的分布式消息系统，现已捐献给Apache基金会。它是一种高吞吐量的分布式发布订阅消息系统，以可水平扩展和高吞吐率而被广泛使用。目前越来越多的开源分布式处理系统如<code>Cloudera</code>、<code>Apache Storm</code>、<code>Spark</code>、<code>Flink</code>等都支持与<code>Kafka</code>集成。</p>
<p><strong>RocketMQ</strong>是阿里开源的消息中间件，目前已经捐献个<code>Apache</code>基金会，它是由<code>Java</code>语言开发的，具备高吞吐量、高可用性、适合大规模分布式系统应用等特点，经历过双11的洗礼，实力不容小觑。</p>
<p><strong>ZeroMQ</strong>号称史上最快的消息队列，基于C语言开发。<code>ZeroMQ</code>是一个消息处理队列库，可在多线程、多内核和主机之间弹性伸缩，虽然大多数时候我们习惯将其归入消息队列家族之中，但是其和前面的几款有着本质的区别，<code>ZeroMQ</code>本身就不是一个消息队列服务器，更像是一组底层网络通讯库，对原有的<code>Socket API</code>上加上一层封装而已。</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;目前市面上的消息中间件还有很多，比如腾讯系的<code>PhxQueue</code>、<code>CMQ</code>、<code>CKafka</code>，又比如基于<code>Go</code>语言的<code>NSQ</code>，有时人们也把类似<code>Redis</code>的产品也看做消息中间件的一种，当然它们都很优秀，但是本文篇幅限制无法穷极所有，下面会针对性的挑选<code>RabbitMQ</code>和<code>Kafka</code>两款典型的消息中间件来做分析，力求站在一个公平公正的立场来阐述消息中间件选型中的各个要点。</p>
<h2 id="三、选型要点概述"><a href="#三、选型要点概述" class="headerlink" title="三、选型要点概述"></a>三、选型要点概述</h2><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;衡量一款消息中间件是否符合需求需要从多个维度进行考察，首要的就是功能维度，这个直接决定了你能否最大程度上的实现开箱即用，进而缩短项目周期、降低成本等。如果一款消息中间件的功能达不到想要的功能，那么就需要进行二次开发，这样会增加项目的技术难度、复杂度以及增大项目周期等。</p>
<h3 id="1-功能维度"><a href="#1-功能维度" class="headerlink" title="1. 功能维度"></a>1. 功能维度</h3><p>功能维度又可以划分个多个子维度，大致可以分为以下这些：</p>
<h4 id="优先级队列"><a href="#优先级队列" class="headerlink" title="优先级队列"></a>优先级队列</h4><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;优先级队列不同于先进先出队列，<strong>优先级高的消息具备优先被消费的特权，这样可以为下游提供不同消息级别的保证</strong>。不过这个优先级也是需要有一个前提的：<strong>如果消费者的消费速度大于生产者的速度，并且消息中间件服务器（一般简单的称之为Broker）中没有消息堆积</strong>，那么对于发送的消息设置优先级也就没有什么实质性的意义了，因为生产者刚发送完一条消息就被消费者消费了，那么就相当于<code>Broker</code>中至多只有一条消息，对于单条消息来说优先级是没有什么意义的。</p>
<h4 id="延迟队列"><a href="#延迟队列" class="headerlink" title="延迟队列"></a>延迟队列</h4><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;当你在网上购物的时候是否会遇到这样的提示：“三十分钟之内未付款，订单自动取消”？这个是延迟队列的一种典型应用场景。<strong>延迟队列存储的是对应的延迟消息，所谓“延迟消息”是指当消息被发送以后，并不想让消费者立刻拿到消息，而是等待特定时间后，消费者才能拿到这个消息进行消费</strong>。</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;延迟队列一般分为两种：基于消息的延迟和基于队列的延迟。<strong>基于消息的延迟是指为每条消息设置不同的延迟时间，那么每当队列中有新消息进入的时候就会重新根据延迟时间排序，当然这也会对性能造成极大的影响</strong>。实际应用中大多采用基于队列的延迟，设置不同延迟级别的队列，比如<code>5s</code>、<code>10s</code>、<code>30s</code>、<code>1min</code>、<code>5mins</code>、<code>10mins</code>等，每个队列中消息的延迟时间都是相同的，这样免去了延迟排序所要承受的性能之苦，通过一定的扫描策略（比如定时）即可投递超时的消息。</p>
<h4 id="死信队列"><a href="#死信队列" class="headerlink" title="死信队列"></a>死信队列</h4><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<strong>由于某些原因消息无法被正确的投递，为了确保消息不会被无故的丢弃，一般将其置于一个特殊角色的队列，这个队列一般称之为死信队列</strong>。</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;与此对应的还有一个“回退队列”的概念，试想<strong>如果消费者在消费时发生了异常，那么就不会对这一次消费进行确认（<code>Ack</code>）,进而发生回滚消息的操作之后消息始终会放在队列的顶部，然后不断被处理和回滚，导致队列陷入死循环</strong>。为了解决这个问题，可以<strong>为每个队列设置一个回退队列，它和死信队列都是为异常的处理提供的一种机制保障</strong>。实际情况下，回退队列的角色可以由死信队列和重试队列来扮演。</p>
<h4 id="重试队列"><a href="#重试队列" class="headerlink" title="重试队列"></a>重试队列</h4><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;重试队列其实可以看成是一种回退队列，<strong>具体指消费端消费消息失败时，为防止消息无故丢失而重新将消息回滚到<code>Broker</code>中</strong>。与回退队列不同的是重试队列一般分成多个重试等级，每个重试等级一般也会设置重新投递延时，重试次数越多投递延时就越大。举个例子：消息第一次消费失败入重试队列<code>Q1</code>，<code>Q1</code>的重新投递延迟为5s，在5s过后重新投递该消息；如果消息再次消费失败则入重试队列<code>Q2</code>，<code>Q2</code>的重新投递延迟为10s，在10s过后再次投递该消息。以此类推，<strong>重试越多次重新投递的时间就越久，为此需要设置一个上限，超过投递次数就入死信队列</strong>。重试队列与延迟队列有相同的地方，都是需要设置延迟级别，它们彼此的区别是：<strong>延迟队列动作由内部触发，重试队列动作由外部消费端触发；延迟队列作用一次，而重试队列的作用范围会向后传递</strong>。</p>
<h4 id="消费模式"><a href="#消费模式" class="headerlink" title="消费模式"></a>消费模式</h4><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;消费模式分为推（<code>push</code>）模式和拉（<code>pull</code>）模式。推模式是指由<code>Broker</code>主动推送消息至消费端，实时性较好，不过<strong>需要一定的流制机制来确保服务端推送过来的消息不会压垮消费端</strong>。而拉模式是指消费端主动向Broker端请求拉取（一般是定时或者定量）消息，实时性较推模式差，但是可以根据自身的处理能力而控制拉取的消息量。</p>
<h4 id="广播消费"><a href="#广播消费" class="headerlink" title="广播消费"></a>广播消费</h4><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;消息一般有两种传递模式：点对点（<code>P2P，Point-to-Point</code>）模式和发布/订阅（<code>Pub/Sub</code>）模式。<strong>对于点对点的模式而言，消息被消费以后，队列中不会再存储，所以消息消费者不可能消费到已经被消费的消息</strong>。虽然队列可以支持多个消费者，但是一条消息只会被一个消费者消费。发布订阅模式定义了如何向一个内容节点发布和订阅消息，这个内容节点称为主题（<code>topic</code>），<strong>主题可以认为是消息传递的中介，消息发布者将消息发布到某个主题，而消息订阅者则从主题中订阅消息</strong>。<strong>主题使得消息的订阅者与消息的发布者互相保持独立，不需要进行接触即可保证消息的传递，发布/订阅模式在消息的一对多广播时采用</strong>。</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<code>RabbitMQ</code>是一种典型的点对点模式，而Kafka是一种典型的发布订阅模式。但是<code>RabbitMQ</code>中可以通过设置交换器类型来实现发布订阅模式而达到广播消费的效果。</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<code>Kafka</code>中也能以点对点的形式消费，你完全可以把其消费组（<code>consumer group</code>）的概念看成是队列的概念。不过对比来说，<code>Kafka</code>中因为有了消息回溯功能的存在，对于广播消费的力度支持比<code>RabbitMQ</code>的要强。</p>
<h4 id="消息回溯"><a href="#消息回溯" class="headerlink" title="消息回溯"></a>消息回溯</h4><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;一般消息在消费完成之后就被处理了，之后再也不能消费到该条消息。消息回溯正好相反，是指消息在消费完成之后，还能消费到之前被消费掉的消息。对于消息而言，经常面临的问题是<strong>“消息丢失”，至于是真正由于消息中间件的缺陷丢失还是由于使用方的误用而丢失一般很难追查，如果消息中间件本身具备消息回溯功能的话，可以通过回溯消费复现“丢失的”消息进而查出问题的源头之所在</strong>。消息回溯的作用远不止与此，比如还有索引恢复、本地缓存重建，有些业务补偿方案也可以采用回溯的方式来实现。</p>
<h4 id="消息堆积-持久化"><a href="#消息堆积-持久化" class="headerlink" title="消息堆积+持久化"></a>消息堆积+持久化</h4><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;流量削峰是消息中间件的一个非常重要的功能，而这个功能其实得益于其消息堆积能力。从某种意义上来讲，如果一个消息中间件不具备消息堆积的能力，那么就不能把它看做是一个合格的消息中间件。消息堆积分内存式堆积和磁盘式堆积。<code>RabbitMQ</code>是典型的内存式堆积，但这并非绝对，在某些条件触发后会有换页动作来将内存中的消息换页到磁盘（换页动作会影响吞吐），或者直接使用惰性队列来将消息直接持久化至磁盘中。<code>Kafka</code>是一种典型的磁盘式堆积，所有的消息都存储在磁盘中。一般来说，磁盘的容量会比内存的容量要大得多，对于磁盘式的堆积其堆积能力就是整个磁盘的大小。从另外一个角度讲，消息堆积也为消息中间件提供了冗余存储的功能。援引纽约时报的案例（<a href="https://www.confluent.io/blog/publishing-apache-kafka-new-york-times/），其直接将Kafka用作存储系统。" target="_blank" rel="noopener">https://www.confluent.io/blog/publishing-apache-kafka-new-york-times/），其直接将Kafka用作存储系统。</a></p>
<h4 id="消息追踪"><a href="#消息追踪" class="headerlink" title="消息追踪"></a>消息追踪</h4><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;对于分布式架构系统中的链路追踪（<code>trace</code>）而言，大家一定不会陌生。对于消息中间件而言，消息的链路追踪（以下简称消息追踪）同样重要。对于消息追踪最通俗的理解就是要知道消息从哪来，存在哪里以及发往哪里去。基于此功能下，我们可以对发送或者消费完的消息进行链路追踪服务，进而可以进行问题的快速定位与排查。</p>
<h4 id="消息过滤"><a href="#消息过滤" class="headerlink" title="消息过滤"></a>消息过滤</h4><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;消息过滤是指按照既定的过滤规则为下游用户提供指定类别的消息。就以<code>kafka</code>而言，完全可以将不同类别的消息发送至不同的<code>topic</code>中，由此可以实现某种意义的消息过滤，或者<code>Kafka</code>还可以根据分区对同一个<code>topic</code>中的消息进行分类。不过更加严格意义上的消息过滤应该是对既定的消息采取一定的方式按照一定的过滤规则进行过滤。同样以<code>Kafka</code>为例，可以通过客户端提供的<code>ConsumerInterceptor</code>接口或者<code>Kafka Stream</code>的<code>filter</code>功能进行消息过滤。</p>
<h4 id="多租户"><a href="#多租户" class="headerlink" title="多租户"></a>多租户</h4><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;也可以称为多重租赁技术，是一种软件架构技术，主要用来实现多用户的环境下公用相同的系统或程序组件，并且仍可以确保各用户间数据的隔离性。<code>RabbitMQ</code>就能够支持多租户技术，每一个租户表示为一个<code>vhost</code>，其本质上是一个独立的小型<code>RabbitMQ</code>服务器，又有自己独立的队列、交换器及绑定关系等，并且它拥有自己独立的权限。<code>vhost</code>就像是物理机中的虚拟机一样，它们在各个实例间提供逻辑上的分离，为不同程序安全保密地允许数据，它既能将同一个<code>RabbitMQ</code>中的众多客户区分开，又可以避免队列和交换器等命名冲突。</p>
<h4 id="多协议支持"><a href="#多协议支持" class="headerlink" title="多协议支持"></a>多协议支持</h4><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;消息是信息的载体，为了让生产者和消费者都能理解所承载的信息（生产者需要知道如何构造消息，消费者需要知道如何解析消息），它们就需要按照一种统一的格式描述消息，这种统一的格式称之为消息协议。有效的消息一定具有某种格式，而没有格式的消息是没有意义的。一般消息层面的协议有<code>AMQP、MQTT、STOMP、XMPP</code>等（消息领域中的<code>JMS</code>更多的是一个规范而不是一个协议），支持的协议越多其应用范围就会越广，通用性越强，比如<code>RabbitMQ</code>能够支持<code>MQTT</code>协议就让其在物联网应用中获得一席之地。还有的消息中间件是基于其本身的私有协议运转的，典型的如<code>Kafka</code>。</p>
<h4 id="跨语言支持"><a href="#跨语言支持" class="headerlink" title="跨语言支持"></a>跨语言支持</h4><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;对很多公司而言，其技术栈体系中会有多种编程语言，如<code>C/C++、JAVA、Go、PHP</code>等，消息中间件本身具备应用解耦的特性，如果能够进一步的支持多客户端语言，那么就可以将此特性的效能扩大。跨语言的支持力度也可以从侧面反映出一个消息中间件的流行程度。</p>
<h4 id="流量控制"><a href="#流量控制" class="headerlink" title="流量控制"></a>流量控制</h4><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;流量控制（<code>flow control</code>）针对的是发送方和接收方速度不匹配的问题，提供一种速度匹配服务抑制发送速率使接收方应用程序的读取速率与之相适应。通常的流控方法有<code>Stop-and-wait</code>、滑动窗口以及令牌桶等。</p>
<h4 id="消息顺序性"><a href="#消息顺序性" class="headerlink" title="消息顺序性"></a>消息顺序性</h4><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;顾名思义，消息顺序性是指保证消息有序。这个功能有个很常见的应用场景就是<code>CDC（Change Data Chapture）</code>，以<code>MySQL</code>为例，如果其传输的<code>binlog</code>的顺序出错，比如原本是先对一条数据加1，然后再乘以2，发送错序之后就变成了先乘以2后加1了，造成了数据不一致。</p>
<h4 id="安全机制"><a href="#安全机制" class="headerlink" title="安全机制"></a>安全机制</h4><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;在<code>Kafka 0.9</code>版本之后就开始增加了身份认证和权限控制两种安全机制。身份认证是指客户端与服务端连接进行身份认证，包括客户端与<code>Broker</code>之间、<code>Broker</code>与<code>Broker</code>之间、<code>Broker</code>与<code>ZooKeeper</code>之间的连接认证，目前支持<code>SSL</code>、<code>SASL</code>等认证机制。权限控制是指对客户端的读写操作进行权限控制，包括对消息或Kafka集群操作权限控制。权限控制是可插拔的，并支持与外部的授权服务进行集成。对于<code>RabbitMQ</code>而言，其同样提供身份认证（<code>TLS/SSL</code>、<code>SASL</code>）和权限控制（读写操作）的安全机制。</p>
<h4 id="消息幂等性"><a href="#消息幂等性" class="headerlink" title="消息幂等性"></a>消息幂等性</h4><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;对于确保消息在生产者和消费者之间进行传输而言一般有三种传输保障（<code>delivery guarantee</code>）：<code>At most once</code>，至多一次，消息可能丢失，但绝不会重复传输；<code>At least once</code>，至少一次，消息绝不会丢，但是可能会重复；<code>Exactly once</code>，精确一次，每条消息肯定会被传输一次且仅一次。对于大多数消息中间件而言，一般只提供<code>At most once</code>和<code>At least once</code>两种传输保障，对于第三种一般很难做到，由此消息幂等性也很难保证。</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<code>Kafka</code>自0.11版本开始引入了幂等性和事务，<code>Kafka</code>的幂等性是指单个生产者对于单分区单会话的幂等，而事务可以保证原子性地写入到多个分区，即写入到多个分区的消息要么全部成功，要么全部回滚，这两个功能加起来可以让<code>Kafka</code>具备<code>EOS（Exactly Once Semantic）</code>的能力。</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;不过如果要考虑全局的幂等，还需要与从上下游方面综合考虑，即关联业务层面，幂等处理本身也是业务层面所需要考虑的重要议题。以下游消费者层面为例，有可能消费者消费完一条消息之后没有来得及确认消息就发生异常，等到恢复之后又得重新消费原来消费过的那条消息，那么这种类型的消息幂等是无法有消息中间件层面来保证的。如果要保证全局的幂等，需要引入更多的外部资源来保证，比如以订单号作为唯一性标识，并且在下游设置一个去重表。</p>
<h4 id="事务性消息"><a href="#事务性消息" class="headerlink" title="事务性消息"></a>事务性消息</h4><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;事务本身是一个并不陌生的词汇，事务是由事务开始（<code>Begin Transaction</code>）和事务结束（<code>End Transaction</code>）之间执行的全体操作组成。支持事务的消息中间件并不在少数，<code>Kafka</code>和<code>RabbitMQ</code>都支持，不过此两者的事务是指生产者发生消息的事务，要么发送成功，要么发送失败。消息中间件可以作为用来实现分布式事务的一种手段，但其本身并不提供全局分布式事务的功能。</p>
<h4 id="Kafka与RabbitMQ的对比"><a href="#Kafka与RabbitMQ的对比" class="headerlink" title="Kafka与RabbitMQ的对比"></a>Kafka与RabbitMQ的对比</h4><p><img src="//blog.com/2019/07/18/消息中间件选型分析/2.webp" alt=""></p>
<h3 id="2-性能"><a href="#2-性能" class="headerlink" title="2. 性能"></a>2. 性能</h3><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;功能维度是消息中间件选型中的一个重要的参考维度，但这并不是唯一的维度。有时候性能比功能还要重要，况且性能和功能很多时候是相悖的，鱼和熊掌不可兼得，<code>Kafka</code>在开启幂等、事务功能的时候会使其性能降低，<code>RabbitMQ</code>在开启<code>rabbitmq_tracing</code>插件的时候也会极大的影响其性能。消息中间件的性能一般是指其吞吐量，虽然从功能维度上来说，<code>RabbitMQ</code>的优势要大于<code>Kafka</code>，但是<code>Kafka</code>的吞吐量要比<code>RabbitMQ</code>高出1至2个数量级，一般<code>RabbitMQ</code>的单机<code>QPS</code>在万级别之内，而<code>Kafka</code>的单机<code>QPS</code>可以维持在十万级别，甚至可以达到百万级。</p>
<blockquote>
<p> 消息中间件的吞吐量始终会受到硬件层面的限制。就以网卡带宽为例，如果单机单网卡的带宽为1Gbps，如果要达到百万级的吞吐，那么消息体大小不得超过(1Gb/8)/100W，即约等于134B，换句话说如果消息体大小超过134B，那么就不可能达到百万级别的吞吐。这种计算方式同样可以适用于内存和磁盘。</p>
</blockquote>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;时延作为性能维度的一个重要指标，却往往在消息中间件领域所被忽视，因为一般使用消息中间件的场景对时效性的要求并不是很高，如果要求时效性完全可以采用<code>RPC</code>的方式实现。消息中间件具备消息堆积的能力，消息堆积越大也就意味着端到端的时延也就越长，与此同时延时队列也是某些消息中间件的一大特色。那么为什么还要关注消息中间件的时延问题呢？消息中间件能够解耦系统，对于一个时延较低的消息中间件而言，它可以让上游生产者发送消息之后可以迅速的返回，也可以让消费者更加快速的获取到消息，在没有堆积的情况下可以让整体上下游的应用之间的级联动作更加高效，虽然不建议在时效性很高的场景下使用消息中间件，但是如果所使用的消息中间件的时延方面比较优秀，那么对于整体系统的性能将会是一个不小的提升。</p>
<h3 id="3-可靠性-可用性"><a href="#3-可靠性-可用性" class="headerlink" title="3. 可靠性+可用性"></a>3. 可靠性+可用性</h3><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;消息丢失是使用消息中间件时所不得不面对的一个同点，其背后消息可靠性也是衡量消息中间件好坏的一个关键因素。尤其是在金融支付领域，消息可靠性尤为重要。然而说到可靠性必然要说到可用性，注意这两者之间的区别，消息中间件的可靠性是指对消息不丢失的保障程度；而消息中间件的可用性是指无故障运行的时间百分比，通常用几个9来衡量。</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;从狭义的角度来说，分布式系统架构是一致性协议理论的应用实现，对于消息可靠性和可用性而言也可以追溯到消息中间件背后的一致性协议。</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;对于<code>Kafka</code>而言，其采用的是类似<code>PacificA</code>的<strong>一致性协议</strong>，<strong>通过<code>ISR（In-Sync-Replica）</code>来保证多副本之间的同步，并且支持强一致性语义（通过<code>acks</code>实现）</strong>。</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;对应的<code>RabbitMQ</code>是通过<strong>镜像环形队列实现多副本及强一致性语义</strong>的。多副本可以保证在<code>master</code>节点宕机异常之后可以提升<code>slave</code>作为新的<code>master</code>而继续提供服务来保障可用性。</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<code>Kafka</code>设计之初是为日志处理而生，给人们留下了数据可靠性要求不要的不良印象，但是随着版本的升级优化，其可靠性得到极大的增强，详细可以参考<code>KIP101</code>。</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;就目前而言，在金融支付领域使用<code>RabbitMQ</code>居多，而在日志处理、大数据等方面<code>Kafka</code>使用居多，随着<code>RabbitMQ</code>性能的不断提升和<code>Kafka</code>可靠性的进一步增强，相信彼此都能在以前不擅长的领域分得一杯羹。</p>
<blockquote>
<p>同步刷盘是增强一个组件可靠性的有效方式，消息中间件也不例外，Kafka和RabbitMQ都可以支持同步刷盘，但是笔者对同步刷盘有一定的疑问：绝大多数情景下，一个组件的可靠性不应该由同步刷盘这种极其损耗性能的操作来保障，而是采用多副本的机制来保证。</p>
</blockquote>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;这里还要提及的一个方面是扩展能力，这里我狭隘地将此归纳到可用性这一维度，消息中间件的扩展能力能够增强其用可用能力及范围，比如前面提到的<code>RabbitMQ</code>支持多种消息协议，这个就是基于其插件化的扩展实现。还有从集群部署上来讲，归功于<code>Kafka</code>的水平扩展能力，其基本上可以达到线性容量提升的水平，在<code>LinkedIn</code>实践介绍中就提及了有部署超过千台设备的<code>Kafka</code>集群。</p>
<h3 id="5-运维管理"><a href="#5-运维管理" class="headerlink" title="5. 运维管理"></a>5. 运维管理</h3><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;在消息中间件的使用过程中难免会出现各式各样的异常情况，有客户端的，也有服务端的，那么怎样及时有效的进行监测及修复。业务线流量有峰值又低谷，尤其是电商领域，那么怎样前进行有效的容量评估，尤其是大促期间？脚踢电源、网线被挖等事件层出不穷，如何有效的做好异地多活？这些都离不开消息中间件的衍生产品——运维管理。</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;运维管理也可以进行进一步的细分，比如：<strong>申请、审核、监控、告警、管理、容灾、部署等</strong>。</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;申请、审核很好理解，在源头对资源进行管控，既可以进行有效校正应用方的使用规范，配和监控也可以做好流量统计与流量评估工作，一般申请、审核与公司内部系统交融性较大，不适合使用开源类的产品。</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;监控、告警也比较好理解，对消息中间件的使用进行全方位的监控，即可以为系统提供基准数据，也可以在检测到异常的情况配合告警，以便运维、开发人员的迅速介入。除了一般的监控项（比如硬件、<code>GC</code>等）之外，对于消息中间件还需要<strong>关注端到端时延、消息审计、消息堆积等方面</strong>。对于<code>RabbitMQ</code>而言，最正统的监控管理工具莫过于<code>rabbitmq_management</code>插件了，但是社区内还有<code>AppDynamics, Collectd, DataDog, Ganglia, Munin, Nagios, New Relic, Prometheus, Zenoss</code>等多种优秀的产品。<code>Kafka</code>在此方面也毫不逊色，比如：<code>Kafka Manager, Kafka Monitor, Kafka Offset Monitor, Burrow, Chaperone, Confluent Control Center</code>等产品，尤其是<code>Cruise</code>还可以提供自动化运维的功能。</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;不管是<strong>扩容、降级、版本升级、集群节点部署、还是故障处理</strong>都离不开管理工具的应用，一个配套完备的管理工具集可以在遇到变更时做到事半功倍。故障可大可小，一般是一些应用异常，也可以是机器掉电、网络异常、磁盘损坏等单机故障，这些故障单机房内的多副本足以应付。如果是机房故障就要涉及异地容灾了，关键点在于如何有效的进行数据复制，对于<code>Kafka</code>而言，可以参考<code>MirrorMarker</code>、<code>Replicator</code>等产品，而<code>RabbitMQ</code>可以参考<code>Federation</code>和<code>Shovel</code>。</p>
<h3 id="6-社区力度及生态发展"><a href="#6-社区力度及生态发展" class="headerlink" title="6. 社区力度及生态发展"></a>6. 社区力度及生态发展</h3><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;对于目前流行的编程语言而言，如<code>Java</code>、<code>Python</code>，如果你在使用过程中遇到了一些异常，基本上可以通过搜索引擎的帮助来得到解决，因为一个产品用的人越多，踩过的坑也就越多，对应的解决方案也就越多。对于消息中间件也同样适用，如果你选择了一种“生僻”的消息中间件，可能在某些方面运用的得心应手，但是版本更新缓慢、遇到棘手问题也难以得到社区的支持而越陷越深；相反如果你选择了一种“流行”的消息中间件，其更新力度大，不仅可以迅速的弥补之前的不足，而且也能顺应技术的快速发展来变更一些新的功能，这样可以让你以“站在巨人的肩膀上”。在运维管理维度我们提及了<code>Kafka</code>和<code>RabbitMQ</code>都有一系列开源的监控管理产品，这些正是得益于其社区及生态的迅猛发展。</p>
<h2 id="四、消息中间件选型误区探讨"><a href="#四、消息中间件选型误区探讨" class="headerlink" title="四、消息中间件选型误区探讨"></a>四、消息中间件选型误区探讨</h2><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;在进行消息中间件选型之前可以先问自己一个问题：是否真的需要一个消息中间件？在搞清楚这个问题之后，还可以继续问自己一个问题：是否需要自己维护一套消息中间件？很多初创型公司为了节省成本会选择直接购买消息中间件有关的云服务，自己只需要关注收发消息即可，其余的都可以外包出去。</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;很多人面对消息中间件时会有一种自研的冲动，你完全可以对<code>Java</code>中的<code>ArrayBlockingQueue</code>做一个简单的封装，你也可以基于文件、数据库、<code>Redis</code>等底层存储封装而形成一个消息中间件。消息中间件做为一个基础组件并没有想象中的那么简单，其背后还需要配套的管理运维整个生态的产品集。自研还有会交接问题，如果文档不齐全、运作不规范将会带给新人噩梦般的体验。是否真的有自研的必要？如果不是<code>KPI</code>的压迫可以先考虑下这2个问题：1. 目前市面上的消息中间件是否都真的无法满足目前业务需求？ 2. 团队是否有足够的能力、人力、财力、精力来支持自研？</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;很多人在做消息中间件选型时会参考网络上的很多对比类的文章，但是其专业性、严谨性、以及其政治立场问题都有待考证，需要带着怀疑的态度去审视这些文章。比如有些文章会在没有任何限定条件及场景的情况下直接定义某款消息中间件最好，还有些文章没有指明消息中间件版本及测试环境就来做功能和性能对比分析，诸如此类的文章都可以唾弃之。</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;消息中间件犹如小马过河，选择合适的才最重要，这需要贴合自身的业务需求，技术服务于业务，大体上可以根据上一节所提及的功能、性能等6个维度来一一进行筛选。更深层次的抉择在于你能否掌握其魂，笔者鄙见：<strong>RabbitMQ在于routing，而Kafka在于streaming</strong>，了解其根本对于自己能够对症下药选择到合适的消息中间件尤为重要。</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;消息中间件选型切忌一味的追求性能或者功能，性能可以优化，功能可以二次开发。如果要在功能和性能方面做一个抉择的话，那么首选性能，因为总体上来说性能优化的空间没有功能扩展的空间大。然而对于长期发展而言，生态又比性能以及功能都要重要。</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;很多时候，对于可靠性方面也容易存在一个误区：想要找到一个产品来保证消息的绝对可靠，很不幸的是这世界上没有绝对的东西，只能说尽量趋于完美。想要尽可能的保障消息的可靠性也并非单单只靠消息中间件本身，还要依赖于上下游，需要从生产端、服务端和消费端这3个维度去努力保证，《<a href="http://mp.weixin.qq.com/s?__biz=MzU0MzQ5MDA0Mw==&amp;mid=2247483834&amp;idx=1&amp;sn=1a6b968e6e2624bfdf036cd5c797a238&amp;chksm=fb0beb2ecc7c6238fb849963abcf75e387815d1a92d4b7c283f2cd1a9f36de67765d5ac112bf&amp;scene=21#wechat_redirect" target="_blank" rel="noopener">RabbitMQ消息可靠性分析</a>》这篇文章就从这3个维度去分析了<code>RabbitMQ</code>的可靠性。</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;消息中间件选型还有一个考量标准就是<strong>尽量贴合团队自身的技术栈体系</strong>，虽然说没有蹩脚的消息中间件只有蹩脚的程序员，但是让一个<code>C</code>栈的团队去深挖<code>PhxQueue</code>总比去深挖<code>Scala</code>编写的<code>Kafka</code>要容易的多。</p>
<h2 id="五、总结"><a href="#五、总结" class="headerlink" title="五、总结"></a>五、总结</h2><p>消息中间件大道至简：一发一存一消费，没有最好的消息中间件，只有最合适的消息中间件。人过留名，雁过留声，路过记得点个赞。</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://blog.com/2019/07/15/分布式队列编程：模型与实战/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="刘泽明">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="搬运工 + 践行者">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/07/15/分布式队列编程：模型与实战/" itemprop="url">分布式队列编程：模型与实战</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-07-15T12:12:57+08:00">
                2019-07-15
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing">
                  <a href="/categories/架构/" itemprop="url" rel="index">
                    <span itemprop="name">架构</span>
                  </a>
                </span>

                
                
                  ， 
                
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing">
                  <a href="/categories/架构/消息队列/" itemprop="url" rel="index">
                    <span itemprop="name">消息队列</span>
                  </a>
                </span>

                
                
                  ， 
                
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing">
                  <a href="/categories/架构/消息队列/RabbitMQ/" itemprop="url" rel="index">
                    <span itemprop="name">RabbitMQ</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="分布式队列编程：模型与实战"><a href="#分布式队列编程：模型与实战" class="headerlink" title="分布式队列编程：模型与实战"></a>分布式队列编程：模型与实战</h1><p><br></p>
<blockquote>
<p>原文地址：<a href="https://mp.weixin.qq.com/s/yzNPe25s9b1hWQuY7HY1hg" target="_blank" rel="noopener">https://mp.weixin.qq.com/s/yzNPe25s9b1hWQuY7HY1hg</a></p>
</blockquote>
<p><br></p>
<h2 id="何时选择分布式队列"><a href="#何时选择分布式队列" class="headerlink" title="何时选择分布式队列"></a>何时选择分布式队列</h2><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;通讯是人们最基本的需求，同样也是计算机最基本的需求。对于工程师而言，在编程和技术选型的时候，更容易进入大脑的概念是<code>RPC、RESTful、Ajax、Kafka</code>。在这些具体的概念后面，最本质的东西是“通讯”。所以，大部分建模和架构都需要从“通讯”这个基本概念开始。当确定系统之间有通讯需求的时候，工程师们需要做很多的决策和平衡，这直接影响工程师们是否会选择分布式队列编程模型作为架构。从这个角度出发，影响建模的因素有四个：<code>When、Who、Where、How</code>。</p>
<h3 id="When：同步VS异步"><a href="#When：同步VS异步" class="headerlink" title="When：同步VS异步"></a>When：同步VS异步</h3><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;通讯的一个基本问题是：发出去的消息什么时候需要被接收到？这个问题引出了两个基础概念：“同步通讯”和“异步通讯”。根据理论抽象模型，同步通讯和异步通讯最本质的差别来自于时钟机制的有无。同步通讯的双方需要一个校准的时钟，异步通讯的双方不需要时钟。现实的情况是，没有完全校准的时钟，所以没有绝对的同步通讯。同样，绝对异步通讯意味着无法控制一个发出去的消息被接收到的时间点，无期限的等待一个消息显然毫无实际意义。所以，实际编程中所有的通讯既不是“同步通讯”也不是“异步通讯”；或者说，既是“同步通讯”也是“异步通讯”。特别是对于应用层的通讯，其底层架构可能既包含“同步机制”也包含“异步机制”。判断“同步”和“异步”消息的标准问题太深，而不适合继续展开。作者这里给一些启发式的建议：</p>
<ul>
<li><strong>发出去的消息是否需要确认，如果不需要确认，更像是异步通讯，这种通讯有时候也称为单向通讯</strong>（<code>One-Way Communication</code>）。</li>
<li><strong>如果需要确认，可以根据需要确认的时间长短进行判断</strong>。时间长的更像是异步通讯，时间短的更像是同步通讯。当然时间长短的概念是纯粹的主观概念，不是客观标准。</li>
<li><strong>发出去的消息是否阻塞下一个指令的执行，如果阻塞，更像是同步，否则，更像是异步</strong>。</li>
</ul>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;无论如何，工程师们不能生活在混沌之中，不做决定往往是最坏的决定。当分析一个通讯需求或者进行通讯构架的时候，工程师们被迫作出“同步”还是“异步”的决定。当决策的结论是“异步通讯”的时候，分布式队列编程模型就是一个备选项。</p>
<h3 id="Who：发送者接收者解耦"><a href="#Who：发送者接收者解耦" class="headerlink" title="Who：发送者接收者解耦"></a>Who：发送者接收者解耦</h3><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;在进行通讯需求分析的时候，需要回答的另外一个基本问题是：<strong>消息的发送方是否关心谁来接收消息，或者反过来，消息接收方是否关心谁来发送消息</strong>。如果工程师的结论是：消息的发送方和接收方不关心对方是谁、以及在哪里，分布式队列编程模型就是一个备选项。因为在这种场景下，分布式队列架构所带来的解耦能给系统架构带来这些好处：</p>
<ul>
<li>无论是发送方还是接收方，<strong>只需要跟消息中间件通讯，接口统一</strong>。统一意味着降低开发成本。</li>
<li>在不影响性能的前提下，同一套消息中间件部署，可以被不同业务共享。共享意味着降低运维成本。</li>
<li><strong>发送方或者接收方单方面的部署拓扑的变化不影响对应的另一方</strong>。<strong>解藕意味着灵活和可扩展</strong>。</li>
</ul>
<h3 id="Where：消息暂存机制"><a href="#Where：消息暂存机制" class="headerlink" title="Where：消息暂存机制"></a>Where：消息暂存机制</h3><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;在进行通讯发送方设计的时候，令工程师们苦恼的问题是：如果消息无法被迅速处理掉而产生堆积怎么办、能否被直接抛弃？如果<strong>根据需求分析，确认存在消息积存，并且消息不应该被抛弃</strong>，就应该考虑分布式队列编程模型构架，因为队列可以暂存消息。</p>
<h3 id="How：如何传递"><a href="#How：如何传递" class="headerlink" title="How：如何传递"></a>How：如何传递</h3><p>对通讯需求进行架构，一系列的基础挑战会迎面而来，这包括：</p>
<ul>
<li><p><strong>可用性</strong>，如何保障通讯的高可用。</p>
</li>
<li><p><strong>可靠性</strong>，如何保证消息被可靠地传递。</p>
</li>
<li><p><strong>持久化</strong>，如何保证消息不会丢失。</p>
</li>
<li><p><strong>吞吐量和响应时间</strong>。</p>
</li>
<li><p><strong>跨平台兼容性</strong>。</p>
</li>
</ul>
<p>  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;除非工程师对造轮子有足够的兴趣，并且有充足的时间，采用一个满足各项指标的分布式队列编程模型就是一个简单的选择。</p>
<h2 id="分布式队列编程定义"><a href="#分布式队列编程定义" class="headerlink" title="分布式队列编程定义"></a>分布式队列编程定义</h2><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;分布式队列编程模型包含三类角色：发送者（<code>Sender</code>）、分布式队列（<code>Queue</code>）、接收者（<code>Receiver</code>）。发送者和接收者分别指的是生产消息和接收消息的应用程序或服务。</p>
<p>需要重点明确的概念是分布式队列，它是提供以下功能的应用程序或服务：</p>
<ol>
<li><p>接收“发送者”产生的消息实体；</p>
</li>
<li><p>传输、暂存该实体；</p>
</li>
<li><p>为“接收者”提供读取该消息实体的功能。</p>
</li>
</ol>
<p>   特定的场景下，它当然可以是<code>Kafka</code>、<code>RabbitMQ</code>等消息中间件。但它的展现形式并不限于此，例如：</p>
<ul>
<li><p>队列可以是一张数据库的表，发送者将消息写入表，接收者从数据表里读消息。</p>
</li>
<li><p>如果一个程序把数据写入<code>Redis</code>等内存<code>Cache</code>里面，另一个程序从<code>Cache</code>里面读取，缓存在这里就是一种分布式队列。</p>
</li>
<li><p>流式编程里面的的数据流传输也是一种队列。</p>
</li>
<li><p>典型的<code>MVC</code>（<code>Model–view–controller</code>）设计模式里面，如果<code>Model</code>的变化需要导致<code>View</code>的变化，也可以通过队列进行传输。</p>
</li>
</ul>
<p>  这里的分布式队列可以是数据库，也可以是某台服务器上的一块内存。</p>
<h2 id="抽象模型"><a href="#抽象模型" class="headerlink" title="抽象模型"></a>抽象模型</h2><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;最基础的分布式队列编程抽象模型是点对点模型，其他抽象构架模型居于改基本模型上各角色的数量和交互变化所导致的不同拓扑图。具体而言，不同数量的发送者、分布式队列以及接收者组合形成了不同的分布式队列编程模型。记住并理解典型的抽象模型结构对需求分析和建模而言至关重要，同时也会有助于学习和深入理解开源框架以及别人的代码。</p>
<h3 id="点对点模型（Point-to-point）"><a href="#点对点模型（Point-to-point）" class="headerlink" title="点对点模型（Point-to-point）"></a>点对点模型（Point-to-point）</h3><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;基础模型中，只有一个发送者、一个接收者和一个分布式队列。如下图所示：</p>
<p><img src="//blog.com/2019/07/15/分布式队列编程：模型与实战/1.webp" alt="img"></p>
<h3 id="生产者消费者模型（Producer–consumer）"><a href="#生产者消费者模型（Producer–consumer）" class="headerlink" title="生产者消费者模型（Producer–consumer）"></a>生产者消费者模型（Producer–consumer）</h3><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;如果发送者和接收者都可以有多个部署实例，甚至不同的类型；但是共用同一个队列，这就变成了标准的生产者消费者模型。在该模型，三个角色一般称之为生产者（<code>Producer</code>）、分布式队列（<code>Queue</code>）、消费者（<code>Consumer</code>）。</p>
<p><img src="//blog.com/2019/07/15/分布式队列编程：模型与实战/2.webp" alt="img"></p>
<h3 id="发布订阅模型（PubSub）"><a href="#发布订阅模型（PubSub）" class="headerlink" title="发布订阅模型（PubSub）"></a>发布订阅模型（PubSub）</h3><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;如果只有一类发送者，发送者将产生的消息实体按照不同的主题（<code>Topic</code>）分发到不同的逻辑队列。每种主题队列对应于一类接收者。这就变成了典型的发布订阅模型。在该模型，三个角色一般称之为发布者（<code>Publisher</code>），分布式队列（<code>Queue</code>），订阅者（<code>Subscriber</code>）。</p>
<p><img src="//blog.com/2019/07/15/分布式队列编程：模型与实战/3.webp" alt="img"></p>
<h3 id="MVC模型"><a href="#MVC模型" class="headerlink" title="MVC模型"></a>MVC模型</h3><p>如果发送者和接收者存在于同一个实体中，但是共享一个分布式队列。这就很像经典的<code>MVC</code>模型。</p>
<p><img src="//blog.com/2019/07/15/分布式队列编程：模型与实战/4.webp" alt="img"></p>
<h2 id="编程模型"><a href="#编程模型" class="headerlink" title="编程模型"></a>编程模型</h2><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;为了让读者更好地理解分布式队列编程模式概念，这里将其与一些容易混淆的概念做一些对比 。</p>
<h3 id="分布式队列模型编程和异步编程"><a href="#分布式队列模型编程和异步编程" class="headerlink" title="分布式队列模型编程和异步编程"></a>分布式队列模型编程和异步编程</h3><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;分布式队列编程模型的通讯机制一般是采用异步机制，但是它并不等同于异步编程。</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;首先，并非所有的异步编程都需要引入队列的概念，例如：大部分的操作系统异步I/O操作都是通过硬件中断（ <code>Hardware Interrupts</code>）来实现的。</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;其次，异步编程并不一定需要跨进程，所以其应用场景并不一定是分布式环境。</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;最后，分布式队列编程模型强调发送者、接收者和分布式队列这三个角色共同组成的架构。这三种角色与异步编程没有太多关联。</p>
<h3 id="分布式队列模式编程和流式编程"><a href="#分布式队列模式编程和流式编程" class="headerlink" title="分布式队列模式编程和流式编程"></a>分布式队列模式编程和流式编程</h3><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;随着<code>Spark Streaming</code>，<code>Apache Storm</code>等流式框架的广泛应用，流式编程成了当前非常流行的编程模式。但是本文所阐述的分布式队列编程模型和流式编程并非同一概念。<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;首先，本文的队列编程模式不依赖于任何框架，而流式编程是在具体的流式框架内的编程。</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;其次，分布式队列编程模型是一个需求解决方案，关注如何根据实际需求进行分布式队列编程建模。流式框架里的数据流一般都通过队列传递，不过，流式编程的关注点比较聚焦，它关注如何从流式框架里获取消息流，进行<code>map</code>、<code>reduce</code>、 <code>join</code>等转型（<code>Transformation</code>）操作、生成新的数据流，最终进行汇总、统计。</p>
<h2 id="分布式队列编程实战"><a href="#分布式队列编程实战" class="headerlink" title="分布式队列编程实战"></a>分布式队列编程实战</h2><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;这里所有的项目都是作者在新美大工作的真实案例。实战篇的关注点是训练建模思路，所以这些例子都按照挑战、构思、架构三个步骤进行讲解。受限于保密性要求，有些细节并未给出，但这些细节并不影响讲解的完整性。另一方面，特别具体的需求容易让人费解，为了使讲解更加顺畅，作者也会采用一些更通俗易懂的例子。通过本篇的讲解，希望和读者一起去实践“如何从需求出发去构架分布式队列编程模型”。</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;需要声明的是，这里的解决方案并不是所处场景的最优方案。但是，任何一个稍微复杂的问题，都没有最优解决方案，更谈不上唯一的解决方案。实际上，工程师每天所追寻的只是在满足一定约束条件下的可行方案。当然不同的约束会导致不同的方案，约束的松弛度决定了工程师的可选方案的宽广度。</p>
<h3 id="信息采集处理"><a href="#信息采集处理" class="headerlink" title="信息采集处理"></a>信息采集处理</h3><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;信息采集处理应用广泛，例如：广告计费、用户行为收集等。作者碰到的具体项目是为广告系统设计<strong>一套高可用的采集计费系统</strong>。</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;典型的广告<code>CPC</code>、<code>CPM</code>计费原理是：收集用户在客户端或者网页上的点击和浏览行为，按照点击和浏览进行计费。</p>
<p>计费业务有如下典型特征：</p>
<ul>
<li><strong>采集者和处理者解耦，采集发生在客户端，而计费发生在服务端</strong>。</li>
<li>计费与钱息息相关。</li>
<li>重复计费意味着灾难。</li>
<li><strong>计费是动态实时行为</strong>，需要接受预算约束，如果消耗超过预算，则广告投放需要停止。</li>
<li><strong>用户的浏览和点击量非常大</strong>。</li>
</ul>
<h4 id="挑战"><a href="#挑战" class="headerlink" title="挑战"></a>挑战</h4><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;计费业务的典型特征给我们带来了如下挑战：</p>
<ul>
<li><strong>高吞吐量</strong>－－广告的浏览和点击量非常巨大，我们需要设计一个高吞吐量的采集架构。</li>
<li><strong>高可用性</strong>－－计费信息的丢失意味着直接的金钱损失。任何处理服务器的崩溃不应该导致系统不可用。</li>
<li><strong>高一致性要求</strong>－－计费是一个实时动态处理过程，但要受到预算的约束。收集到的浏览和点击行为如果不能快速处理，可能会导致预算花超，或者点击率预估不准确。所以<strong>采集到的信息应该在最短的时间内传输到计费中心进行计费</strong>。</li>
<li><strong>完整性约束</strong>－－这包括反作弊规则，单个用户行为不能重复计费等。这<strong>要求计费是一个集中行为而非分布式行为</strong>。</li>
<li><strong>持久化要求</strong>－－<strong>计费信息需要持久化，避免因为机器崩溃而导致收集到的数据产生丢失</strong>。</li>
</ul>
<h4 id="构思"><a href="#构思" class="headerlink" title="构思"></a>构思</h4><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;采集的高可用性意味着我们需要多台服务器同时采集，为了避免单<code>IDC</code>故障，采集服务器需要部署在多<code>IDC</code>里面。</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;实现一个<strong>高可用、高吞吐量、高一致性</strong>的信息传递系统显然是一个挑战，为了控制项目开发成本，采用开源的消息中间件进行消息传输就成了必然选择。</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<strong>完整性约束要求集中进行计费，所以计费系统发生在核心IDC</strong>。</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;计费服务并不关心采集点在哪里，采集服务也并不关心谁进行计费。</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;根据以上构思，我们认为<strong>采集计费符合典型的“生产者消费者模型”</strong>。</p>
<h4 id="架构"><a href="#架构" class="headerlink" title="架构"></a>架构</h4><p>采集计费系统架构图如下：</p>
<ul>
<li>用户点击浏览收集服务（<code>Click/View Collector</code>）作为生产者部署在多个机房里，以提高收集服务可用性。</li>
<li>每个机房里采集到的数据通过消息队列中间件发送到核心机房<code>IDC_Master</code>。</li>
<li><code>Billing</code>服务作为消费者部署在核心机房集中计费。</li>
</ul>
<p><img src="//blog.com/2019/07/15/分布式队列编程：模型与实战/5.webp" alt="img"></p>
<p>采用此架构，我们可以在如下方面做进一步优化：</p>
<ul>
<li><strong>提高可扩展性</strong>，如果一个<code>Billing</code>部署实例在性能上无法满足要求，可以对采集的数据进行<strong>主题分区</strong>（<code>Topic Partition</code>）计费，即<strong>采用发布订阅模式以提高可扩展性</strong>（<code>Scalability</code>）。</li>
<li><strong>全局排重和反作弊</strong>。采用集中计费架构解决了点击浏览排重的问题，另一方面，这也给反作弊提供了全局信息。</li>
<li><strong>提高计费系统的可用性</strong>。采用下文单例服务优化策略，在保障计费系统集中性的同时，提高计费系统可用性。</li>
</ul>
<h3 id="分布式缓存更新"><a href="#分布式缓存更新" class="headerlink" title="分布式缓存更新"></a>分布式缓存更新</h3><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;缓存是一个非常宽泛的概念，几乎存在于系统各个层级。典型的缓存访问流程如下：</p>
<ul>
<li>接收到请求后，先读取缓存，如果命中则返回结果。</li>
<li>如果缓存不命中，读取<code>DB</code>或其它持久层服务，更新缓存并返回结果。</li>
</ul>
<p><img src="//blog.com/2019/07/15/分布式队列编程：模型与实战/6.webp" alt="img"></p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;对于已经存入缓存的数据，其更新时机和更新频率是一个经典问题，即缓存更新机制（<code>Cache Replacement Algorithms</code>）。典型的缓存更新机制包括：近期最少使用算法（<code>LRU</code>）、最不经常使用算法（<code>LFU</code>）。</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;这两种缓存更新机制的典型实现是：启动一个后台进程，定期清理最近没有使用的，或者在一段时间内最少使用的数据。由于存在缓存驱逐机制，当一个请求在没有命中缓存时，业务层需要从持久层中获取信息并更新缓存，提高一致性。</p>
<h4 id="挑战-1"><a href="#挑战-1" class="headerlink" title="挑战"></a>挑战</h4><p>分布式缓存给缓存更新机制带来了新的问题：</p>
<ul>
<li><p><strong>数据一致性低</strong></p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;分布式缓存中键值数量巨大，从而导致<code>LRU</code>或者<code>LFU</code>算法更新周期很长。在分布式缓存中，拿<code>LRU</code>算法举例，其典型做法是为每个<code>Key</code>值设置一个生存时间（<code>TTL</code>），生存时间到期后将该键值从缓存中驱逐除去。考虑到分布式缓存中庞大的键值数量，生存时间往往会设置的比较长，这就导致缓存和持久层数据不一致时间很长。如果生存时间设置过短，大量请求无法命中缓存被迫读取持久层，系统响应时间会急剧恶化。</p>
</li>
<li><p><strong>新数据不可用</strong></p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;在很多场景下，由于分布式缓存和持久层的访问性能相差太大，在缓存不命中的情况下，一些应用层服务不会尝试读取持久层，而直接返回空结果。漫长的缓存更新周期意味着新数据的可用性就被牺牲了。从统计的角度来讲，新键值需要等待半个更新周期才会可用。</p>
</li>
</ul>
<h4 id="构思-1"><a href="#构思-1" class="headerlink" title="构思"></a>构思</h4><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;根据上面的分析，分布式缓存需要解决的问题是：在保证读取性能的前提下，尽可能地提高老数据的一致性和新数据的可用性。如果仍然假定最近被访问的键值最有可能被再次访问（这是<code>LRU</code>或者<code>LFU</code>成立的前提），键值每次被访问后触发一次异步更新就是提高可用性和一致性最早的时机。无论是高性能要求还是业务解耦都要求缓存读取和缓存更新分开，所以我们应该<strong>构建一个单独的集中的缓存更新服务</strong>。<strong>集中进行缓存更新的另外一个好处来自于频率控制</strong>。由于在一段时间内，很多类型访问键值的数量满足高斯分布，短时间内重复对同一个键值进行更新<code>Cache</code>并不会带来明显的好处，甚至造成缓存性能的下降。通过控制同一键值的更新频率可以大大缓解该问题，同时有利于提高整体数据的一致性，参见“排重优化”。</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;综上所述，<strong>业务访问方需要把请求键值快速传输给缓存更新方，它们之间不关心对方的业务</strong>。要快速、高性能地实现大量请求键值消息的传输，高性能分布式消息中间件就是一个可选项。这三方一起组成了一个典型的分布式队列编程模型。</p>
<h4 id="架构-1"><a href="#架构-1" class="headerlink" title="架构"></a>架构</h4><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;如下图，所有的业务请求方作为生产者，在返回业务代码处理之前将请求键值写入高性能队列。<code>Cache Updater</code>作为消费者从队列中读取请求键值，将持久层中数据更新到缓存中。</p>
<p><img src="//blog.com/2019/07/15/分布式队列编程：模型与实战/7.webp" alt="img"></p>
<p>采用此架构，我们可以在如下方面做进一步优化：</p>
<ul>
<li><strong>提高可扩展性</strong>，如果一个<code>Cache Updater</code>在性能上无法满足要求，可以对键值进行<strong>主题分区</strong>（<code>Topic Partition</code>）进行并行缓存更新，即<strong>采用发布订阅模式以提高可扩展性</strong>（<code>Scalability</code>）。</li>
<li><strong>更新频率控制</strong>。缓存更新都集中处理，对于发布订阅模式，同一类主题（<code>Topic</code>）的键值集中处理。<code>Cache Updater</code>可以控制对同一键值的在短期内的更新频率（参见下文排重优化）。</li>
</ul>
<h3 id="后台任务处理"><a href="#后台任务处理" class="headerlink" title="后台任务处理"></a>后台任务处理</h3><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;典型的后台任务处理应用包括工单处理、火车票预订系统、机票选座等。我们所面对的问题是为运营人员创建工单。一次可以为多个运营人员创建多个工单。这个应用场景和火车票购买非常类似。工单相对来说更加抽象，所以，下文会结合火车票购买和运营人员工单分配这两种场景同时讲解。</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;典型的工单创建要经历两个阶段：<strong>数据筛选阶段、工单创建阶段</strong>。例如，在火车票预订场景，数据筛选阶段用户选择特定时间、特定类型的火车，而在工单创建阶段，用户下单购买火车票。</p>
<h4 id="挑战-2"><a href="#挑战-2" class="headerlink" title="挑战"></a>挑战</h4><p>工单创建往往会面临如下挑战：</p>
<ul>
<li><strong>数据一致性问题</strong>。以火车票预订为例，用户筛选火车票和最终购买之间往往有一定的<strong>时延，意味着两个操作之间数据是不一致的</strong>。在筛选阶段，工程师们需决定是否进行车票锁定，<strong>如果不锁定，则无法保证出票成功</strong>。反之，<strong>如果在筛选地时候锁定车票，则会大大降低系统效率和出票吞吐量</strong>。</li>
<li><strong>约束问题</strong>。工单创建需要满足很多约束，主要包含两种类型：动态约束，与操作者的操作行为有关，例如购买几张火车票的决定往往发生在筛选最后阶段。隐性约束，这种约束很难通过界面进行展示，例如一个用户购买了5张火车票，这些票应该是在同一个车厢的临近位置。</li>
<li><strong>优化问题</strong>。工单创建往往是约束下的优化，这是典型的统筹优化问题，而统筹优化往往需要比较长的时间。</li>
<li><strong>响应时间问题</strong>。对于多任务工单，一个请求意味着多个任务产生。这些任务的创建往往需要遵循事务性原则，即<code>All or Nothing</code>。在数据层面，这意味着工单之间需要满足串行化需求（<code>Serializability</code>）。大数据量的串行化往往意味着锁冲突延迟甚至失败。无论是延迟机制所导致的长时延，还是高创建失败率，都会大大伤害用户体验。</li>
</ul>
<h4 id="构思-2"><a href="#构思-2" class="headerlink" title="构思"></a>构思</h4><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;如果<strong>将用户筛选的最终规则做为消息存储下来，并发送给工单创建系统</strong>。此时，工单创建系统将具备创建工单所需的全局信息，具备在满足各种约束的条件下进行统筹优化的能力。</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;如果<strong>工单创建阶段采用单实例部署，就可以避免数据锁定问题</strong>，同时也意味着没有锁冲突，所以也不会有死锁或任务延迟问题。<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;居于以上思路，<strong>在多工单处理系统的模型中，筛选阶段的规则创建系统将充当生产者角色，工单创建系统将充当消费者角色，筛选规则将作为消息在两者之间进行传递</strong>。这就是典型的分布式队列编程架构。根据工单创建量的不同，可以采用数据库或开源的分布式消息中间件作为分布式队列。</p>
<h4 id="架构-2"><a href="#架构-2" class="headerlink" title="架构"></a>架构</h4><p>该架构流程如下图：</p>
<ul>
<li>用户首选进行规则创建，这个过程主要是一些搜索筛选操作；</li>
<li>用户点击工单创建，<code>TicketRule Generator</code>将把所有的筛选性组装成规则消息并发送到队列里面去；</li>
<li><code>Ticket Generator</code>作为一个消费者，实时从队列中读取工单创建请求，开始真正创建工单。</li>
</ul>
<p><img src="//blog.com/2019/07/15/分布式队列编程：模型与实战/8.webp" alt="img"></p>
<p>采用该架构，我们在数据锁定、运筹优化、原子性问题都能得到比较好成果：</p>
<ul>
<li><strong>数据锁定推迟到工单创建阶段，可以减少数据锁定范围</strong>，最大程度的降低工单创建对其他在线操作的影响范围。</li>
<li>如果需要进行统筹优化，可以将<code>Ticket Generator</code>以单例模式进行部署（参见单例服务优化）。这样，<code>Ticket Generator</code>可以读取一段时间内的工单请求，进行全局优化。例如，在我们的项目中，在某种条件下，运营人员需要满足分级公平原则，即相同级别的运营人员的工单数量应该接近，不同级别的运营人员工单数量应该有所区分。如果不集中进行统筹优化，实现这种优化规则将会很困难。</li>
<li>保障了约束完整性。例如，在我们的场景里面，每个运营人员每天能够处理的工单是有数量限制的，如果采用并行处理的方式，这种完整性约束将会很难实施。</li>
</ul>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://blog.com/2019/07/15/美团-消息队列设计精要/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="刘泽明">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="搬运工 + 践行者">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/07/15/美团-消息队列设计精要/" itemprop="url">美团-消息队列设计精要</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-07-15T12:12:57+08:00">
                2019-07-15
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing">
                  <a href="/categories/架构/" itemprop="url" rel="index">
                    <span itemprop="name">架构</span>
                  </a>
                </span>

                
                
                  ， 
                
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing">
                  <a href="/categories/架构/消息队列/" itemprop="url" rel="index">
                    <span itemprop="name">消息队列</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="美团-消息队列设计精要"><a href="#美团-消息队列设计精要" class="headerlink" title="美团-消息队列设计精要"></a>美团-消息队列设计精要</h1><p><br></p>
<blockquote>
<p>原文地址：<a href="https://mp.weixin.qq.com/s/tcbX1FlSvxSm1C238aR3zA" target="_blank" rel="noopener">https://mp.weixin.qq.com/s/tcbX1FlSvxSm1C238aR3zA</a></p>
</blockquote>
<p><br></p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;消息队列已经逐渐成为企业IT系统内部通信的核心手段。它具有低耦合、可靠投递、广播、流量控制、最终一致性等一系列功能，成为异步<code>RPC</code>的主要手段之一。</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;当今市面上有很多主流的消息中间件，如老牌的<code>ActiveMQ</code>、<code>RabbitMQ</code>，炙手可热的<code>Kafka</code>，阿里巴巴自主开发的<code>Notify</code>、<code>MetaQ</code>、<code>RocketMQ</code>等。</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;本文不会一一介绍这些消息队列的所有特性，而是探讨一下自主开发设计一个消息队列时，你需要思考和设计的重要方面。过程中我们会参考这些成熟消息队列的很多重要思想。</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;本文首先会阐述什么时候你需要一个消息队列，然后以Push模型为主，从零开始分析设计一个消息队列时需要考虑到的问题，如<code>RPC</code>、高可用、顺序和重复消息、可靠投递、消费关系解析等。</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;也会分析以<code>Kafka</code>为代表的<code>pull</code>模型所具备的优点。最后是一些高级主题，如用批量/异步提高性能、<code>pull</code>模型的系统设计理念、存储子系统的设计、流量控制的设计、公平调度的实现等。其中最后四个方面会放在下篇讲解。</p>
<h2 id="何时需要消息队列"><a href="#何时需要消息队列" class="headerlink" title="何时需要消息队列"></a>何时需要消息队列</h2><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;当你需要使用消息队列时，首先需要考虑它的必要性。可以使用<code>mq</code>的场景有很多，最常用的几种，是做业务解耦/最终一致性/广播/错峰流控等。反之，<strong>如果需要强一致性，关注业务逻辑的处理结果，则<code>RPC</code>显得更为合适</strong>。</p>
<h3 id="解耦"><a href="#解耦" class="headerlink" title="解耦"></a>解耦</h3><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;解耦是消息队列要解决的最本质问题。所谓解耦，简单点讲就是一个事务，只关心核心的流程。而<strong>需要依赖其他系统但不那么重要的事情，有通知即可，无需等待结果</strong>。换句话说，<strong>基于消息的模型，关心的是“通知”，而非“处理”</strong>。</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;比如在美团旅游，我们有一个产品中心，产品中心上游对接的是主站、移动后台、旅游供应链等各个数据源；下游对接的是筛选系统、API系统等展示系统。当上游的数据发生变更的时候，如果不使用消息系统，势必要调用我们的接口来更新数据，就特别依赖产品中心接口的稳定性和处理能力。但其实，作为旅游的产品中心，也许只有对于旅游自建供应链，产品中心更新成功才是他们关心的事情。而对于团购等外部系统，产品中心更新成功也好、失败也罢，并不是他们的职责所在。他们只需要保证在信息变更的时候通知到我们就好了。</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;而我们的下游，可能有更新索引、刷新缓存等一系列需求。对于产品中心来说，这也不是我们的职责所在。说白了，如果他们定时来拉取数据，也能保证数据的更新，只是实时性没有那么强。但使用接口方式去更新他们的数据，显然对于产品中心来说太过于“重量级”了，只需要发布一个产品ID变更的通知，由下游系统来处理，可能更为合理。</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;再举一个例子，对于我们的订单系统，订单最终支付成功之后可能需要给用户发送短信积分什么的，但其实这已经不是我们系统的核心流程了。如果外部系统速度偏慢（比如短信网关速度不好），那么主流程的时间会加长很多，用户肯定不希望点击支付过好几分钟才看到结果。那么我们只需要通知短信系统“我们支付成功了”，不一定非要等待它处理完成。</p>
<h3 id="最终一致性"><a href="#最终一致性" class="headerlink" title="最终一致性"></a>最终一致性</h3><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<strong>最终一致性指的是两个系统的状态保持一致，要么都成功，要么都失败</strong>。当然有个时间限制，理论上越快越好，但实际上在各种异常的情况下，可能会有一定延迟达到最终一致状态，但最后两个系统的状态是一样的。</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;业界有一些为“最终一致性”而生的消息队列，如<code>Notify</code>（阿里）、<code>QMQ</code>（去哪儿）等，其设计初衷，就是为了交易系统中的高可靠通知。</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;以一个银行的转账过程来理解最终一致性，转账的需求很简单，如果A系统扣钱成功，则B系统加钱一定成功。反之则一起回滚，像什么都没发生一样。</p>
<p>然而，这个过程中存在很多可能的意外：</p>
<ol>
<li>A扣钱成功，调用B加钱接口失败。</li>
<li>A扣钱成功，调用B加钱接口虽然成功，但获取最终结果时网络异常引起超时。</li>
<li>A扣钱成功，B加钱失败，A想回滚扣的钱，但A机器down机。</li>
</ol>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;可见，想把这件看似简单的事真正做成，真的不那么容易。所有跨VM的一致性问题，从技术的角度讲通用的解决方案是：</p>
<ol>
<li><p>强一致性，分布式事务，但落地太难且成本太高，后文会具体提到。</p>
</li>
<li><p>最终一致性，主要是用“记录”和“补偿”的方式。在做所有的不确定的事情之前，先把事情记录下来，然后去做不确定的事情，结果可能是：成功、失败或是不确定，“不确定”（例如超时等）可以等价为失败。成功就可以把记录的东西清理掉了，对于失败和不确定，可以依靠定时任务等方式把所有失败的事情重新搞一遍，直到成功为止。</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;回到刚才的例子，系统在A扣钱成功的情况下，把要给B“通知”这件事记录在库里（为了保证最高的可靠性可以把通知B系统加钱和扣钱成功这两件事维护在一个本地事务里），通知成功则删除这条记录，通知失败或不确定则依靠定时任务补偿性地通知我们，直到我们把状态更新成正确的为止。整个这个模型依然可以基于    <code>RPC</code>来做，但可以抽象成一个统一的模型，基于消息队列来做一个“企业总线”。</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;具体来说，本地事务维护业务变化和通知消息，一起落地（失败则一起回滚），然后<code>RPC</code>到达<code>broker</code>，在<code>broker</code>成功落地后，<code>RPC</code>返回成功，本地消息可以删除。否则本地消息一直靠定时任务轮询不断重发，这样就保证了消息可靠落地<code>broker</code>。</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<code>broker</code>往<code>consumer</code>发送消息的过程类似，一直发送消息，直到<code>consumer</code>发送消费成功确认。<br>我们先不理会重复消息的问题，通过两次消息落地加补偿，下游是一定可以收到消息的。然后依赖状态机版本号等方式做判重，更新自己的业务，就实现了最终一致性。</p>
</li>
</ol>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;最终一致性不是消息队列的必备特性，但确实可以依靠消息队列来做最终一致性的事情。另外，所有不保证100%不丢消息的消息队列，理论上无法实现最终一致性。好吧，应该说理论上的100%，排除系统严重故障和bug。</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;像<code>Kafka</code>一类的设计，在设计层面上就有丢消息的可能（比如定时刷盘，如果掉电就会丢消息）。哪怕只丢千分之一的消息，业务也必须用其他的手段来保证结果正确。</p>
<h3 id="广播"><a href="#广播" class="headerlink" title="广播"></a>广播</h3><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;消息队列的基本功能之一是进行广播。如果没有消息队列，每当一个新的业务方接入，我们都要联调一次新接口。<strong>有了消息队列，我们只需要关心消息是否送达了队列，至于谁希望订阅，是下游的事情，无疑极大地减少了开发和联调的工作量</strong>。</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;比如本文开始提到的产品中心发布产品变更的消息，以及景点库很多去重更新的消息，可能“关心”方有很多个，但产品中心和景点库只需要发布变更消息即可，谁关心谁接入。</p>
<h3 id="错峰与流控"><a href="#错峰与流控" class="headerlink" title="错峰与流控"></a>错峰与流控</h3><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;试想上下游对于事情的处理能力是不同的。比如，<code>Web</code>前端每秒承受上千万的请求，并不是什么神奇的事情，只需要加多一点机器，再搭建一些<code>LVS</code>负载均衡设备和<code>Nginx</code>等即可。但数据库的处理能力却十分有限，即使使用<code>SSD</code>加分库分表，单机的处理能力仍然在万级。由于成本的考虑，我们不能奢求数据库的机器数量追上前端。</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;这种问题同样存在于系统和系统之间，如短信系统可能由于短板效应，速度卡在网关上（每秒几百次请求），跟前端的并发量不是一个数量级。但用户晚上个半分钟左右收到短信，一般是不会有太大问题的。如果没有消息队列，两个系统之间通过协商、滑动窗口等复杂的方案也不是说不能实现。但系统复杂性指数级增长，势必在上游或者下游做存储，并且要处理定时、拥塞等一系列问题。而且每当有处理能力有差距的时候，都需要单独开发一套逻辑来维护这套逻辑。所以，利用中间系统转储两个系统的通信内容，并在下游系统有能力处理这些消息的时候，再处理这些消息，是一套相对较通用的方式。</p>
<h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;总而言之，消息队列不是万能的。<strong>对于需要强事务保证而且延迟敏感的，RPC是优于消息队列的</strong>。</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;对于一些无关痛痒，或者对于别人非常重要但是对于自己不是那么关心的事情，可以利用消息队列去做。</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;支持最终一致性的消息队列，能够用来处理延迟不那么敏感的“分布式事务”场景，而且相对于笨重的分布式事务，可能是更优的处理方式。</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;当上下游系统处理能力存在差距的时候，利用消息队列做一个通用的“漏斗”。在下游有能力处理的时候，再进行分发。</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;如果下游有很多系统关心你的系统发出的通知的时候，果断地使用消息队列吧。</p>
<h2 id="如何设计一个消息队列"><a href="#如何设计一个消息队列" class="headerlink" title="如何设计一个消息队列"></a>如何设计一个消息队列</h2><h3 id="综述"><a href="#综述" class="headerlink" title="综述"></a>综述</h3><p>我们现在明确了消息队列的使用场景，下一步就是如何设计实现一个消息队列了。</p>
<p><img src="//blog.com/2019/07/15/美团-消息队列设计精要/1.webp" alt=""></p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;基于消息的系统模型，不一定需要<code>broker</code>(消息队列服务端)。市面上的的<code>Akka</code>（<code>actor</code>模型）、<code>ZeroMQ</code>等，其实都是基于消息的系统设计范式，但是没有<code>broker</code>。</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;我们之所以要设计一个消息队列，并且配备<code>broker</code>，无外乎要做两件事情：</p>
<ol>
<li><p><strong>消息的转储，在更合适的时间点投递，或者通过一系列手段辅助消息最终能送达消费机</strong>。</p>
</li>
<li><p><strong>规范一种范式和通用的模式，以满足解耦、最终一致性、错峰等需求</strong>。</p>
</li>
</ol>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;掰开了揉碎了看，最简单的消息队列可以做成一个消息转发器，把一次<code>RPC</code>做成两次<code>RPC</code>。发送者把消息投递到服务端（以下简称<code>broker</code>），服务端再将消息转发一手到接收端，就是这么简单。</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;一般来讲，设计消息队列的整体思路是先<code>build</code>一个整体的数据流,例如<code>producer</code>发送给<code>broker</code>,<code>broker</code>发送给<code>consumer</code>,<code>consumer</code>回复消费确认，<code>broker</code>删除/备份消息等。</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;利用<code>RPC</code>将数据流串起来。然后考虑<code>RPC</code>的高可用性，尽量做到无状态，方便水平扩展。</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;之后考虑如何承载消息堆积，然后在合适的时机投递消息，而处理堆积的最佳方式，就是存储，存储的选型需要综合考虑性能/可靠性和开发维护成本等诸多因素。</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;为了实现广播功能，我们必须要维护消费关系，可以利用<code>zk/config server</code>等保存消费关系。</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;在完成了上述几个功能后，消息队列基本就实现了。然后我们可以考虑一些高级特性，如可靠投递，事务特性，性能优化等。</p>
<h3 id="实现队列基本功能"><a href="#实现队列基本功能" class="headerlink" title="实现队列基本功能"></a>实现队列基本功能</h3><h4 id="RPC通信协议"><a href="#RPC通信协议" class="headerlink" title="RPC通信协议"></a>RPC通信协议</h4><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;刚才讲到，所谓消息队列，无外乎两次<code>RPC</code>加一次转储，当然需要消费端最终做消费确认的情况是三次<code>RPC</code>。既然是<code>RPC</code>，就必然牵扯出一系列话题，什么负载均衡啊、服务发现啊、通信协议啊、序列化协议啊，等等。在这一块，我的强烈建议是不要重复造轮子。利用公司现有的<code>RPC</code>框架：<code>Thrift</code>也好，<code>Dubbo</code>也好，或者是其他自定义的框架也好。因为消息队列的<code>RPC</code>，和普通的<code>RPC</code>没有本质区别。当然了，自主利用<code>Memchached</code>或者<code>Redis</code>协议重新写一套<code>RPC</code>框架并非不可（如<code>MetaQ</code>使用了自己封装的<code>Gecko NIO</code>框架，卡夫卡也用了类似的协议）。但实现成本和难度无疑倍增。排除对效率的极端要求，都可以使用现成的<code>RPC</code>框架。</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;简单来讲，服务端提供两个<code>RPC</code>服务，一个用来接收消息，一个用来确认消息收到。并且做到不管哪个server收到消息和确认消息，结果一致即可。当然这中间可能还涉及跨<code>IDC</code>的服务的问题。这里和<code>RPC</code>的原则是一致的，尽量优先选择本机房投递。你可能会问，如果<code>producer</code>和<code>consumer</code>本身就在两个机房了，怎么办？首先，<code>broker</code>必须保证感知的到所有<code>consumer</code>的存在。其次，<code>producer</code>尽量选择就近的机房就好了。</p>
<h4 id="高可用"><a href="#高可用" class="headerlink" title="高可用"></a>高可用</h4><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;其实所有的高可用，是依赖于<code>RPC</code>和存储的高可用来做的。先来看<code>RPC</code>的高可用，美团的基于<code>MTThrift</code>的<code>RPC</code>框架，阿里的<code>Dubbo</code>等，其本身就具有服务自动发现，负载均衡等功能。而消息队列的高可用，只要保证<code>broker</code>接受消息和确认消息的接口是幂等的，并且<code>consumer</code>的几台机器处理消息是幂等的，这样就把消息队列的可用性，转交给<code>RPC</code>框架来处理了。</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;那么怎么保证幂等呢？最简单的方式莫过于共享存储。<code>broker</code>多机器共享一个<code>DB</code>或者一个分布式文件/kv系统，则处理消息自然是幂等的。就算有单点故障，其他节点可以立刻顶上。另外<code>failover</code>可以依赖定时任务的补偿，这是消息队列本身天然就可以支持的功能。存储系统本身的可用性我们不需要操太多心，放心大胆的交给<code>DBA</code>们吧！</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;对于不共享存储的队列，如<code>Kafka</code>使用分区加主备模式，就略微麻烦一些。需要保证每一个分区内的高可用性，也就是每一个分区至少要有一个主备且需要做数据的同步。</p>
<h4 id="服务端承载消息堆积的能力"><a href="#服务端承载消息堆积的能力" class="headerlink" title="服务端承载消息堆积的能力"></a>服务端承载消息堆积的能力</h4><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;消息到达服务端如果不经过任何处理就到接收者了，<code>broker</code>就失去了它的意义。为了满足我们错峰/流控/最终可达等一系列需求，把消息存储下来，然后选择时机投递就显得是顺理成章的了。</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;只是这个存储可以做成很多方式。比如存储在内存里，存储在分布式<code>KV</code>里，存储在磁盘里，存储在数据库里等等。但归结起来，主要有持久化和非持久化两种。</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;持久化的形式能更大程度地保证消息的可靠性（如断电等不可抗外力），并且理论上能承载更大限度的消息堆积（外存的空间远大于内存）。</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;但并不是每种消息都需要持久化存储。很多消息对于投递性能的要求大于可靠性的要求，且数量极大（如日志）。这时候，消息不落地直接暂存内存，尝试几次<code>failover</code>，最终投递出去也未尝不可。</p>
<h4 id="存储子系统的选择"><a href="#存储子系统的选择" class="headerlink" title="存储子系统的选择"></a>存储子系统的选择</h4><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;我们来看看如果需要数据落地的情况下各种存储子系统的选择。理论上，从速度来看，文件系统&gt;分布式<code>KV</code>（持久化）&gt;分布式文件系统&gt;数据库，而可靠性却截然相反。还是要从支持的业务场景出发作出最合理的选择，如果你们的消息队列是用来支持支付/交易等对可靠性要求非常高，但对性能和量的要求没有这么高，而且没有时间精力专门做文件存储系统的研究，DB是最好的选择。</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;但是<code>DB</code>受制于<code>IOPS</code>，如果要求单<code>broker</code>5位数以上的<code>QPS</code>性能，基于文件的存储是比较好的解决方案。整体上可以采用数据文件+索引文件的方式处理。</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;分布式<code>KV</code>（如<code>MongoDB，HBase</code>）等，或者持久化的<code>Redis</code>，由于其编程接口较友好，性能也比较可观，如果在可靠性要求不是那么高的场景，也不失为一个不错的选择。</p>
<h4 id="消费关系解析"><a href="#消费关系解析" class="headerlink" title="消费关系解析"></a>消费关系解析</h4><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;现在我们的消息队列初步具备了转储消息的能力。下面一个重要的事情就是解析发送接收关系，进行正确的消息投递了。</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;市面上的消息队列定义了一堆让人晕头转向的名词，如<code>JMS</code>规范中的<code>Topic/Queue</code>，<code>Kafka</code>里面的<code>Topic/Partition/ConsumerGroup</code>，<code>RabbitMQ</code>里面的<code>Exchange</code>等等。</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;抛开现象看本质，无外乎是<strong>单播与广播</strong>的区别。<strong>所谓单播，就是点到点；而广播，是一点对多点</strong>。当然，对于互联网的大部分应用来说，组间广播、组内单播是最常见的情形。</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;消息需要通知到多个业务集群，而一个业务集群内有很多台机器，只要一台机器消费这个消息就可以了。</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;当然这不是绝对的，很多时候组内的广播也是有适用场景的，如本地缓存的更新等等。另外，消费关系除了组内组间，可能会有多级树状关系。这种情况太过于复杂，一般不列入考虑范围。所以，<strong>一般比较通用的设计是支持组间广播，不同的组注册不同的订阅</strong>。<strong>组内的不同机器，如果注册一个相同的ID，则单播；如果注册不同的ID(如IP地址+端口)，则广播</strong>。</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;至于<strong>广播关系的维护，一般由于消息队列本身都是集群，所以都维护在公共存储上</strong>，如<code>config server</code>、<code>zookeeper</code>等。维护广播关系所要做的事情基本是一致的:</p>
<ol>
<li>发送关系的维护。</li>
<li>发送关系变更时的通知。</li>
</ol>
<h3 id="队列高级特性设计"><a href="#队列高级特性设计" class="headerlink" title="队列高级特性设计"></a>队列高级特性设计</h3><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;上面都是些消息队列基本功能的实现，下面来看一些关于消息队列特性相关的内容，不管可靠投递/消息丢失与重复以及事务乃至于性能，不是每个消息队列都会照顾到，所以要依照业务的需求，来仔细衡量各种特性实现的成本，利弊，最终做出最为合理的设计。</p>
<h4 id="可靠投递（最终一致性）"><a href="#可靠投递（最终一致性）" class="headerlink" title="可靠投递（最终一致性）"></a>可靠投递（最终一致性）</h4><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;这是个激动人心的话题，完全不丢消息，究竟可不可能？答案是，完全可能，<strong>前提是消息可能会重复，并且，在异常情况下，要接受消息的延迟</strong>。</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;方案说简单也简单，就是每当要发生不可靠的事情（<code>RPC</code>等）之前，先将消息落地，然后发送。当失败或者不知道成功失败（比如超时）时，消息状态是待发送，定时任务不停轮询所有待发送消息，最终一定可以送达。</p>
<p>具体来说：</p>
<ol>
<li><p><code>producer</code>往<code>broker</code>发送消息之前，需要做一次落地。</p>
</li>
<li><p>请求到<code>server</code>后，<code>server</code>确保数据落地后再告诉客户端发送成功。</p>
</li>
<li><p>支持广播的消息队列需要对每个待发送的<code>endpoint</code>，持久化一个发送状态，直到所有<code>endpoint</code>状态都<code>OK</code>才可删除消息。</p>
</li>
</ol>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;对于各种不确定（超时、<code>down</code>机、消息没有送达、送达后数据没落地、数据落地了回复没收到），其实对于发送方来说，都是一件事情，就是消息没有送达。</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;重推消息所面临的问题就是消息重复。<strong>重复和丢失就像两个噩梦，你必须要面对一个</strong>。好在消息重复还有处理的机会，消息丢失再想找回就难了。</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<code>Anyway</code>，作为一个成熟的消息队列，应该尽量在各个环节减少重复投递的可能性，不能因为重复有解决方案就放纵的乱投递。</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;最后说一句，不是所有的系统都要求最终一致性或者可靠投递，比如一个论坛系统、一个招聘系统。一个重复的简历或话题被发布，可能比丢失了一个发布显得更让用户无法接受。不断重复一句话，任何基础组件要服务于业务场景。</p>
<h4 id="消费确认"><a href="#消费确认" class="headerlink" title="消费确认"></a>消费确认</h4><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;当<code>broker</code>把消息投递给消费者后，消费者可以立即响应我收到了这个消息。但收到了这个消息只是第一步，我能不能处理这个消息却不一定。或许因为消费能力的问题，系统的负荷已经不能处理这个消息；或者是刚才状态机里面提到的消息不是我想要接收的消息，主动要求重发。</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;把消息的送达和消息的处理分开，这样才真正的实现了消息队列的本质-解耦。所以，允许消费者主动进行消费确认是必要的。当然，对于没有特殊逻辑的消息，默认<code>Auto Ack</code>也是可以的，但一定要<strong>允许消费方主动ack</strong>。</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;对于正确消费<code>ack</code>的，没什么特殊的。但是对于<code>reject</code>和<code>error</code>，需要特别说明。<code>reject</code>这件事情，往往业务方是无法感知到的，系统的流量和健康状况的评估，以及处理能力的评估是一件非常复杂的事情。举个极端的例子，收到一个消息开始<code>build</code>索引，可能这个消息要处理半个小时，但消息量却是非常的小。所以<code>reject</code>这块建议做成滑动窗口/线程池类似的模型来控制，消费能力不匹配的时候，直接拒绝，过一段时间重发，减少业务的负担。</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;但业务出错这件事情是只有业务方自己知道的，就像上文提到的状态机等等。这时应该允许业务方主动<code>ack error</code>，并可以与<code>broker</code>约定下次投递的时间。</p>
<h4 id="重复消息和顺序消息"><a href="#重复消息和顺序消息" class="headerlink" title="重复消息和顺序消息"></a>重复消息和顺序消息</h4><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;上文谈到重复消息是不可能100%避免的，除非可以允许丢失，那么，顺序消息能否100%满足呢? 答案是可以，但条件更为苛刻：</p>
<ol>
<li><strong>允许消息丢失</strong>。</li>
<li><strong>从发送方到服务方到接受者都是单点单线程</strong>。</li>
</ol>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;所以<strong>绝对的顺序消息基本上是不能实现的，当然在<code>METAQ/Kafka</code>等pull模型的消息队列中，单线程生产/消费，排除消息丢失，也是一种顺序消息的解决方案</strong>。</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;一般来讲，<strong>一个主流消息队列的设计范式里，应该是不丢消息的前提下，尽量减少重复消息，不保证消息的投递顺序</strong>。</p>
<p>谈到重复消息，主要是两个话题：</p>
<ol>
<li>如何鉴别消息重复，并幂等的处理重复消息。</li>
<li>一个消息队列如何尽量减少重复消息的投递。</li>
</ol>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;先来看看第一个话题，每一个消息应该有它的唯一身份。不管是业务方自定义的，还是根据<strong>IP/PID/时间戳生成的<code>MessageId</code>，如果有地方记录这个<code>MessageId</code>，消息到来是能够进行比对就能完成重复的鉴定</strong>。数据库的唯一键/<code>bloom filter</code>/分布式<code>KV</code>的<code>key</code>，都是不错的选择。由于消息不能被永久存储，所以理论上都存在消息从持久化存储移除的瞬间上游还在投递的可能（上游因种种原因投递失败，不停重试，都到了下游清理消息的时间）。这种事情都是异常情况下才会发生的，毕竟是小众情况。两分钟消息都还没送达，多送一次又能怎样呢？幂等的处理消息是一门艺术，因为种种原因重复消息或者错乱的消息还是来到了，说两种通用的解决方案：</p>
<ol>
<li>版本号</li>
<li>状态机</li>
</ol>
<h5 id="版本号"><a href="#版本号" class="headerlink" title="版本号"></a>版本号</h5><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;举个简单的例子，一个产品的状态有上线/下线状态。如果消息1是下线，消息2是上线。不巧消息1判重失败，被投递了两次，且第二次发生在2之后，如果不做重复性判断，显然最终状态是错误的。</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;但是，如果每个消息自带一个版本号。上游发送的时候，标记消息1版本号是1，消息2版本号是2。如果再发送下线消息，则版本号标记为3。下游对于每次消息的处理，同时维护一个版本号。</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;每次只接受比当前版本号大的消息。初始版本为0，当消息1到达时，将版本号更新为1。消息2到来时，因为版本号&gt;1.可以接收，同时更新版本号为2.当另一条下线消息到来时，如果版本号是3.则是真实的下线消息。如果是1，则是重复投递的消息。</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;如果业务方只关心消息重复不重复，那么问题就已经解决了。但很多时候另一个头疼的问题来了，就是消息顺序如果和想象的顺序不一致。比如应该的顺序是12，到来的顺序是21。则最后会发生状态错误。</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;参考<code>TCP/IP</code>协议，<strong>如果想让乱序的消息最后能够正确的被组织，那么就应该只接收比当前版本号大一的消息</strong>。<strong>并且在一个<code>session</code>周期内要一直保存各个消息的版本号</strong>。</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;如果到来的顺序是21，则先把2存起来，待2到来后，再处理1，这样重复性和顺序性要求就都达到了。</p>
<h5 id="状态机"><a href="#状态机" class="headerlink" title="状态机"></a>状态机</h5><p>基于版本号来处理重复和顺序消息听起来是个不错的主意，但凡事总有瑕疵。<strong>使用版本号的最大问题是</strong>：</p>
<ol>
<li><strong>对发送方必须要求消息带业务版本号</strong>。</li>
<li><strong>下游必须存储消息的版本号，对于要严格保证顺序的</strong>。</li>
</ol>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;还不能只存储最新的版本号的消息，要把乱序到来的消息都存储起来。而且必须要对此做出处理。试想一个永不过期的<code>&quot;session&quot;</code>，比如一个物品的状态，会不停流转于上下线。那么<strong>中间环节的所有存储就必须保留，直到在某个版本号之前的版本一个不丢的到来，成本太高</strong>。</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;就刚才的场景看，如果消息没有版本号，该怎么解决呢？<strong>业务方只需要自己维护一个状态机，定义各种状态的流转关系</strong>。例如，”下线”状态只允许接收”上线”消息，“上线”状态只能接收“下线消息”，如果上线收到上线消息，或者下线收到下线消息，在消息不丢失和上游业务正确的前提下。要么是消息发重了，要么是顺序到达反了。这时消费者只需要把“我不能处理这个消息”告诉投递者，<strong>要求投递者过一段时间重发即可</strong>。而且<strong>重发一定要有次数限制</strong>，比如5次，避免死循环，就解决了。</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;举例子说明，假设产品本身状态是下线，1是上线消息，2是下线消息，3是上线消息，正常情况下，消息应该的到来顺序是123，但实际情况下收到的消息状态变成了3123。</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;那么下游收到3消息的时候，判断状态机流转是下线-&gt;上线，可以接收消息。然后收到消息1，发现是上线-&gt;上线，拒绝接收，要求重发。然后收到消息2，状态是上线-&gt;下线，于是接收这个消息。</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;此时无论重发的消息1或者3到来，还是可以接收。另外的重发，在一定次数拒绝后停止重发，业务正确。</p>
<h4 id="中间件对于重复消息的处理"><a href="#中间件对于重复消息的处理" class="headerlink" title="中间件对于重复消息的处理"></a>中间件对于重复消息的处理</h4><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;回归到消息队列的话题来讲。上述通用的版本号/状态机/ID判重解决方案里，哪些是消息队列该做的、哪些是消息队列不该做业务方处理的呢？其实这里没有一个完全严格的定义，但回到我们的出发点，我们保证不丢失消息的情况下尽量少重复消息，消费顺序不保证。那么<strong>重复消息下和乱序消息下业务的正确，应该是由消费方保证的</strong>，我们要做的是减少消息发送的重复。</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;我们无法定义业务方的业务版本号/状态机，如果<code>API</code>里强制需要指定版本号，则显得过于绑架客户了。况且，在消费方维护这么多状态，就涉及到一个消费方的消息落地/多机间的同步消费状态问题，复杂度指数级上升，而且只能解决部分问题。</p>
<p>减少重复消息的关键步骤：</p>
<ol>
<li><p><code>broker</code>记录<code>MessageId</code>，直到投递成功后清除，重复的ID到来不做处理，这样只要发送者在清除周期内能够感知到消息投递成功，就基本不会在<code>server</code>端产生重复消息。</p>
</li>
<li><p>对于<code>server</code>投递到<code>consumer</code>的消息，由于不确定对端是在处理过程中还是消息发送丢失的情况下，有必要记录下投递的<code>IP</code>地址。决定重发之前询问这个<code>IP</code>，消息处理成功了吗？如果询问无果，再重发。</p>
</li>
</ol>
<h4 id="事务"><a href="#事务" class="headerlink" title="事务"></a>事务</h4><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;持久性是事务的一个特性，然而只满足持久性却不一定能满足事务的特性。还是拿扣钱/加钱的例子讲。满足事务的一致性特征，则必须要么都不进行，要么都能成功。解决方案从大方向上有两种：</p>
<ol>
<li><strong>两阶段提交，分布式事务</strong>。</li>
<li><strong>本地事务，本地落地，补偿发送</strong>。</li>
</ol>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;分布式事务存在的最大问题是成本太高，两阶段提交协议，对于仲裁<code>down</code>机或者单点故障，几乎是一个无解的黑洞。对于交易密集型或者I/O密集型的应用，没有办法承受这么高的网络延迟，系统复杂性。</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;并且成熟的分布式事务一定构建与比较靠谱的商用DB和商用中间件上，成本也太高。</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;那如何使用本地事务解决分布式事务的问题呢？以本地和业务在一个数据库实例中建表为例子，与扣钱的业务操作同一个事务里，将消息插入本地数据库。</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;如果消息入库失败，则业务回滚；如果消息入库成功，事务提交。</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;然后发送消息（注意这里可以实时发送，不需要等定时任务检出，以提高消息实时性）。以后的问题就是前文的最终一致性问题所提到的了，<strong>只要消息没有发送成功，就一直靠定时任务重试</strong>。</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;这里有一个关键的点，<strong>本地事务做的，是业务落地和消息落地的事务，而不是业务落地和RPC成功的事务</strong>。这里很多人容易混淆，如果是后者，无疑是事务嵌套<code>RPC</code>，是大忌，会有长事务死锁等各种风险。</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;而消息只要成功落地，很大程度上就没有丢失的风险（磁盘物理损坏除外）。</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;而消息只要投递到服务端确认后本地才做删除，就完成了<code>producer-&gt;broker</code>的可靠投递，并且当消息存储异常时，业务也是可以回滚的。</p>
<p><strong>本地事务存在两个最大的使用障碍</strong>：</p>
<ol>
<li><strong>配置较为复杂，“绑架”业务方，必须本地数据库实例提供一个库表</strong>。</li>
<li><strong>对于消息延迟高敏感的业务不适用</strong>。</li>
</ol>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;话说回来，不是每个业务都需要强事务的。扣钱和加钱需要事务保证，但下单和生成短信却不需要事务，不能因为要求发短信的消息存储投递失败而要求下单业务回滚。所以，一个完整的消息队列应该定义清楚自己可以投递的消息类型，如事务型消息，本地非持久型消息，以及服务端不落地的非可靠消息等。对不同的业务场景做不同的选择。另外事务的使用应该尽量低成本、透明化，可以依托于现有的成熟框架，如<code>Spring</code>的声明式事务做扩展。业务方只需要使用<code>@Transactional</code>标签即可。</p>
<h3 id="性能相关"><a href="#性能相关" class="headerlink" title="性能相关"></a>性能相关</h3><h4 id="异步-同步"><a href="#异步-同步" class="headerlink" title="异步/同步"></a>异步/同步</h4><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;首先澄清一个概念，异步，同步和<code>oneway</code>是三件事。异步，归根结底你还是需要关心结果的，但可能不是当时的时间点关心，可以用轮询或者回调等方式处理结果；同步是需要当时关心的结果的；而<code>oneway</code>是发出去就不管死活的方式，这种对于某些完全对可靠性没有要求的场景还是适用的，但不是我们重点讨论的范畴。</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;回归来看，任何的<code>RPC</code>都是存在客户端异步与服务端异步的，而且是可以任意组合的：客户端同步对服务端异步，客户端异步对服务端异步，客户端同步对服务端同步，客户端异步对服务端同步。</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;对于客户端来说，同步与异步主要是拿到一个<code>Result</code>，还是<code>Future(Listenable)</code>的区别。实现方式可以是线程池，<code>NIO</code>或者其他事件机制，这里先不展开讲。</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;服务端异步可能稍微难理解一点，这个是需要<code>RPC</code>协议支持的。参考<code>servlet 3.0</code>规范，服务端可以吐一个<code>future</code>给客户端，并且在<code>future done</code>的时候通知客户端。</p>
<p>整个过程可以参考下面的代码：</p>
<p>客户端同步服务端异步。</p>
<p><img src="//blog.com/2019/07/15/美团-消息队列设计精要/2.webp" alt="img"></p>
<p>客户端同步服务端同步。</p>
<p><img src="//blog.com/2019/07/15/美团-消息队列设计精要/3.webp" alt="img"></p>
<p>客户端异步服务端同步(这里用线程池的方式)。</p>
<p><img src="//blog.com/2019/07/15/美团-消息队列设计精要/4.webp" alt="img"></p>
<p>客户端异步服务端异步。</p>
<p><img src="//blog.com/2019/07/15/美团-消息队列设计精要/5.webp" alt="img"></p>
<p>上面说了这么多，其实是想让大家脱离两个误区：</p>
<ol>
<li><code>RPC</code>只有客户端能做异步，服务端不能。</li>
<li>异步只能通过线程池。</li>
</ol>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;那么，服务端使用异步最大的好处是什么呢？说到底，是解放了线程和<code>I/O</code>。试想服务端有一堆<code>I/O</code>等待处理，如果每个请求都需要同步响应，每条消息都需要结果立刻返回，那么就几乎没法做<code>I/O</code>合并（当然接口可以设计成<code>batch</code>的，但可能<code>batch</code>发过来的仍然数量较少）。而如果用异步的方式返回给客户端<code>future</code>，就可以有机会进行<code>I/O</code>的合并，把几个批次发过来的消息一起落地（这种合并对于<code>MySQL</code>等允许<code>batch insert</code>的数据库效果尤其明显），并且彻底释放了线程。不至于说来多少请求开多少线程，能够支持的并发量直线提高。</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;来看第二个误区，返回<code>future</code>的方式不一定只有线程池。换句话说，可以在线程池里面进行同步操作，也可以进行异步操作，也可以不使用线程池使用异步操作（<code>NIO</code>、事件）。</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;回到消息队列的议题上，我们当然不希望消息的发送阻塞主流程（前面提到了，<code>server</code>端如果使用异步模型，则可能因消息合并带来一定程度上的消息延迟），所以可以先使用线程池提交一个发送请求，主流程继续往下走。</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;但是线程池中的请求关心结果吗？<code>Of course</code>，必须等待服务端消息成功落地，才算是消息发送成功。所以这里的模型，准确地说事客户端半同步半异步（使用线程池不阻塞主流程，但线程池中的任务需要等待<code>server</code>端的返回），<code>server</code>端是纯异步。客户端的线程池<code>wait</code>在<code>server</code>端吐回的<code>future</code>上，直到<code>server</code>端处理完毕，才解除阻塞继续进行。</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;总结一句，<strong>同步能够保证结果，异步能够保证效率</strong>，要合理的结合才能做到最好的效率。</p>
<h4 id="批量"><a href="#批量" class="headerlink" title="批量"></a>批量</h4><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;谈到批量就不得不提生产者消费者模型。但生产者消费者模型中最大的痛点是：消费者到底应该何时进行消费。大处着眼来看，<strong>消费动作都是事件驱动的</strong>。主要事件包括：</p>
<ol>
<li><strong>攒够了一定数量</strong>。</li>
<li><strong>到达了一定时间</strong>。</li>
<li><strong>队列里有新的数据到来</strong>。</li>
</ol>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;对于及时性要求高的数据，可用采用方式3来完成，比如客户端向服务端投递数据。只要队列有数据，就把队列中的所有数据刷出，否则将自己挂起，等待新数据的到来。</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;在第一次把队列数据往外刷的过程中，又积攒了一部分数据，第二次又可以形成一个批量。伪代码如下:</p>
<p><img src="//blog.com/2019/07/15/美团-消息队列设计精要/6.webp" alt=""></p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;这种方式是消息延迟和批量的一个比较好的平衡，但优先响应低延迟。延迟的最高程度由上一次发送的等待时间决定。但可能造成的问题是发送过快的话批量的大小不够满足性能的极致。</p>
<p><img src="//blog.com/2019/07/15/美团-消息队列设计精要/7.webp" alt=""></p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;相反对于可以用适量的延迟来换取高性能的场景来说，用定时/定量二选一的方式可能会更为理想，既到达一定数量才发送，但如果数量一直达不到，也不能干等，有一个时间上限。</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;具体说来，在上文的<code>submit</code>之前，多判断一个时间和数量，并且<code>Runnable</code>内部维护一个定时器，避免没有新任务到来时旧的任务永远没有机会触发发送条件。对于<code>server</code>端的数据落地，使用这种方式就非常方便。</p>
<p>最后啰嗦几句，曾经有人问我，为什么网络请求小包合并成大包会提高性能？主要原因有两个：</p>
<ol>
<li>减少无谓的请求头，如果你每个请求只有几字节，而头却有几十字节，无疑效率非常低下。</li>
<li>减少回复的<code>ack</code>包个数。把请求合并后，<code>ack</code>包数量必然减少，确认和重发的成本就会降低。</li>
</ol>
<h3 id="push还是pull"><a href="#push还是pull" class="headerlink" title="push还是pull"></a>push还是pull</h3><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;上文提到的消息队列，大多是针对<code>push</code>模型的设计。现在市面上有很多经典的也比较成熟的<code>pull</code>模型的消息队列，如<code>Kafka</code>、<code>MetaQ</code>等。这跟<code>JMS</code>中传统的<code>push</code>方式有很大的区别，可谓另辟蹊径。</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;我们简要分析下<code>push</code>和<code>pull</code>模型各自存在的利弊。</p>
<h4 id="慢消费"><a href="#慢消费" class="headerlink" title="慢消费"></a>慢消费</h4><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<strong>慢消费无疑是<code>push</code>模型最大的致命伤，穿成流水线来看，如果消费者的速度比发送者的速度慢很多，势必造成消息在<code>broker</code>的堆积</strong>。假设这些消息都是有用的无法丢弃的，消息就要一直在<code>broker</code>端保存。当然这还不是最致命的，最致命的是<code>broker</code>给<code>consumer</code>推送一堆<code>consumer</code>无法处理的消息，<code>consumer</code>不是<code>reject</code>就是<code>error</code>，然后来回踢皮球。</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;反观<code>pull</code>模式，<code>consumer</code>可以按需消费，不用担心自己处理不了的消息来骚扰自己，而<code>broker</code>堆积消息也会相对简单，无需记录每一个要发送消息的状态，只需要维护所有消息的队列和偏移量就可以了。所以对于建立索引等慢消费，消息量有限且到来的速度不均匀的情况，<code>pull</code>模式比较合适。</p>
<h4 id="消息延迟与忙等"><a href="#消息延迟与忙等" class="headerlink" title="消息延迟与忙等"></a>消息延迟与忙等</h4><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;这是<code>pull</code>模式最大的短板。由于主动权在消费方，消费方无法准确地决定何时去拉取最新的消息。如果一次<code>pull</code>取到消息了还可以继续去<code>pull</code>，如果没有<code>pull</code>取到则需要等待一段时间重新<code>pull</code>。</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;但等待多久就很难判定了。你可能会说，我可以有xx动态<code>pull</code>取时间调整算法，但问题的本质在于，有没有消息到来这件事情决定权不在消费方。也许1分钟内连续来了1000条消息，然后半个小时没有新消息产生，可能你的算法算出下次最有可能到来的时间点是31分钟之后，或者60分钟之后，结果下条消息10分钟后到了，是不是很让人沮丧？</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;当然也不是说延迟就没有解决方案了，业界较成熟的做法是从短时间开始（不会对<code>broker</code>有太大负担），然后<strong>指数级增长等待</strong>。比如开始等5ms，然后10ms，然后20ms，然后40ms……直到有消息到来，然后再回到5ms。</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;即使这样，依然存在延迟问题：假设40ms到80ms之间的50ms消息到来，消息就延迟了30ms，而且对于半个小时来一次的消息，这些开销就是白白浪费的。</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;在阿里的<code>RocketMq</code>里，有一种优化的做法-<strong>长轮询</strong>，来平衡推拉模型各自的缺点。基本思路是:消费者如果尝试拉取失败，不是直接<code>return</code>,而是把连接挂在那里<code>wait</code>,服务端如果有新的消息到来，把连接<code>notify</code>起来，这也是不错的思路。但海量的长连接<code>block</code>对系统的开销还是不容小觑的，还是要<strong>合理的评估时间间隔，给<code>wait</code>加一个时间上限比较好</strong>~</p>
<h4 id="顺序消息"><a href="#顺序消息" class="headerlink" title="顺序消息"></a>顺序消息</h4><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;如果<code>push</code>模式的消息队列，支持分区，单分区只支持一个消费者消费，并且消费者只有确认一个消息消费后才能<code>push</code>送另外一个消息，还要发送者保证全局顺序唯一，听起来也能做顺序消息，但成本太高了，尤其是<strong>必须每个消息消费确认后才能发下一条消息，这对于本身堆积能力和慢消费就是瓶颈的<code>push</code>模式的消息队列，简直是一场灾难</strong>。</p>
<p>反观<code>pull</code>模式，如果想做到全局顺序消息，就相对容易很多：</p>
<ol>
<li><strong><code>producer</code>对应<code>partition</code>，并且单线程</strong>。</li>
<li><strong><code>consumer</code>对应<code>partition</code>，消费确认（或批量确认），继续消费即可</strong>。</li>
</ol>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;所以对于日志<code>push</code>送这种最好全局有序，但允许出现小误差的场景，<code>pull</code>模式非常合适。如果你不想看到通篇乱套的日志~~Anyway，需要顺序消息的场景还是比较有限的而且成本太高，请慎重考虑。</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://blog.com/2019/07/15/分布式队列编程优化篇/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="刘泽明">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="搬运工 + 践行者">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/07/15/分布式队列编程优化篇/" itemprop="url">分布式队列编程优化篇</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-07-15T12:12:57+08:00">
                2019-07-15
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing">
                  <a href="/categories/架构/" itemprop="url" rel="index">
                    <span itemprop="name">架构</span>
                  </a>
                </span>

                
                
                  ， 
                
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing">
                  <a href="/categories/架构/消息队列/" itemprop="url" rel="index">
                    <span itemprop="name">消息队列</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="分布式队列编程优化篇"><a href="#分布式队列编程优化篇" class="headerlink" title="分布式队列编程优化篇"></a>分布式队列编程优化篇</h1><p><br></p>
<blockquote>
<p>原文地址：<a href="https://mp.weixin.qq.com/s/uTZgkcBUH_6DefpqpLkWQQ" target="_blank" rel="noopener">https://mp.weixin.qq.com/s/uTZgkcBUH_6DefpqpLkWQQ</a></p>
</blockquote>
<p><br></p>
<h2 id="生产者优化"><a href="#生产者优化" class="headerlink" title="生产者优化"></a>生产者优化</h2><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;在分布式队列编程中，生产者往往并非真正的生产源头，只是整个数据流中的一个节点，这种生产者的操作是<strong>处理－转发（<code>Process-Forward</code>）模式</strong>。</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;这种模式给工程师们带来的第一个问题是吞吐量问题。<strong>这种模式下运行的生产者，一边接收上游的数据，一边将处理完的数据发送给下游</strong>。本质上，它是一个非常经典的数学问题，<strong>其抽象模型是一些没有盖子的水箱，每个水箱接收来自上一个水箱的水，进行处理之后，再将水发送到下一个水箱</strong>。<strong>工程师需要预测水源的流量、每个环节水箱的处理能力、水龙头的排水速度，最终目的是避免水溢出水箱，或者尽可能地减小溢出事件的概率</strong>。实际上流式编程框架以及其开发者花了大量的精力去处理和优化这个问题。下文的缓存优化和批量写入优化都是针对该问题的解决方案。</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;第二个需要考虑的问题是持久化。由于各种原因，系统总是会宕机。如果信息比较敏感，例如计费信息、火车票订单信息等，工程师们需要考虑系统宕机所带来的损失，找到让损失最小化的解决方案。持久化优化重点解决这一类问题。</p>
<h2 id="缓存优化"><a href="#缓存优化" class="headerlink" title="缓存优化"></a>缓存优化</h2><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<strong>处于“处理－转发”模式下运行的生产者往往被设计成请求驱动型的服务，即每个请求都会触发一个处理线程，线程处理完后将结果写入分布式队列</strong>。如果由于某种原因队列服务不可用，或者性能恶化，随着新请求的到来，生产者的处理线程就会产生堆积。这可能会导致如下两个问题：</p>
<ul>
<li><strong>系统可用性降低</strong>。由于每个线程都需要一定的<strong>内存开销</strong>，线程过多会使系统<strong>内存耗尽</strong>，甚至可能<strong>产生雪崩效应导致最终完全不可用</strong>。</li>
<li><strong>信息丢失</strong>。为了避免系统崩溃，工程师可能会<strong>给请求驱动型服务设置一个处理线程池，设置最大处理线程数量</strong>。这是一种典型的降级策略，目的是为了系统崩溃。但是，<strong>后续的请求会因为没有处理线程而被迫阻塞，最终可能产生信息丢失</strong>。例如：对于广告计费采集，如果采集系统因为线程耗尽而不接收客户端的计费行为，这些计费行为就会丢失。</li>
</ul>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;缓解这类问题的思路来自于<code>CAP</code>理论，即通过降低一致性来提高可用性。<strong>生产者接收线程在收到请求之后第一时间不去处理，直接将请求缓存在内存中（牺牲一致性），而在后台启动多个处理线程从缓存中读取请求、进行处理并写入分布式队列</strong>。与线程所占用的内存开销相比，大部分的请求所占内存几乎可以忽略。通过在接收请求和处理请求之间增加一层内存缓存，可以大大提高系统的处理吞吐量和可扩展性。这个方案本质上是一个<strong>内存生产者消费者模型</strong>。</p>
<h2 id="批量写入优化"><a href="#批量写入优化" class="headerlink" title="批量写入优化"></a>批量写入优化</h2><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;如果生产者的请求过大，写分布式队列可能成为性能瓶颈，有如下几个因素：</p>
<ul>
<li><strong>队列自身性能不高</strong>。</li>
<li>分布式队列编程模型往往被应用在跨机房的系统里面，<strong>跨机房的网络开销</strong>往往容易成为系统瓶颈。</li>
<li><strong>消息确认机制往往会大大降低队列的吞吐量以及响应时间</strong>。</li>
</ul>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;如果<strong>在处理请求和写队列之间添加一层缓存，消息写入程序批量将消息写入队列</strong>，可以大大提高系统的吞吐量。原因如下：</p>
<ul>
<li><strong>批量写队列可以大大减少生产者和分布式队列的交互次数和消息传输量</strong>。特别是对于高吞吐小载荷的消息实体，批量写可以显著降低网络传输量。</li>
<li><strong>对于需要确认机制的消息，确认机制往往会大大降低队列的吞吐量以及响应时间，某些高敏感的消息需要多个消息中间件代理同时确认，这近一步恶化性能</strong>。在生产者的应用层将多条消息批量组合成一个消息体，消息中间件就只需要对批量消息进行一次确认，这可能会数量级的提高消息传输性能。</li>
</ul>
<h2 id="持久化优化"><a href="#持久化优化" class="headerlink" title="持久化优化"></a><strong>持久化优化</strong></h2><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;通过添加缓存，消费者服务的吞吐量和可用性都得到了提升。但缓存引入了一个新问题——<strong>内存数据丢失</strong>。对于敏感数据，工程师需要考虑如下两个潜在问题：</p>
<ul>
<li>如果内存中存在未处理完的请求，而某些原因导致<strong>生产者服务宕机，内存数据就会丢失而可能无法恢复</strong>。</li>
<li><strong>如果分布式队列长时间不可用，随着请求数量的不断增加，最终系统内存可能会耗尽而崩溃，内存的消息也可能丢失</strong>。</li>
</ul>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;所以缓存中的数据需要定期被持久化到磁盘等持久层设备中，典型的持久化触发策略主要有两种：</p>
<ul>
<li><p><strong>定期触发，即每隔一段时间进行一次持久化</strong>。</p>
</li>
<li><p><strong>定量触发，即每当缓存中的请求数量达到一定阈值后进行持久化</strong>。</p>
</li>
</ul>
<p>  是否需要持久化优化，以及<strong>持久化策略应该由请求数据的敏感度、请求量、持久化性能等因素共同决定</strong>。</p>
<h2 id="中间件选型"><a href="#中间件选型" class="headerlink" title="中间件选型"></a>中间件选型</h2><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;分布式队列不等同于各种开源的或者收费的消息中间件，甚至在一些场景下完全不需要使用消息中间件。但是，消息中间件产生的目的就是解决消息传递问题，这为分布式队列编程架构提供了很多的便利。在实际工作中，工程师们应该将成熟的消息中间件作为队列的首要备选方案。</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;本小节对消息中间件的功能、模型进行阐述，并给出一些消息中间件选型、部署的具体建议。</p>
<h3 id="中间件的功能"><a href="#中间件的功能" class="headerlink" title="中间件的功能"></a>中间件的功能</h3><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;明白一个系统的每个具体功能是设计和架构一个系统的基础。典型的消息中间件主要包含如下几个功能：</p>
<ul>
<li><strong>消息接收</strong></li>
<li><strong>消息分发</strong></li>
<li><strong>消息存储</strong></li>
<li><strong>消息读取</strong></li>
</ul>
<h3 id="概念模型"><a href="#概念模型" class="headerlink" title="概念模型"></a>概念模型</h3><p>抽象的消息中间件模型包含如下几个角色：</p>
<ul>
<li><p>发送者和接收者客户端（<code>Sender/Receiver Client</code>），在具体实施过程中，它们一般以库的形式嵌入到应用程序代码中。</p>
</li>
<li><p>代理服务器（<code>Broker Server</code>），它们是与客户端代码直接交互的服务端代码。</p>
</li>
<li><p>消息交换机（<code>Exchanger</code>），接收到的消息一般需要通过消息交换机（<code>Exchanger</code>）分发到具体的消息队列中。</p>
</li>
<li><p>消息队列，一般是一块内存数据结构或持久化数据。</p>
</li>
</ul>
<p>  概念模型如下图：</p>
<p>  <img src="//blog.com/2019/07/15/分布式队列编程优化篇/1.webp" alt="img"></p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;为了提高分发性能，很多消息中间件把消息代理服务器的拓扑图发送到发送者和接收者客户端（<code>Sender/Receiver Client</code>），如此一来，发送源可以直接进行消息分发。</p>
<h3 id="选型标准"><a href="#选型标准" class="headerlink" title="选型标准"></a>选型标准</h3><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;要完整的描述消息中间件各个方面非常困难，大部分良好的消息中间件都有完善的文档，这些文档的长度远远超过本文的总长度。但如下几个标准是工程师们在进行消息中间件选型时经常需要考虑和权衡的。</p>
<h4 id="性能"><a href="#性能" class="headerlink" title="性能"></a>性能</h4><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;性能主要有两个方面需要考虑：<strong>吞吐量（<code>Throughput</code>）和响应时间<code>（Latency</code>）</strong>。</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;不同的消息队列中间件的吞吐量和响应时间相差甚远，在选型时可以去网上查看一些性能对比报告。</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;对于同一种中间件，不同的配置方式也会影响性能。主要有如下几方面的配置：</p>
<ul>
<li><strong>是否需要确认机制</strong>，即写入队列后，或从队列读取后，是否需要进行确认。确认机制对响应时间的影响往往很大。</li>
<li><strong>能否批处理</strong>，即消息能否批量读取或者写入。批量操作可以大大减少应用程序与消息中间件的交互次数和消息传递量，大大提高吞吐量。</li>
<li><strong>能否进行分区</strong>（<code>Partition</code>）。将某一主题消息队列进行分区，同一主题消息可以有多台机器并行处理。这不仅仅能影响消息中间件的吞吐量，还决定着消息中间件是否具备良好的可伸缩性（<code>Scalability</code>）。</li>
<li><strong>是否需要进行持久化</strong>。将消息进行持久化往往会同时影响吞吐量和响应时间。</li>
</ul>
<h4 id="可靠性"><a href="#可靠性" class="headerlink" title="可靠性"></a>可靠性</h4><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;可靠性主要包含：<strong>可用性、持久化、确认机制</strong>等。</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;高可用性的消息中间件应该具备如下特征：</p>
<ul>
<li><p><strong>消息中间件代理服务器（<code>Broker</code>）具有主从备份</strong>。即当一台代理服务宕机之后，备用服务器能接管相关的服务。</p>
</li>
<li><p><strong>消息中间件中缓存的消息是否有备份、并持久化</strong>。</p>
</li>
</ul>
<p>  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;根据<code>CAP</code>理论，高可用、高一致性以及网络分裂不可兼得。根据作者的观察，大部分的消息中间件在面临网络分裂的情况下下，都很难保证数据的一致性以及可用性。 很多消息中间件都会提供一些可配置策略，让使用者在可用性和一致性之间做权衡。</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<strong>高可靠的消息中间件应该确保从发送者接收到的消息不会丢失</strong>。中间件代理服务器的宕机并不是小概率事件，所以保存在内存中的消息很容易发生丢失。大部分的<strong>消息中间件都依赖于消息的持久化去降低消息丢失损失，即将接收到的消息写入磁盘。即使提供持久化</strong>，仍有两个问题需要考虑：</p>
<ul>
<li><p><strong>磁盘损坏问题</strong>。长时间来看，磁盘出问题的概率仍然存在。</p>
</li>
<li><p><strong>性能问题</strong>。与操作内存相比，磁盘<code>I/O</code>的操作性能要慢几个数量级。频繁持久化不仅会增加响应时间，也会降低吞吐量。</p>
</li>
</ul>
<p>  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;解决这两个问题的一个解决方案就是：<strong>多机确认，定期持久化</strong>。即消息被缓存在多台机器的内存中，只有每台机器都确认收到消息，才跟发送者确认（很多消息中间件都会提供相应的配置选项，让用户设置最少需要多少台机器接收到消息）。由于多台独立机器同时出故障的概率遵循乘法法则，指数级降低，这会大大提高消息中间件的可靠性。</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<strong>确认机制本质上是通讯的握手机制（<code>Handshaking</code>）</strong>。如果没有该机制，消息在传输过程中丢失将不会被发现。高敏感的消息要求选取具备确认机制的消息中间件。当然如果没有接收到消息中间件确认完成的指令，应用程序需要决定如何处理。典型的做法有两个：</p>
<ul>
<li><strong>多次重试</strong>。</li>
<li><strong>暂存到本地磁盘或其它持久化媒介</strong>。</li>
</ul>
<h4 id="客户端接口所支持语言"><a href="#客户端接口所支持语言" class="headerlink" title="客户端接口所支持语言"></a>客户端接口所支持语言</h4><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;采用现存消息中间件就意味着避免重复造轮子。如果某个消息中间件未能提供对应语言的客户端接口，则意味着极大的成本和兼容性问题。</p>
<h4 id="投递策略（Delivery-policies）"><a href="#投递策略（Delivery-policies）" class="headerlink" title="投递策略（Delivery policies）"></a>投递策略（Delivery policies）</h4><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;投递策略指的是一个消息会被发送几次。主要包含三种策略：最多一次（<code>At most Once</code> ）、最少一次（<code>At least Once</code>）、仅有一次（<code>Exactly Once</code>）。</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;在实际应用中，<strong>只考虑消息中间件的投递策略并不能保证业务的投递策略，因为接收者在确认收到消息和处理完消息并持久化之间存在一个时间窗口</strong>。例如，即使消息中间件保证仅有一次（<code>Exactly Once</code>），如果接收者先确认消息，在持久化之前宕机，则该消息并未被处理。从应用的角度，这就是最多一次（<code>At most Once</code>）。反之，接收者先处理消息并完成持久化，但在确认之前宕机，消息就要被再次发送，这就是最少一次（<code>At least Once</code>）。 如果消息投递策略非常重要，应用程序自身也需要仔细设计。</p>
<h2 id="消费者优化"><a href="#消费者优化" class="headerlink" title="消费者优化"></a>消费者优化</h2><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;消费者是分布式队列编程中真正的数据处理方，数据处理方最常见的挑战包括：有序性、串行化（<code>Serializability</code>）、频次控制、完整性和一致性等。</p>
<h3 id="挑战"><a href="#挑战" class="headerlink" title="挑战"></a>挑战</h3><h4 id="有序性"><a href="#有序性" class="headerlink" title="有序性"></a>有序性</h4><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;在很多场景下，如何保证队列信息的有序处理是一个棘手的问题。如下图，假定分布式队列保证请求严格有序，请求<code>ri2</code>和<code>ri1</code>都是针对同一数据记录的不同状态，<code>ri2</code>的状态比<code>ri1</code>的状态新。<code>T1</code>、<code>T2</code>、<code>T3</code>和<code>T4</code>代表各个操作发生的时间，并且 <code>T1</code> &lt; <code>T2</code> &lt; <code>T3</code> &lt; <code>T4</code>（”&lt;”代表早于）。</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;采用多消费者架构，这两条记录被两个消费者（<code>Consumer1</code>和<code>Consumer2）</code>处理后更新到数据库里面。<code>Consumer1</code>虽然先读取<code>ri1</code>但是却后写入数据库，这就导致，新的状态被老的状态覆盖，所以<strong>多消费者不保证数据的有序性</strong>。</p>
<p><img src="//blog.com/2019/07/15/分布式队列编程优化篇/2.webp" alt="img"></p>
<h4 id="串行化"><a href="#串行化" class="headerlink" title="串行化"></a>串行化</h4><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;很多场景下，串行化是数据处理的一个基本需求，这是保证数据完整性、可恢复性、事务原子性等的基础。为了在并行计算系统里实现串行化，一系列的相关理论和实践算法被提出。对于分布式队列编程架构，要在在多台消费者实现串行化非常复杂，无异于重复造轮子。</p>
<h4 id="频次控制"><a href="#频次控制" class="headerlink" title="频次控制"></a>频次控制</h4><p>有时候，消费者的消费频次需要被控制，可能的原因包括：</p>
<ul>
<li><strong>费用问题</strong>。如果每次消费所引起的操作都需要收费，而同一个请求消息在队列中保存多份，不进行频次控制，就会导致无谓的浪费。</li>
<li><strong>性能问题</strong>。每次消费可能会引起对其他服务的调用，被调用服务希望对调用量有所控制，对同一个请求消息的多次访问就需要有所控制。</li>
</ul>
<h4 id="完整性和一致性"><a href="#完整性和一致性" class="headerlink" title="完整性和一致性"></a>完整性和一致性</h4><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;完整性和一致性是所有多线程和多进程的代码都面临的问题。在多线程或者多进程的系统中考虑完整性和一致性往往会大大地增加代码的复杂度和系统出错的概率。</p>
<h2 id="单例服务优化"><a href="#单例服务优化" class="headerlink" title="单例服务优化"></a>单例服务优化</h2><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;几乎所有串行化理论真正解决的问题只有一个：性能。 所以，在<strong>性能允许的前提下，对于消费者角色，建议采用单实例部署</strong>。<strong>通过单实例部署，有序性、串行化、完整性和一致性问题自动获得了解决</strong>。另外，单实例部署的消费者拥有全部所需信息，它可以在频次控制上采取很多优化策略。</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;天下没有免费的午餐。同样，单实例部署并非没有代价，它意味着系统可用性的降低，很多时候，这是无法接受的。<strong>解决可用性问题的最直接的思路就是冗余（<code>Redundancy</code>）</strong>。最常用的冗余方案是<code>Master-slave</code>架构，不过大部分的<code>Master-slave</code>架构都是<code>Active/active</code>模式，即主从服务器都提供服务。例如，数据库的<code>Master-slave</code>架构就是主从服务器都提供读服务，只有主服务器提供写服务。大部分基于负载均衡设计的<code>Master-slave</code>集群中，主服务器和从服务器同时提供相同的服务。这显然不满足单例服务优化需求。<strong>有序性和串行化需要<code>Active/passive</code>架构，即在某一时刻只有主实例提供服务，其他的从服务等待主实例失效</strong>。这是<strong>典型的领导人选举架构，即只有获得领导权的实例才能充当实际消费者，其他实例都在等待下一次选举</strong>。采用领导人选举的<code>Active/passive</code>架构可以大大缓解纯粹的单实例部署所带来的可用性问题。</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;令人遗憾的是，除非<strong>工程师们自己在消费者实例里面实现<code>Paxos</code>等算法，并在每次消息处理之前都执行领导人选举</strong>。否则，理论上讲，没有方法可以保障在同一个时刻只有一个领导者。而对每个消息都执行一次领导人选举，显然性能不可行。实际工作中，最容易出现的问题时机发生在领导人交接过程中，即前任领导人实例变成辅助实例，新部署实例开始承担领导人角色。为了平稳过渡，这两者之间需要有一定的通讯机制，但是，无论是网络分区（<code>Network partition</code>）还是原领导人服务崩溃都会使这种通讯机制变的不可能。</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;对于完整性和一致性要求很高的系统，我们需要在选举制度和交接制度这两块进行优化。</p>
<h3 id="领导人选举架构"><a href="#领导人选举架构" class="headerlink" title="领导人选举架构"></a>领导人选举架构</h3><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;典型的领导人选举算法有<code>Paxos</code>、<code>ZAB（ ZooKeeper Atomic Broadcast protocol）</code>。为了避免重复造轮子，建议<strong>采用<code>ZooKeeper</code>的分布式锁来实现领导人选举</strong>。典型的<code>ZooKeeper</code>实现算法如下（摘自参考资料[4]）：</p>
<blockquote>
<p>Let ELECTION be a path of choice of the application. To volunteer to be a leader:</p>
<p>1.Create znode z with path “ELECTION/guid-n_” with both SEQUENCE and EPHEMERAL flags;<br>2.Let C be the children of “ELECTION”, and i be the sequence number of z;<br>3.Watch for changes on “ELECTION/guid-n_j”, where j is the largest sequence number such that j &lt; i and n_j is a znode in C;</p>
<p>Upon receiving a notification of znode deletion:</p>
<p>1.Let C be the new set of children of ELECTION;<br>2.If z is the smallest node in C, then execute leader procedure;<br>3.Otherwise, watch for changes on “ELECTION/guid-n_j”, where j is the largest sequence number such that j &lt; i and n_j is a znode in C;</p>
</blockquote>
<h3 id="领导人交接架构"><a href="#领导人交接架构" class="headerlink" title="领导人交接架构"></a>领导人交接架构</h3><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;领导人选举的整个过程发生在<code>ZooKeeper</code>集群中，各个消费者实例在这场选举中只充当被告知者角色（Learner）。领导人选举算法，只能保证最终只有一个<code>Leader</code>被选举出来，并不保障被告知者对<code>Leader</code>的理解是完全一致的。本质上，上文的架构里，选举的结果是作为令牌（<code>Token</code>）传递给消费者实例，消费者将自身的ID与令牌进行对比，如果相等，则开始执行消费操作。所以当发生领导人换届的情况，不同的<code>Learner</code>获知新<code>Leader</code>的时间并不同。例如，前任<code>Leader</code>如果因为网络问题与<code>ZooKeeper</code>集群断开，前任<code>Leader</code>只能在超时后才能判断自己是否不再承担<code>Leader</code>角色了，而新的<code>Leader</code>可能在这之前已经产生。另一方面，<strong>即使前任<code>Leader</code>和新<code>Leader</code>同时接收到新<code>Leader</code>选举结果，某些业务的完整性要求迫使前任<code>Leader</code>仍然完成当前未完成的工作</strong>。以上的讲解非常抽象，生活中却给了一些更加具体的例子。众所周知，美国总统候选人在选举结束后并不直接担任美国总统，从选举到最终承担总统角色需要一个过渡期。对于新当选<code>Leader</code>的候选人而言，过渡期间称之为加冕阶段<code>（Inauguration）</code>。对于即将卸任的<code>Leader</code>，过渡期称为交接阶段（<code>HandOver</code>）。所以一个基于领导人选举的消费者从加冕到卸任经历三个阶段：<code>Inauguration</code>、<code>Execution</code>、<code>HandOver</code>。在加冕阶段，新领导需要进行一些初始化操作。<code>Execution</code>阶段是真正的队列消息处理阶段。在交接阶段，前任领导需要进行一些清理操作。</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;类似的，为了解决领导人交接问题，所有的消费者从代码实现的角度都需要实现类似<code>ILeaderCareer</code>接口。这个接口包含三个方发<code>inaugurate()</code>，<code>handOver()</code>和<code>execute（）</code>。某个部署实例（<code>Learner</code>）在得知自己承担领导人角色后，需要调用<code>inaugurate()</code>方法，进行加冕。主要的消费逻辑通过不停的执行<code>execute（）</code>实现。当确认自己不再承担领导人之后，执行<code>handOver()</code>进行交接。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">interface</span> <span class="title">ILeaderCareer</span> </span>&#123;    </span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">inaugurate</span><span class="params">()</span></span>;  </span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">handOver</span><span class="params">()</span></span>;    </span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">boolean</span> <span class="title">execute</span><span class="params">()</span></span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;如果承担领导人角色的消费者，在执行<code>execute（）</code>阶段得知自己将要下台，根据消息处理的原子性，该领导人可以决定是否提前终止操作。如果整个消息处理是一个原子性事务，直接终止该操作可以快速实现领导人换届。否则，前任领导必须完成当前消息处理后，才进入交接阶段。这意味着<strong>新的领导人，在<code>inaugurate()</code>阶段需要进行一定时间的等待</strong>。</p>
<h2 id="排重优化"><a href="#排重优化" class="headerlink" title="排重优化"></a>排重优化</h2><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;频次控制是一个经典问题。对于分布式队列编程架构，相同请求重复出现在队列的情况并不少见。<strong>如果相同请求在队列中重复太多，排重优化就显得很必要</strong>。分布式缓存更新是一个典型例子，所有请求都被发送到队列中用于缓存更新。如果请求符合典型的高斯分布，在一段时间内会出现大量重复的请求，而同时多线程更新同一请求缓存显然没有太大的意义。</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;排重优化是一个算法，其本质是基于状态机的编程，整个讲解通过模型、构思和实施三个步骤完成。</p>
<h3 id="模型"><a href="#模型" class="headerlink" title="模型"></a>模型</h3><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;进行排重优化的前提是大量重复的请求。在模型这一小节，我们首先阐述重复度模型、以及不同重复度所导致的消费模型，最后基于这两个模型去讲解排重状态机。</p>
<h4 id="重复度模型"><a href="#重复度模型" class="headerlink" title="重复度模型"></a>重复度模型</h4><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;首先我们给出<strong>最小重复长度</strong>的概念。同一请求最小重复长度：同一请求在队列中的重复出现的最小间距。例如，请求<code>ri</code>第一次出现在位置3，第二次出现在10，最小重复长度等于7。</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;是否需要进行排重优化取决于队列中请求的重复度。由于不同请求之间并不存在重复的问题，不失一般性，这里的模型只考了单个请求的重复度，<strong>重复度分为三个类：无重复、稀疏重复、高重复</strong>。</p>
<p>无重复：在整个请求过程，没有任何一个请求出现一次以上。</p>
<p>稀疏重复：主要的请求最小重复长度大于消费队列长度。</p>
<p>高重复：大量请求最小重复长度小于消费队列长度。</p>
<p>对于不同的重复度，会有不同的消费模型。</p>
<h5 id="无重复消费模型"><a href="#无重复消费模型" class="headerlink" title="无重复消费模型"></a>无重复消费模型</h5><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<strong>在整个队列处理过程中，所有的请求都不相同</strong>，如下图：<img src="//blog.com/2019/07/15/分布式队列编程优化篇/3.webp" alt="img"></p>
<h5 id="稀疏重复消费模型"><a href="#稀疏重复消费模型" class="headerlink" title="稀疏重复消费模型"></a>稀疏重复消费模型</h5><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;当同一请求最小重复长度大于消费者队列长度，如下图。假定有3个消费者，<code>Consumer1</code>将会处理<code>r1</code>，<code>Consumer2</code>将会处理<code>r2</code>，<code>Consumer3</code>将会处理<code>r3</code>，如果每个请求处理的时间严格相等，<code>Consumer1</code>在处理完<code>r1</code>之后，接着处理<code>r4</code>，<code>Consumer2</code>将会处理<code>r2</code>之后会处理<code>r1</code>。<strong>虽然<code>r1</code>被再次处理，但是任何时刻，只有这一个消费者在处理<code>r1</code>，不会出现多个消费者同时处理同一请求的场景</strong>。</p>
<p><img src="//blog.com/2019/07/15/分布式队列编程优化篇/4.webp" alt="img"></p>
<h5 id="高重复消费模型"><a href="#高重复消费模型" class="headerlink" title="高重复消费模型"></a>高重复消费模型</h5><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;如下图，仍然假定有3个消费者，<strong>队列中前面4个请求都是<code>r1</code>，它会同时被3个消费者线程处理</strong>：</p>
<p><img src="//blog.com/2019/07/15/分布式队列编程优化篇/5.webp" alt="img"></p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;显然，对于无重复和稀疏重复的分布式队列，排重优化并不会带来额外的好处。<strong>排重优化所针对的对象是高重复消费模型，特别是对于并行处理消费者比较多的情况，重复处理同一请求，资源消耗极大</strong>。</p>
<h3 id="排重状态机"><a href="#排重状态机" class="headerlink" title="排重状态机"></a>排重状态机</h3><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<strong>排重优化的主要对象是高重复的队列，多个消费者线程或进程同时处理同一个幂等请求只会浪费计算资源并延迟其他待请求处理</strong>。所以，<strong>排重状态机的一个目标是处理唯一性，即：同一时刻，同一个请求只有一个消费者处理</strong>。</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;如果消费者获取一条请求消息，但发现其他消费者正在处理该消息，则当前消费者应该处于等待状态。如果对同一请求，有一个消费者在处理，一个消费者在等待，而同一请求再次被消费者读取，再次等待则没有意义。所以，<strong>状态机的第二个目标是等待唯一性，即：同一时刻，同一个请求最多只有一个消费者处于等待状态</strong>。</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;总上述，<strong>状态机的目标是：处理唯一性和等待唯一性。我们把正在处理的请求称为头部请求，正在等待的请求称为尾部请求</strong>。</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;由于状态机的处理单元是请求，所以<strong>需要针对每一个请求建立一个排重状态机</strong>。基于以上要求，我们设计的排重状态机包含4个状态<code>Init</code>，<code>Process</code>，<code>Block</code>，<code>Decline</code>。各个状态之间转化过程如下图：</p>
<p><img src="//blog.com/2019/07/15/分布式队列编程优化篇/6.webp" alt="img"></p>
<ol>
<li>状态机创建时处于<code>Init</code>状态。</li>
<li>对<code>Init</code>状态进行<code>Enqueue</code>操作，即接收一个请求，开始处理（称为头部请求），状态机进入<code>Process</code>状态。</li>
<li>状态机处于<code>Process</code>状态，表明当前有消费者正在处理头部请求。此时，如果进行<code>Dequeue</code>操作，即头部请求处理完成，返回<code>Init</code>状态。如果进行<code>Enqueue</code>操作，即另一个消费者准备处理同一个请求，状态机进入<code>Block</code>状态（该请求称为尾部请求）。</li>
<li>状态机处于<code>Block</code>状态，表明头部请求正在处理，尾部请求处于阻塞状态。此时，进行<code>Dequeue</code>操作，即头部请求处理完成，返回<code>Process</code>状态，并且尾部请求变成头部请求，原尾部请求消费者结束阻塞状态，开始处理。进行<code>Enqueue</code>操作，表明一个新的消费者准备处理同一个请求，状态机进入<code>Decline</code>状态。</li>
<li>状态机进入<code>Decline</code>状态，根据等待唯一性目标，处理最新请求的消费者将被抛弃该消息，状态机自动转换回<code>Block</code>状态。</li>
</ol>
<h4 id="构思"><a href="#构思" class="headerlink" title="构思"></a>构思</h4><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;状态机描述的是针对单个请求操作所引起状态变化，排重优化需要解决队列中所有请求的排重问题，需要对所有请求的状态机进行管理。这里只考虑单虚拟机内部对所有请求状态机的管理，对于跨虚拟机的管理可以采用类似的方法。对于多状态机管理主要包含三个方面：一致性问题、完整性问题和请求缓存驱逐问题。</p>
<h5 id="一致性问题"><a href="#一致性问题" class="headerlink" title="一致性问题"></a>一致性问题</h5><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<strong>一致性在这里要求同一请求的不同消费者只会操作一个状态机</strong>。<strong>由于每个请求都产生一个状态机，系统将会包含大量的状态机</strong>。为了兼顾性能和一致性，我们采用<code>ConcurrentHashMap</code>保存所有的状态机。用<code>ConcurrentHashMap</code>而不是对整个状态机队列进行加锁，可以提高并行处理能力，使得系统可以同时操作不同状态机。为了避免处理同一请求的多消费者线程同时对<code>ConcurrentHashMap</code>进行插入所导致状态机不一致问题，我们利用了<code>ConcurrentHashMap</code>的<code>putIfAbsent（）</code>方法。代码方案如下，<code>key2Status</code>用于存储所有的状态机。消费者在处理请求之前，从状态机队列中读取排重状态机<code>TrafficAutomate</code>。如果没有找到，则创建一个新的状态机，并通过<code>putIfAbsent（）</code>方法插入到状态机队列中。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">private ConcurrentHashMap&lt;T, TrafficAutomate&gt; key2Status = new ConcurrentHashMap();</span><br><span class="line">TrafficAutomate trafficAutomate = key2Status.get(key);if(trafficAutomate == null)</span><br><span class="line">&#123;</span><br><span class="line">    trafficAutomate = new TrafficAutomate();</span><br><span class="line">    TrafficAutomate oldAutomate = key2Status.putIfAbsent(key, trafficAutomate);    if(oldAutomate != null)</span><br><span class="line">    &#123;</span><br><span class="line">        trafficAutomate = oldAutomate;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h5 id="完整性问题"><a href="#完整性问题" class="headerlink" title="完整性问题"></a>完整性问题</h5><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;完整性要求保障状态机<code>Init</code>，<code>Process</code>，<code>Block</code>，<code>Decline</code>四种状态正确、状态之间的转换也正确。由于状态机的操作非常轻量级，兼顾完整性和降低代码复杂度，我们对状态机的所有方法进行加锁。</p>
<h5 id="请求缓存驱逐问题（Cache-Eviction）"><a href="#请求缓存驱逐问题（Cache-Eviction）" class="headerlink" title="请求缓存驱逐问题（Cache Eviction）"></a>请求缓存驱逐问题（Cache Eviction）</h5><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;如果不同请求的数量太多，内存永久保存所有请求的状态机的内存开销太大。所以，某些状态机需要在恰当的时候被驱逐出内存。这里有两个思路：</p>
<ul>
<li>当状态机返回<code>Init</code>状态时，清除出队列。</li>
<li>启动一个后台线程，定时扫描状态机队列，采用<code>LRU</code>等标准缓存清除机制。</li>
</ul>
<h5 id="标识问题"><a href="#标识问题" class="headerlink" title="标识问题"></a>标识问题</h5><p>每个请求对应于一个状态机，不同的状态机采用不同的请求进行识别。<br>对于同一状态机的不同消费者，在单虚拟机方案中，我们采用线程id进行标识。</p>
<h4 id="实施"><a href="#实施" class="headerlink" title="实施"></a>实施</h4><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;排重优化的主要功能都是通过排重状态机（<code>TrafficAutomate</code>）和状态机队列（<code>QueueCoordinator</code>）来实施的。排重状态机描述的是针对单个请求的排重问题，状态机队列解决所有请求状态机的排重问题。</p>
<h5 id="状态机实施（TrafficAutomate）"><a href="#状态机实施（TrafficAutomate）" class="headerlink" title="状态机实施（TrafficAutomate）"></a>状态机实施（TrafficAutomate）</h5><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;根据状态机模型，其主要操作为<code>enQueue</code>和<code>deQueue</code>，其状态由头部请求和尾部请求的状态共同决定，所以需要定义两个变量为<code>head</code>和<code>tail</code>，用于表示头部请求和尾部请求。为了确保多线程操作下状态机的完整性（<code>Integraty</code>），所有的操作都将加上锁。</p>
<h6 id="enQueue操作"><a href="#enQueue操作" class="headerlink" title="enQueue操作"></a>enQueue操作</h6><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;当一个消费者执行<code>enQueue</code>操作时：如果此时尾部请求不为空，根据等待唯一性要求，返回<code>DECLINE</code>，当前消费者应该抛弃该请求；如果头部请求为空，返回<code>ACCPET</code>，当前消费者应该立刻处理该消息；否则，返回<code>BLOCK</code>，该消费者应该等待，并不停的查看状态机的状态，一直到头部请求处理完成。<code>enQueue</code>代码如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">synchronized ActionEnum enQueue(long id)&#123; </span><br><span class="line">    if(tail != INIT_QUEUE_ID)</span><br><span class="line">    &#123;        return DECLINE;</span><br><span class="line">    &#125;    if(head == INIT_QUEUE_ID)</span><br><span class="line">    &#123;</span><br><span class="line">        head = id;        return ACCEPT;</span><br><span class="line">    &#125;    else</span><br><span class="line">    &#123;</span><br><span class="line">        tail = id;        return BLOCK;</span><br><span class="line">    &#125;</span><br><span class="line">｝</span><br></pre></td></tr></table></figure>
<h6 id="deQueue操作"><a href="#deQueue操作" class="headerlink" title="deQueue操作"></a>deQueue操作</h6><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;对于<code>deQueue</code>操作，首先将尾部请求赋值给头部请求，并将尾部请求置为无效。<code>deQueue</code>代码如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">synchronized boolean deQueue(long id)&#123;</span><br><span class="line">        head = tail;</span><br><span class="line">        tail = INIT_QUEUE_ID;        return true;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h5 id="状态机队列实施-QueueCoordinator"><a href="#状态机队列实施-QueueCoordinator" class="headerlink" title="状态机队列实施(QueueCoordinator)"></a>状态机队列实施(QueueCoordinator)</h5><h6 id="接口定义"><a href="#接口定义" class="headerlink" title="接口定义"></a>接口定义</h6><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;状态机队列集中管理所有请求的排重状态机，所以其操作和单个状态机一样，即<code>enQueue</code>和<code>deQueuqe</code>接口。这两个接口的实现需要识别特定请求的状态机，所以它们的入参应该是请求。为了兼容不同类型的请求消息，我们采用了<code>Java</code>泛型编程。接口定义如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">public interface QueueCoordinator&lt;T&gt; &#123;    public boolean enQueue(T key);    public void deQueue(T key);</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h6 id="enQueue操作-1"><a href="#enQueue操作-1" class="headerlink" title="enQueue操作"></a>enQueue操作</h6><p><code>enQueue</code>操作过程如下：</p>
<p>首先，根据传入的请求<code>key</code>值，获取状态机， 如果不存在则创建一个新的状态机，并保存在<code>ConcurrentHashMap</code>中。<br>接下来，获取线程id作为该消费者的唯一标识，并对对应状态机进行<code>enQueue</code>操作。</p>
<p>如果状态机返回值为<code>ACCEPT</code>或者<code>DECLINE</code>，返回业务层处理代码，<code>ACCEPT</code>意味着业务层需要处理该消息，<code>DECLINE</code>表示业务层可以抛弃当前消息。如果状态机返回值为<code>Block</code>，则该线程保持等待状态。</p>
<p>异常处理。在某些情况下，头部请求线程可能由于异常，未能对状态机进行<code>deQueue</code>操作（作为组件提供方，不能假定所有的规范被使用方实施）。为了避免处于阻塞状态的消费者无期限地等待，建议对状态机设置安全超时时限。超过了一定时间后，状态机强制清空头部请求，返回到业务层，业务层开始处理该请求。</p>
<p>代码如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">public boolean enQueue(T key) &#123;</span><br><span class="line">    _loggingStastic();</span><br><span class="line"></span><br><span class="line">    TrafficAutomate trafficAutomate = key2Status.get(key);    if(trafficAutomate == null)</span><br><span class="line">    &#123;</span><br><span class="line">        trafficAutomate = new TrafficAutomate();</span><br><span class="line">        TrafficAutomate oldAutomate = key2Status.putIfAbsent(key, trafficAutomate);        if(oldAutomate != null)</span><br><span class="line">        &#123;</span><br><span class="line">            trafficAutomate = oldAutomate;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;    long threadId = Thread.currentThread().getId();</span><br><span class="line"></span><br><span class="line">    ActionEnum action = trafficAutomate.enQueue(threadId);    if(action == DECLINE)</span><br><span class="line">    &#123;        return false;</span><br><span class="line">    &#125;    else if (action == ACCEPT)</span><br><span class="line">    &#123;        return true;</span><br><span class="line">    &#125;    //Blocking status means some other thread are working on this key, so just wait till timeout</span><br><span class="line">    long start = System.currentTimeMillis();    long span = 0;</span><br><span class="line">    do &#123;</span><br><span class="line">        _nonExceptionSleep(NAP_TIME_IN_MILL);        if(trafficAutomate.isHead(threadId))</span><br><span class="line">        &#123;            return true;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        span = System.currentTimeMillis() - start;</span><br><span class="line">    &#125;while(span &lt;= timeout);    //remove head so that it won&apos;t block the queue for too long</span><br><span class="line">    trafficAutomate.evictHeadByForce(threadId);    return true;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h6 id="deQueue操作-1"><a href="#deQueue操作-1" class="headerlink" title="deQueue操作"></a>deQueue操作</h6><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<code>deQueue</code>操作首先从<code>ConcurrentHashMap</code>获取改请求所对应的状态机，接着获取该线程的线程id，对状态机进行<code>deQueue</code>操作。</p>
<p><code>enQueue</code>代码如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">public void deQueue(T key) &#123;</span><br><span class="line">    TrafficAutomate trafficAutomate = key2Status.get(key);    if(trafficAutomate == null)</span><br><span class="line">    &#123;</span><br><span class="line">        logger.error(&quot;key &#123;&#125; doesn&apos;t exist &quot;, key);        return;</span><br><span class="line">    &#125;    long threadId = Thread.currentThread().getId();</span><br><span class="line"></span><br><span class="line">    trafficAutomate.deQueue(threadId)；</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://blog.com/2019/07/13/Redis高可用-主从复制/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="刘泽明">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="搬运工 + 践行者">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/07/13/Redis高可用-主从复制/" itemprop="url">Redis高可用-主从复制</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-07-13T12:12:57+08:00">
                2019-07-13
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing">
                  <a href="/categories/Redis/" itemprop="url" rel="index">
                    <span itemprop="name">Redis</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="Redis高可用-主从复制"><a href="#Redis高可用-主从复制" class="headerlink" title="Redis高可用-主从复制"></a>Redis高可用-主从复制</h1><p><br></p>
<blockquote>
<p>原文地址：<a href="https://www.cnblogs.com/kismetv/p/9236731.html" target="_blank" rel="noopener">https://www.cnblogs.com/kismetv/p/9236731.html</a></p>
</blockquote>
<p><br></p>
<h2 id="一、主从复制概述"><a href="#一、主从复制概述" class="headerlink" title="一、主从复制概述"></a>一、主从复制概述</h2><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;主从复制，是指将一台<code>Redis</code>服务器的数据，复制到其他的<code>Redis</code>服务器。前者称为主节点(<code>master</code>)，后者称为从节点(<code>slave</code>)；数据的复制是单向的，只能由主节点到从节点。</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;默认情况下，每台<code>Redis</code>服务器都是主节点；且一个主节点可以有多个从节点(或没有从节点)，但一个从节点只能有一个主节点。</p>
<h3 id="主从复制的作用"><a href="#主从复制的作用" class="headerlink" title="主从复制的作用"></a>主从复制的作用</h3><ol>
<li><strong>数据冗余</strong>：主从复制实现了数据的热备份，是持久化之外的一种数据冗余方式。</li>
<li><strong>故障恢复</strong>：当主节点出现问题时，可以由从节点提供服务，实现快速的故障恢复；实际上是一种服务的冗余。</li>
<li><strong>负载均衡</strong>：在主从复制的基础上，配合读写分离，可以由主节点提供写服务，由从节点提供读服务（即写<code>Redis</code>数据时应用连接主节点，读<code>Redis</code>数据时应用连接从节点），分担服务器负载；尤其是在写少读多的场景下，通过多个从节点分担读负载，可以大大提高<code>Redis</code>服务器的并发量。</li>
<li><strong>高可用基石</strong>：除了上述作用以外，主从复制还是哨兵和集群能够实施的基础，因此说主从复制是<code>Redis</code>高可用的基础。</li>
</ol>
<h3 id="主从复制存在的问题"><a href="#主从复制存在的问题" class="headerlink" title="主从复制存在的问题"></a>主从复制存在的问题</h3><ol>
<li><strong>故障切换</strong>：一旦主节点出现故障，需要手动将一个从节点晋升为主节点，同时需要修改应用方的主节点地址，还需要命令其它从节点去复制新的主节点，这个过程都需要人工干预。</li>
<li>主节点的<strong>写能力</strong>受到单机的限制。</li>
<li>主节点的<strong>存储能力</strong>受到单机的限制。</li>
</ol>
<h2 id="二、如何使用主从复制"><a href="#二、如何使用主从复制" class="headerlink" title="二、如何使用主从复制"></a>二、如何使用主从复制</h2><h3 id="1-建立复制"><a href="#1-建立复制" class="headerlink" title="1. 建立复制"></a>1. 建立复制</h3><p>需要注意，<strong>主从复制的开启，完全是在从节点发起的；不需要我们在主节点做任何事情。</strong></p>
<p>从节点开启主从复制，有3种方式：</p>
<p>（1）配置文件</p>
<p>在从服务器的配置文件中加入：<code>slaveof &lt;masterip&gt; &lt;masterport&gt;</code></p>
<p>（2）启动命令</p>
<p><code>redis-server</code>启动命令后加入 <code>--slaveof &lt;masterip&gt; &lt;masterport&gt;</code></p>
<p>（3）客户端命令</p>
<p><code>Redis</code>服务器启动后，直接通过客户端执行命令：<code>slaveof &lt;masterip&gt; &lt;masterport&gt;</code>，则该<code>Redis</code>实例成为从节点。</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;上述3种方式是等效的，下面以客户端命令的方式为例，看一下当执行了<code>slave of</code>后，<code>Redis</code>主节点和从节点的变化。</p>
<h3 id="2-实例"><a href="#2-实例" class="headerlink" title="2. 实例"></a>2. 实例</h3><h4 id="准备工作：启动两个节点"><a href="#准备工作：启动两个节点" class="headerlink" title="准备工作：启动两个节点"></a>准备工作：启动两个节点</h4><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;方便起见，实验所使用的主从节点是在一台机器上的不同<code>Redis</code>实例，其中主节点监听<code>6379</code>端口，从节点监听<code>6380</code>端口；从节点监听的端口号可以在配置文件中修改：</p>
<p><img src="//blog.com/2019/07/13/Redis高可用-主从复制/1174710-20180628011219164-269585034.png" alt="img"></p>
<p>启动后可以看到：</p>
<p><img src="//blog.com/2019/07/13/Redis高可用-主从复制/1174710-20180628011229205-1259881734.png" alt="img"></p>
<p>两个<code>Redis</code>节点启动后（分别称为6379节点和6380节点），默认都是主节点。</p>
<h4 id="建立复制"><a href="#建立复制" class="headerlink" title="建立复制"></a>建立复制</h4><p>此时在6380节点执行<code>slaveof</code>命令，使之变为从节点：</p>
<p><img src="//blog.com/2019/07/13/Redis高可用-主从复制/1174710-20180628011236215-1776355426.png" alt="img"></p>
<h4 id="观察效果"><a href="#观察效果" class="headerlink" title="观察效果"></a>观察效果</h4><p>下面验证一下，在主从复制建立后，主节点的数据会复制到从节点中。</p>
<p>（1）首先在从节点查询一个不存在的<code>key</code>：</p>
<p><img src="//blog.com/2019/07/13/Redis高可用-主从复制/1174710-20180628011248863-775666084.png" alt="img"></p>
<p>（2）然后在主节点中增加这个<code>key</code>：</p>
<p><img src="//blog.com/2019/07/13/Redis高可用-主从复制/1174710-20180628011255637-1820178900.png" alt="img"></p>
<p>（3）此时在从节点中再次查询这个<code>key</code>，会发现主节点的操作已经同步至从节点：</p>
<p><img src="//blog.com/2019/07/13/Redis高可用-主从复制/1174710-20180628011302404-417931505.png" alt="img"></p>
<p>（4）然后在主节点删除这个<code>key</code>：</p>
<p><img src="//blog.com/2019/07/13/Redis高可用-主从复制/1174710-20180628011309069-1397319420.png" alt="img"></p>
<p>（5）此时在从节点中再次查询这个<code>key</code>，会发现主节点的操作已经同步至从节点：</p>
<p><img src="//blog.com/2019/07/13/Redis高可用-主从复制/1174710-20180628011318424-343599365.png" alt="img"></p>
<h3 id="3-断开复制"><a href="#3-断开复制" class="headerlink" title="3. 断开复制"></a>3. 断开复制</h3><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;通过<code>slaveof &lt;masterip&gt; &lt;masterport&gt;</code>命令建立主从复制关系以后，可以通过<code>slaveof no one</code>断开。需要注意的是，<strong>从节点断开复制后，不会删除已有的数据，只是不再接受主节点新的数据变化</strong>。</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;从节点执行<code>slaveof no one</code>后，打印日志如下所示；可以看出断开复制后，从节点又变回为主节点。</p>
<p><img src="//blog.com/2019/07/13/Redis高可用-主从复制/1174710-20180628011330683-644091604.png" alt="img"></p>
<p>主节点打印日志如下：</p>
<p><img src="//blog.com/2019/07/13/Redis高可用-主从复制/1174710-20180628011337649-815413808.png" alt="img"></p>
<h2 id="三、主从复制的实现原理"><a href="#三、主从复制的实现原理" class="headerlink" title="三、主从复制的实现原理"></a>三、主从复制的实现原理</h2><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;主从复制过程大体可以分为3个阶段：<strong>连接建立阶段（即准备阶段）、数据同步阶段、命令传播阶段</strong>；下面分别进行介绍。</p>
<h3 id="1-连接建立阶段"><a href="#1-连接建立阶段" class="headerlink" title="1. 连接建立阶段"></a>1. 连接建立阶段</h3><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;该阶段的主要作用是在主从节点之间建立连接，为数据同步做好准备。</p>
<h4 id="步骤1：保存主节点信息"><a href="#步骤1：保存主节点信息" class="headerlink" title="步骤1：保存主节点信息"></a>步骤1：保存主节点信息</h4><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;从节点服务器内部维护了两个字段，即<code>master_host</code>和<code>master_port</code>字段，用于存储主节点的<code>ip</code>和<code>port</code>信息。</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;需要注意的是，<code>slaveof</code>是<strong>异步命令</strong>，从节点完成主节点<code>ip</code>和<code>port</code>的保存后，向发送<code>slaveof</code>命令的客户端直接返回<code>OK</code>，实际的复制操作在这之后才开始进行。</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;这个过程中，可以看到从节点打印日志如下：</p>
<p><img src="//blog.com/2019/07/13/Redis高可用-主从复制/1174710-20180628011353199-43354413.png" alt="img"></p>
<h4 id="步骤2：建立socket连接"><a href="#步骤2：建立socket连接" class="headerlink" title="步骤2：建立socket连接"></a>步骤2：建立socket连接</h4><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;从节点每秒1次调用复制定时函数<code>replicationCron()</code>，如果发现了有主节点可以连接，便会根据主节点的<code>ip</code>和port，创建<code>socket</code>连接。如果连接成功，则：</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;从节点：为该<code>socket</code>建立一个专门处理复制工作的文件事件处理器，负责后续的复制工作，如接收<code>RDB</code>文件、接收命令传播等。</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;主节点：接收到从节点的<code>socket</code>连接后（即<code>accept</code>之后），为该<code>socket</code>创建相应的客户端状态，<strong>并将从节点看做是连接到主节点的一个客户端，后面的步骤会以从节点向主节点发送命令请求的形式来进行。</strong></p>
<p>这个过程中，从节点打印日志如下：</p>
<p><img src="//blog.com/2019/07/13/Redis高可用-主从复制/1174710-20180628011400533-833761023.png" alt="img"></p>
<h4 id="步骤3：发送ping命令"><a href="#步骤3：发送ping命令" class="headerlink" title="步骤3：发送ping命令"></a>步骤3：发送ping命令</h4><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;从节点成为主节点的客户端之后，发送<code>ping</code>命令进行首次请求，目的是：检查<code>socket</code>连接是否可用，以及主节点当前是否能够处理请求。</p>
<p>从节点发送<code>ping</code>命令后，可能出现3种情况：</p>
<p>（1）返回<code>pong</code>：说明<code>socket</code>连接正常，且主节点当前可以处理请求，复制过程继续。</p>
<p>（2）超时：一定时间后从节点仍未收到主节点的回复，说明<code>socket</code>连接不可用，则从节点断开<code>socket</code>连接，并重连。</p>
<p>（3）返回<code>pong</code>以外的结果：如果主节点返回其他结果，如正在处理超时运行的脚本，说明主节点当前无法处理命令，则从节点断开<code>socket</code>连接，并重连。</p>
<p>在主节点返回<code>pong</code>情况下，从节点打印日志如下：</p>
<p><img src="//blog.com/2019/07/13/Redis高可用-主从复制/1174710-20180628011408822-1702643515.png" alt="img"></p>
<h4 id="步骤4：身份验证"><a href="#步骤4：身份验证" class="headerlink" title="步骤4：身份验证"></a>步骤4：身份验证</h4><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;如果从节点中设置了<code>masterauth</code>选项，则从节点需要向主节点进行身份验证；没有设置该选项，则不需要验证。从节点进行身份验证是通过向主节点发送<code>auth</code>命令进行的，<code>auth</code>命令的参数即为配置文件中的<code>masterauth</code>的值。</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;如果主节点设置密码的状态，与从节点<code>masterauth</code>的状态一致（一致是指都存在，且密码相同，或者都不存在），则身份验证通过，复制过程继续；如果不一致，则从节点断开<code>socket</code>连接，并重连。</p>
<h4 id="步骤5：发送从节点端口信息"><a href="#步骤5：发送从节点端口信息" class="headerlink" title="步骤5：发送从节点端口信息"></a>步骤5：发送从节点端口信息</h4><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;身份验证之后，从节点会向主节点发送其监听的端口号（前述例子中为6380），主节点将该信息保存到该从节点对应的客户端的<code>slave_listening_port</code>字段中；该端口信息除了在主节点中执行<code>info Replication</code>时显示以外，没有其他作用。</p>
<h3 id="2-数据同步阶段"><a href="#2-数据同步阶段" class="headerlink" title="2. 数据同步阶段"></a>2. 数据同步阶段</h3><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;主从节点之间的连接建立以后，便可以开始进行数据同步，该阶段可以理解为从节点数据的初始化。</p>
<p>具体执行的方式是：从节点向主节点发送<code>psync</code>命令（<code>Redis2.8</code>以前是<code>sync</code>命令），开始同步。</p>
<p>数据同步阶段是主从复制最核心的阶段，根据主从节点当前状态的不同，可以分为全量复制和部分复制，下面会有一章专门讲解这两种复制方式以及<code>psync</code>命令的执行过程，这里不再详述。</p>
<p>需要注意的是，在数据同步阶段之前，从节点是主节点的客户端，主节点不是从节点的客户端；而到了这一阶段及以后，主从节点互为客户端。</p>
<p>原因在于：在此之前，主节点只需要响应从节点的请求即可，不需要主动发请求，而在数据同步阶段和后面的命令传播阶段，主节点需要主动向从节点发送请求（如推送缓冲区中的写命令），才能完成复制。</p>
<h3 id="3-命令传播阶段"><a href="#3-命令传播阶段" class="headerlink" title="3. 命令传播阶段"></a>3. 命令传播阶段</h3><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;数据同步阶段完成后，主从节点进入命令传播阶段；在这个阶段<strong>主节点将自己执行的写命令发送给从节点，从节点接收命令并执行，从而保证主从节点数据的一致性</strong>。</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;在命令传播阶段，除了发送写命令，<strong>主从节点还维持着心跳机制</strong>：<code>PING</code>和<code>REPLCONF ACK</code>。</p>
<p><strong>延迟与不一致</strong></p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;需要注意的是，<strong>命令传播是异步的过程，即主节点发送写命令后并不会等待从节点的回复</strong>；因此实际上主从节点之间很难保持实时的一致性，延迟在所难免。数据不一致的程度，与主从节点之间的网络状况、主节点写命令的执行频率、以及主节点中的<code>repl-disable-tcp-nodelay</code>配置等有关。</p>
<blockquote>
<p><code>repl-disable-tcp-nodelay no</code>：该配置作用于命令传播阶段，控制主节点是否禁止与从节点的<code>TCP_NODELAY</code>；默认<code>no</code>，即不禁止<code>TCP_NODELAY</code>。当设置为<code>yes</code>时，<code>TCP</code>会对包进行合并从而减少带宽，但是发送的频率会降低，从节点数据延迟增加，一致性变差；具体发送频率与<code>Linux</code>内核的配置有关，默认配置为<code>40ms</code>。当设置为<code>no</code>时，<code>TCP</code>会立马将主节点的数据发送给从节点，带宽增加但延迟变小。</p>
</blockquote>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;一般来说，只有当应用对<code>Redis</code>数据不一致的容忍度较高，且主从节点之间网络状况不好时，才会设置为<code>yes</code>；多数情况使用默认值<code>no</code>。</p>
<h2 id="四、【数据同步阶段】全量复制和部分复制"><a href="#四、【数据同步阶段】全量复制和部分复制" class="headerlink" title="四、【数据同步阶段】全量复制和部分复制"></a>四、【数据同步阶段】全量复制和部分复制</h2><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;在<code>Redis2.8</code>以前，从节点向主节点发送<code>sync</code>命令请求同步数据，此时的同步方式是全量复制；</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;在<code>Redis2.8</code>及以后，从节点可以发送<code>psync</code>命令请求同步数据，此时根据主从节点当前状态的不同，同步方式可能是全量复制或部分复制。</p>
<ol>
<li>全量复制：用于初次复制或其他无法进行部分复制的情况，将主节点中的所有数据都发送给从节点，是一个非常重型的操作。</li>
<li>部分复制：<strong>用于网络中断等情况后的复制，只将中断期间主节点执行的写命令发送给从节点</strong>，与全量复制相比更加高效。需要注意的是，如果网络中断时间过长，导致主节点没有能够完整地保存中断期间执行的写命令，则无法进行部分复制，仍使用全量复制。</li>
</ol>
<h3 id="1-全量复制"><a href="#1-全量复制" class="headerlink" title="1. 全量复制"></a>1. 全量复制</h3><p><code>Redis</code>通过<code>psync</code>命令进行全量复制的过程如下：</p>
<p>（1）从节点判断无法进行部分复制，向主节点发送全量复制的请求；或从节点发送部分复制的请求，但主节点判断无法进行全量复制；</p>
<p>（2）<strong>主节点收到全量复制的命令后，执行<code>bgsave</code>，在后台生成<code>RDB</code>文件，并使用一个缓冲区（称为复制缓冲区）记录从现在开始执行的所有写命令</strong>；</p>
<p>（3）主节点的<code>bgsave</code>执行完成后，将<code>RDB</code>文件发送给从节点；<strong>从节点首先清除自己的旧数据，然后载入接收的RDB文件</strong>，将数据库状态更新至主节点执行<code>bgsave</code>时的数据库状态；</p>
<p>（4）<strong>主节点将复制缓冲区中的所有写命令发送给从节点，从节点执行这些写命令，将数据库状态更新至主节点的最新状态</strong>；</p>
<p>（5）如果从节点开启了<code>AOF</code>，则会触发<code>bgrewriteaof</code>的执行，从而保证<code>AOF</code>文件更新至主节点的最新状态；</p>
<p>下面是执行全量复制时，主从节点打印的日志；可以看出日志内容与上述步骤是完全对应的。</p>
<p>主节点的打印日志如下：</p>
<p><img src="//blog.com/2019/07/13/Redis高可用-主从复制/1174710-20180628011456793-1960688528.png" alt="img"></p>
<p>从节点打印日志如下图所示：</p>
<p><img src="//blog.com/2019/07/13/Redis高可用-主从复制/1174710-20180628011502796-1951938935.png" alt="img"></p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;其中，有几点需要注意：从节点接收了来自主节点的<code>89260</code>个字节的数据；从节点在载入主节点的数据之前要先将老数据清除；从节点在同步完数据后，调用了<code>bgrewriteaof</code>。</p>
<p>通过全量复制的过程可以看出，全量复制是非常重型的操作：</p>
<p>（1）主节点通过<code>bgsave</code>命令<code>fork</code>子进程进行<code>RDB</code>持久化，该过程是非常消耗<code>CPU</code>、内存(页表复制)、硬盘<code>IO</code>的；</p>
<p>（2）主节点通过网络将<code>RDB</code>文件发送给从节点，对主从节点的带宽都会带来很大的消耗；</p>
<p>（3）从节点清空老数据、载入新<code>RDB</code>文件的过程是阻塞的，无法响应客户端的命令；如果从节点执行<code>bgrewriteaof</code>，也会带来额外的消耗；</p>
<h3 id="2-部分复制"><a href="#2-部分复制" class="headerlink" title="2. 部分复制"></a>2. 部分复制</h3><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;由于全量复制在主节点数据量较大时效率太低，因此<code>Redis2.8</code>开始提供部分复制，用于处理网络中断时的数据同步。</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;部分复制的实现，依赖于三个重要的概念：</p>
<h4 id="（1）复制偏移量"><a href="#（1）复制偏移量" class="headerlink" title="（1）复制偏移量"></a>（1）复制偏移量</h4><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;主节点和从节点分别维护一个复制偏移量（<code>offset</code>），代表的是<strong>主节点向从节点传递的字节数</strong>；主节点每次向从节点传播N个字节数据时，主节点的<code>offset</code>增加N；从节点每次收到主节点传来的N个字节数据时，从节点的<code>offset</code>增加N。</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<strong><code>offset</code>用于判断主从节点的数据库状态是否一致</strong>，如果二者<code>offset</code>相同，则一致；如果<code>offset</code>不同，则不一致，此时可以根据两个<code>offset</code>找出从节点缺少的那部分数据。</p>
<blockquote>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;例如，如果主节点的offset是1000，而从节点的offset是500，那么部分复制就需要将offset为501-1000的数据传递给从节点。而offset为501-1000的数据存储的位置，就是下面要介绍的复制积压缓冲区。</p>
</blockquote>
<h4 id="（2）复制积压缓冲区"><a href="#（2）复制积压缓冲区" class="headerlink" title="（2）复制积压缓冲区"></a>（2）复制积压缓冲区</h4><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<strong>复制积压缓冲区是由主节点维护的、固定长度的、先进先出(FIFO)队列，默认大小1MB；</strong>当主节点开始有从节点时创建，其作用是备份主节点最近发送给从节点的数据。注意，无论主节点有一个还是多个从节点，都只需要一个复制积压缓冲区。</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;在命令传播阶段，主节点除了将写命令发送给从节点，还会发送一份给复制积压缓冲区，作为写命令的备份；除了存储写命令，复制积压缓冲区中还存储了其中的每个字节对应的复制偏移量（<code>offset</code>）。<strong>由于复制积压缓冲区定长且是先进先出，所以它保存的是主节点最近执行的写命令；时间较早的写命令会被挤出缓冲区</strong>。</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;由于该缓冲区长度固定且有限，因此可以备份的写命令也有限，<strong>当主从节点<code>offset</code>的差距过大超过缓冲区长度时，将无法执行部分复制，只能执行全量复制</strong>。反过来说，为了提高网络中断时部分复制执行的概率，可以根据需要增大复制积压缓冲区的大小(通过配置<code>repl-backlog-size</code>)；例如如果网络中断的平均时间是<code>60s</code>，而主节点平均每秒产生的写命令(特定协议格式)所占的字节数为<code>100KB</code>，则复制积压缓冲区的平均需求为<code>6MB</code>，保险起见，可以设置为<code>12MB</code>，来保证绝大多数断线情况都可以使用部分复制。</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;从节点将<code>offset</code>发送给主节点后，主节点根据<code>offset</code>和缓冲区大小决定能否执行部分复制：</p>
<ul>
<li><strong>如果offset偏移量之后的数据，仍然都在复制积压缓冲区里，则执行部分复制</strong>；</li>
<li><strong>如果offset偏移量之后的数据已不在复制积压缓冲区中（数据已被挤出），则执行全量复制</strong>；</li>
</ul>
<h4 id="（3）服务器运行ID-runid"><a href="#（3）服务器运行ID-runid" class="headerlink" title="（3）服务器运行ID(runid)"></a>（3）服务器运行ID(runid)</h4><blockquote>
<p>单纯的通过ip和port无法判断主节点是否重启过，如果主节点整体重启替换了数据集（替换RDB/AOF文件），那么对于从节点再基于偏移量来复制数据是不安全的</p>
</blockquote>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;每个<code>Redis</code>节点(无论主从)，<strong>在启动时都会自动生成一个随机ID(每次启动都不一样)，由40个随机的十六进制字符组成</strong>；<code>runid</code>用来唯一识别一个<code>Redis</code>节点。通过<code>info Server</code>命令，可以查看节点的<code>runid</code>：</p>
<p><img src="//blog.com/2019/07/13/Redis高可用-主从复制/1174710-20180628011537662-712436367.png" alt="img"></p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;主从节点初次复制时，主节点将自己的<code>runid</code>发送给从节点，从节点将这个<code>runid</code>保存起来；当断线重连时，从节点会将这个<code>runid</code>发送给主节点；主节点根据<code>runid</code>判断能否进行部分复制：</p>
<ul>
<li><p>如果从节点保存的<code>runid</code>与主节点现在的<code>runid</code>相同，说明主从节点之前同步过，主节点会继续尝试使用部分复制(到底能不能部分复制还要看offset和复制积压缓冲区的情况)；</p>
</li>
<li><p>如果<strong>从节点保存的<code>runid</code>与主节点现在的<code>runid</code>不同</strong>，说明从节点在断线前同步的<code>Redis</code>节点并不是当前的主节点，<strong>只能进行全量复制</strong>。</p>
</li>
</ul>
<h3 id="3-psync命令的执行"><a href="#3-psync命令的执行" class="headerlink" title="3. psync命令的执行"></a>3. psync命令的执行</h3><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;在了解了复制偏移量、复制积压缓冲区、节点运行id之后，本节将介绍psync命令的参数和返回值，从而说明<code>psync</code>命令执行过程中，主从节点是如何确定使用全量复制还是部分复制的。</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<code>psync</code>命令的执行过程可以参见下图：</p>
<p><img src="//blog.com/2019/07/13/Redis高可用-主从复制/1174710-20180628011547892-692403928.png" alt="img"> </p>
<p>（1）首先，从节点根据当前状态，决定如何调用<code>psync</code>命令：</p>
<ul>
<li>如果从节点之前未执行过<code>slaveof</code>或最近执行了<code>slaveof no one</code>，则从节点发送命令为<code>psync ? -1</code>，向主节点请求全量复制；</li>
<li>如果从节点之前执行了<code>slaveof</code>，则发送命令为<code>psync &lt;runid&gt; &lt;offset&gt;</code>，其中<code>runid</code>为上次复制的主节点的<code>runid</code>，<code>offset</code>为上次复制截止时从节点保存的复制偏移量。</li>
</ul>
<p>（2）主节点根据收到的<code>psync</code>命令，及当前服务器状态，决定执行全量复制还是部分复制：</p>
<ul>
<li>如果主节点版本低于<code>Redis2.8</code>，则返回<code>-ERR</code>回复，此时从节点重新发送<code>sync</code>命令执行全量复制；</li>
<li><strong>如果主节点版本够新，且<code>runid</code>与从节点发送的<code>runid</code>相同，且从节点发送的<code>offset</code>之后的数据在复制积压缓冲区中都存在，则回复<code>+CONTINUE</code>，表示将进行部分复制，从节点等待主节点发送其缺少的数据即可</strong>；</li>
<li>如果主节点版本够新，但是<code>runid</code>与从节点发送的<code>runid</code>不同，或从节点发送的<code>offset</code>之后的数据已不在复制积压缓冲区中(在队列中被挤出了)，则回复<code>+FULLRESYNC &lt;runid&gt; &lt;offset&gt;</code>，表示要进行全量复制，其中<code>runid</code>表示主节点当前的<code>runid</code>，<code>offset</code>表示主节点当前的<code>offset</code>，从节点保存这两个值，以备使用。</li>
</ul>
<h3 id="4-部分复制演示"><a href="#4-部分复制演示" class="headerlink" title="4. 部分复制演示"></a>4. 部分复制演示</h3><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;在下面的演示中，网络中断几分钟后恢复，断开连接的主从节点进行了部分复制；为了便于模拟网络中断，本例中的主从节点在局域网中的两台机器上。</p>
<p><strong>网络中断</strong></p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;网络中断一段时间后，主节点和从节点都会发现失去了与对方的连接（关于主从节点对超时的判断机制，后面会有说明）；此后，从节点便开始执行对主节点的重连，由于此时网络还没有恢复，重连失败，从节点会一直尝试重连。</p>
<p>主节点日志如下：</p>
<p><img src="//blog.com/2019/07/13/Redis高可用-主从复制/1174710-20180628011609348-1802555230.png" alt="img"></p>
<p>从节点日志如下：</p>
<p><img src="//blog.com/2019/07/13/Redis高可用-主从复制/1174710-20180628011615849-621207089.png" alt="img"></p>
<p><strong>网络恢复</strong></p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;网络恢复后，从节点连接主节点成功，并请求进行部分复制，主节点接收请求后，二者进行部分复制以同步数据。</p>
<p>主节点日志如下：</p>
<p><img src="//blog.com/2019/07/13/Redis高可用-主从复制/1174710-20180628011623772-1032753515.png" alt="img"></p>
<p>从节点日志如下：</p>
<p><img src="//blog.com/2019/07/13/Redis高可用-主从复制/1174710-20180628011630326-1569443208.png" alt="img"></p>
<h2 id="五、【命令传播阶段】心跳机制"><a href="#五、【命令传播阶段】心跳机制" class="headerlink" title="五、【命令传播阶段】心跳机制"></a>五、【命令传播阶段】心跳机制</h2><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;在命令传播阶段，除了发送写命令，主从节点还维持着心跳机制：<code>PING</code>和<code>REPLCONF ACK</code>。心跳机制对于主从复制的超时判断、数据安全等有作用。</p>
<h3 id="1-主-gt-从：PING"><a href="#1-主-gt-从：PING" class="headerlink" title="1.主-&gt;从：PING"></a>1.主-&gt;从：PING</h3><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;每隔指定的时间，<strong>主节点会向从节点发送PING命令</strong>，这个<code>PING</code>命令的作用，主要是为了<strong>让从节点进行超时判断</strong>。</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<strong><code>PING</code>发送的频率由<code>repl-ping-slave-period</code>参数控制，单位是秒，默认值是<code>10s</code></strong>。</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;关于该<code>PING</code>命令究竟是由主节点发给从节点，还是相反，有一些争议；因为在<code>Redis</code>的官方文档中，对该参数的注释中说明是从节点向主节点发送<code>PING</code>命令，如下图所示：</p>
<p><img src="//blog.com/2019/07/13/Redis高可用-主从复制/1174710-20180628011653835-25800141.png" alt="img"></p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;但是根据该参数的名称(含有<code>ping-slave</code>)，以及代码实现，我认为该<code>PING</code>命令是主节点发给从节点的。相关代码如下：</p>
<p><img src="//blog.com/2019/07/13/Redis高可用-主从复制/1174710-20180628011700171-401488218.png" alt="img"></p>
<h3 id="2-从-gt-主：REPLCONF-ACK"><a href="#2-从-gt-主：REPLCONF-ACK" class="headerlink" title="2. 从-&gt;主：REPLCONF ACK"></a>2. 从-&gt;主：REPLCONF ACK</h3><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;在命令传播阶段，<strong>从节点会向主节点发送REPLCONF ACK命令，频率是每秒1次</strong>；命令格式为：<code>REPLCONF ACK {offset}</code>，其中<code>offset</code>指从节点保存的复制偏移量。<code>REPLCONF ACK</code>命令的作用包括：</p>
<p>（1）<strong>实时监测主从节点网络状态：该命令会被主节点用于复制超时的判断</strong>。此外，在主节点中使用<code>info Replication</code>，可以看到其从节点的状态中的<code>lag</code>值，代表的是主节点上次收到该<code>REPLCONF ACK</code>命令的时间间隔，在正常情况下，该值应该是0或1，如下图所示：</p>
<p><img src="//blog.com/2019/07/13/Redis高可用-主从复制/1174710-20180628011708219-1385546367.png" alt="img"></p>
<p>（2）<strong>检测命令丢失</strong>：从节点发送了自身的<code>offset</code>，主节点会与自己的<code>offset</code>对比，如果从节点数据缺失（如网络丢包），主节点会推送缺失的数据（这里也会利用复制积压缓冲区）。<strong>注意，offset和复制积压缓冲区，不仅可以用于部分复制，也可以用于处理命令丢失等情形</strong>；区别在于前者是在断线重连后进行的，而后者是在主从节点没有断线的情况下进行的。</p>
<p>（3）<strong>辅助保证从节点的数量和延迟</strong>：<code>Redis</code>主节点中使用<code>min-slaves-to-write</code>和<code>min-slaves-max-lag</code>参数，来<strong>保证主节点在不安全的情况下不会执行写命令；所谓不安全，是指从节点数量太少，或延迟过高</strong>。</p>
<blockquote>
<p>例如<code>min-slaves-to-write</code>和<code>min-slaves-max-lag</code>分别是3和10，含义是如果从节点数量小于3个，或所有从节点的延迟值都大于10s，则主节点拒绝执行写命令。而这里从节点延迟值的获取，就是通过主节点接收到<code>REPLCONF ACK</code>命令的时间来判断的，即前面所说的<code>info Replication</code>中的<code>lag</code>值。</p>
</blockquote>
<h2 id="六、应用中的问题"><a href="#六、应用中的问题" class="headerlink" title="六、应用中的问题"></a>六、应用中的问题</h2><h3 id="1-读写分离及其中的问题"><a href="#1-读写分离及其中的问题" class="headerlink" title="1. 读写分离及其中的问题"></a>1. 读写分离及其中的问题</h3><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;在主从复制基础上实现的读写分离，可以实现<code>Redis</code>的读负载均衡：由主节点提供写服务，由一个或多个从节点提供读服务（多个从节点既可以提高数据冗余程度，也可以最大化读负载能力）；在读负载较大的应用场景下，可以大大提高<code>Redis</code>服务器的并发量。下面介绍在使用<code>Redis</code>读写分离时，需要注意的问题。</p>
<h4 id="（1）延迟与不一致问题"><a href="#（1）延迟与不一致问题" class="headerlink" title="（1）延迟与不一致问题"></a>（1）延迟与不一致问题</h4><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;由于主从复制的命令传播是异步的，延迟与数据的不一致不可避免。如果应用对数据不一致的接受程度程度较低，可能的优化措施包括：优化主从节点之间的网络环境（如在同机房部署）；监控主从节点延迟（通过<code>offset</code>）判断，如果从节点延迟过大，通知应用不再通过该从节点读取数据；使用集群同时扩展写负载和读负载等。</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;在命令传播阶段以外的其他情况下，从节点的数据不一致可能更加严重，例如连接在数据同步阶段，或从节点失去与主节点的连接时等。从节点的<code>slave-serve-stale-data</code>参数便与此有关：它控制这种情况下从节点的表现；如果为<code>yes</code>（默认值），则从节点仍能够响应客户端的命令，如果为<code>no</code>，则从节点只能响应<code>info</code>、<code>slaveof</code>等少数命令。该参数的设置与应用对数据一致性的要求有关；如果对数据一致性要求很高，则应设置为<code>no</code>。</p>
<h4 id="（2）数据过期问题"><a href="#（2）数据过期问题" class="headerlink" title="（2）数据过期问题"></a>（2）数据过期问题</h4><p>在单机版<code>Redis</code>中，存在两种删除策略：</p>
<ul>
<li>惰性删除：服务器不会主动删除数据，只有当客户端查询某个数据时，服务器判断该数据是否过期，如果过期则删除。</li>
<li>定期删除：服务器执行定时任务删除过期数据，但是考虑到内存和CPU的折中（删除会释放内存，但是频繁的删除操作对CPU不友好），该删除的频率和执行时间都受到了限制。</li>
</ul>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;在主从复制场景下，为了主从节点的数据一致性，从节点不会主动删除数据，而是由主节点控制从节点中过期数据的删除。由于主节点的惰性删除和定期删除策略，都不能保证主节点及时对过期数据执行删除操作，因此，当客户端通过<code>Redis</code>从节点读取数据时，很容易读取到已经过期的数据。</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<code>Redis 3.2</code>中，从节点在读取数据时，增加了对数据是否过期的判断：如果该数据已过期，则不返回给客户端；将<code>Redis</code>升级到<code>3.2</code>可以解决数据过期问题。</p>
<h4 id="（3）故障切换问题"><a href="#（3）故障切换问题" class="headerlink" title="（3）故障切换问题"></a>（3）故障切换问题</h4><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;在没有使用哨兵的读写分离场景下，应用针对读和写分别连接不同的<code>Redis</code>节点；当主节点或从节点出现问题而发生更改时，需要及时修改应用程序读写<code>Redis</code>数据的连接；连接的切换可以手动进行，或者自己写监控程序进行切换，但前者响应慢、容易出错，后者实现复杂，成本都不算低。</p>
<h4 id="（4）总结"><a href="#（4）总结" class="headerlink" title="（4）总结"></a>（4）总结</h4><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;在使用读写分离之前，可以考虑其他方法增加<code>Redis</code>的读负载能力：如尽量优化主节点（减少慢查询、减少持久化等其他情况带来的阻塞等）提高负载能力；使用<code>Redis</code>集群同时提高读负载能力和写负载能力等。如果使用读写分离，可以使用哨兵，使主从节点的故障切换尽可能自动化，并减少对应用程序的侵入。</p>
<h3 id="2-复制超时问题"><a href="#2-复制超时问题" class="headerlink" title="2. 复制超时问题"></a>2. 复制超时问题</h3><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;主从节点复制超时是导致复制中断的最重要的原因之一。</p>
<h4 id="超时判断意义"><a href="#超时判断意义" class="headerlink" title="超时判断意义"></a>超时判断意义</h4><p>在复制连接建立过程中及之后，主从节点都有机制判断连接是否超时，其意义在于：</p>
<p>（1）如果主节点判断连接超时，其会释放相应从节点的连接，从而<strong>释放各种资源</strong>，否则无效的从节点仍会占用主节点的各种资源（输出缓冲区、带宽、连接等）；</p>
<p>（2）连接超时的判断可以让主节点更准确的知道当前有效从节点的个数，<strong>有助于保证数据安全</strong>（配合前面讲到的<code>min-slaves-to-write</code>等参数）。</p>
<p>（2）如果从节点判断连接超时，则可以及时重新建立连接，<strong>避免与主节点数据长期的不一致</strong>。</p>
<h4 id="判断机制"><a href="#判断机制" class="headerlink" title="判断机制"></a>判断机制</h4><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<strong>主从复制超时判断的核心，在于<code>repl-timeout</code>参数，该参数规定了超时时间的阈值（默认60s），对于主节点和从节点同时有效</strong>；主从节点触发超时的条件分别如下：</p>
<p>（1）主节点：每秒1次调用复制定时函数<code>replicationCron()</code>，在其中判断当前时间距离上次收到各个从节点<code>REPLCONF ACK</code>的时间，是否超过了<code>repl-timeout</code>值，如果超过了则释放相应从节点的连接。</p>
<p>（2）从节点：从节点对超时的判断同样是在复制定时函数中判断，基本逻辑是：</p>
<ul>
<li>如果当前处于连接建立阶段，且距离上次收到主节点的信息的时间已超过<code>repl-timeout</code>，则释放与主节点的连接；</li>
<li>如果当前处于数据同步阶段，且收到主节点的<code>RDB</code>文件的时间超时，则停止数据同步，释放连接；</li>
<li>如果当前处于命令传播阶段，且距离上次收到主节点的<code>PING</code>命令或数据的时间已超过<code>repl-timeout</code>值，则释放与主节点的连接。</li>
</ul>
<p>主从节点判断连接超时的相关源代码如下：</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/* Replication cron function, called 1 time per second. */</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">replicationCron</span><span class="params">(<span class="keyword">void</span>)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">static</span> <span class="keyword">long</span> <span class="keyword">long</span> replication_cron_loops = <span class="number">0</span>;</span><br><span class="line"> </span><br><span class="line">    <span class="comment">/* Non blocking connection timeout? */</span></span><br><span class="line">    <span class="keyword">if</span> (server.masterhost &amp;&amp;</span><br><span class="line">        (server.repl_state == REDIS_REPL_CONNECTING ||</span><br><span class="line">         slaveIsInHandshakeState()) &amp;&amp;</span><br><span class="line">         (time(<span class="literal">NULL</span>)-server.repl_transfer_lastio) &gt; server.repl_timeout)</span><br><span class="line">    &#123;</span><br><span class="line">        redisLog(REDIS_WARNING,<span class="string">"Timeout connecting to the MASTER..."</span>);</span><br><span class="line">        undoConnectWithMaster();</span><br><span class="line">    &#125;</span><br><span class="line"> </span><br><span class="line">    <span class="comment">/* Bulk transfer I/O timeout? */</span></span><br><span class="line">    <span class="keyword">if</span> (server.masterhost &amp;&amp; server.repl_state == REDIS_REPL_TRANSFER &amp;&amp;</span><br><span class="line">        (time(<span class="literal">NULL</span>)-server.repl_transfer_lastio) &gt; server.repl_timeout)</span><br><span class="line">    &#123;</span><br><span class="line">        redisLog(REDIS_WARNING,<span class="string">"Timeout receiving bulk data from MASTER... If the problem persists try to set the 'repl-timeout' parameter in redis.conf to a larger value."</span>);</span><br><span class="line">        replicationAbortSyncTransfer();</span><br><span class="line">    &#125;</span><br><span class="line"> </span><br><span class="line">    <span class="comment">/* Timed out master when we are an already connected slave? */</span></span><br><span class="line">    <span class="keyword">if</span> (server.masterhost &amp;&amp; server.repl_state == REDIS_REPL_CONNECTED &amp;&amp;</span><br><span class="line">        (time(<span class="literal">NULL</span>)-server.master-&gt;lastinteraction) &gt; server.repl_timeout)</span><br><span class="line">    &#123;</span><br><span class="line">        redisLog(REDIS_WARNING,<span class="string">"MASTER timeout: no data nor PING received..."</span>);</span><br><span class="line">        freeClient(server.master);</span><br><span class="line">    &#125;</span><br><span class="line"> </span><br><span class="line">    <span class="comment">//此处省略无关代码……</span></span><br><span class="line"> </span><br><span class="line">    <span class="comment">/* Disconnect timedout slaves. */</span></span><br><span class="line">    <span class="keyword">if</span> (listLength(server.slaves)) &#123;</span><br><span class="line">        listIter li;</span><br><span class="line">        listNode *ln;</span><br><span class="line">        listRewind(server.slaves,&amp;li);</span><br><span class="line">        <span class="keyword">while</span>((ln = listNext(&amp;li))) &#123;</span><br><span class="line">            redisClient *slave = ln-&gt;value;</span><br><span class="line">            <span class="keyword">if</span> (slave-&gt;replstate != REDIS_REPL_ONLINE) <span class="keyword">continue</span>;</span><br><span class="line">            <span class="keyword">if</span> (slave-&gt;flags &amp; REDIS_PRE_PSYNC) <span class="keyword">continue</span>;</span><br><span class="line">            <span class="keyword">if</span> ((server.unixtime - slave-&gt;repl_ack_time) &gt; server.repl_timeout)</span><br><span class="line">            &#123;</span><br><span class="line">                redisLog(REDIS_WARNING, <span class="string">"Disconnecting timedout slave: %s"</span>,</span><br><span class="line">                    replicationGetSlaveName(slave));</span><br><span class="line">                freeClient(slave);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"> </span><br><span class="line">    <span class="comment">//此处省略无关代码……</span></span><br><span class="line"> </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>　　</p>
<h4 id="需要注意的坑"><a href="#需要注意的坑" class="headerlink" title="需要注意的坑"></a>需要注意的坑</h4><p>下面介绍与复制阶段连接超时有关的一些实际问题：</p>
<p>（1）<strong>数据同步阶段</strong>：在主从节点进行全量复制<code>bgsave</code>时，主节点需要首先<code>fork</code>子进程将当前数据保存到<code>RDB</code>文件中，然后再将<code>RDB</code>文件通过网络传输到从节点。如果<code>RDB</code>文件过大，主节点在<code>fork</code>子进程+保存<code>RDB</code>文件时耗时过多，可能会导致从节点长时间收不到数据而触发超时；此时从节点会重连主节点，然后再次全量复制，再次超时，再次重连……这是个悲伤的循环。为了避免这种情况的发生，除了<strong>注意<code>Redis</code>单机数据量不要过大，另一方面就是适当增大<code>repl-timeout</code>值，具体的大小可以根据<code>bgsave</code>耗时来调整</strong>。</p>
<p>（2）<strong>命令传播阶段</strong>：如前所述，在该阶段主节点会向从节点发送<code>PING</code>命令，频率由<code>repl-ping-slave-period</code>控制；该参数应明显小于<code>repl-timeout</code>值(后者至少是前者的几倍)。否则，<strong>如果两个参数相等或接近，网络抖动导致个别<code>PING</code>命令丢失，此时恰巧主节点也没有向从节点发送数据，则从节点很容易判断超时</strong>。</p>
<p>（3）<strong>慢查询导致的阻塞</strong>：如果主节点或从节点执行了一些慢查询（如<code>keys *</code>或者对大数据的<code>hgetall</code>等），导致服务器阻塞；<strong>阻塞期间无法响应复制连接中对方节点的请求，可能导致复制超时</strong>。</p>
<h3 id="3-复制中断问题"><a href="#3-复制中断问题" class="headerlink" title="3. 复制中断问题"></a>3. 复制中断问题</h3><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;主从节点超时是复制中断的原因之一，除此之外，还有其他情况可能导致复制中断，其中最主要的是复制缓冲区溢出问题。</p>
<h4 id="复制缓冲区溢出"><a href="#复制缓冲区溢出" class="headerlink" title="复制缓冲区溢出"></a>复制缓冲区溢出</h4><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;前面曾提到过，在全量复制阶段，主节点会将执行的写命令放到复制缓冲区中，该缓冲区存放的数据包括了以下几个时间段内主节点执行的写命令：<code>bgsave</code>生成<code>RDB</code>文件、<code>RDB</code>文件由主节点发往从节点、从节点清空老数据并载入<code>RDB</code>文件中的数据。当主节点数据量较大，或者主从节点之间网络延迟较大时，可能导致该缓冲区的大小超过了限制，此时主节点会断开与从节点之间的连接；这种情况可能引起全量复制-&gt;复制缓冲区溢出导致连接中断-&gt;重连-&gt;全量复制-&gt;复制缓冲区溢出导致连接中断……的循环。</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;复制缓冲区的大小由<code>client-output-buffer-limit slave {hard limit} {soft limit} {soft seconds}</code>配置，默认值为<code>client-output-buffer-limit slave 256MB 64MB 60</code>，其含义是：如果<code>buffer</code>大于256MB，或者连续60s大于64MB，则主节点会断开与该从节点的连接。该参数是可以通过<code>config set</code>命令动态配置的（即不重启<code>Redis</code>也可以生效）。</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;当复制缓冲区溢出时，主节点打印日志如下所示：</p>
<p><img src="//blog.com/2019/07/13/Redis高可用-主从复制/1174710-20180628012000540-1190099498.png" alt="img"></p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<strong>需要注意的是，复制缓冲区是客户端输出缓冲区的一种，主节点会为每一个从节点分别分配复制缓冲区；而复制积压缓冲区则是一个主节点只有一个，无论它有多少个从节点。</strong></p>
<h3 id="4-各场景下复制的选择及优化技巧"><a href="#4-各场景下复制的选择及优化技巧" class="headerlink" title="4. 各场景下复制的选择及优化技巧"></a>4. 各场景下复制的选择及优化技巧</h3><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;在介绍了<code>Redis</code>复制的种种细节之后，现在我们可以来总结一下，在下面常见的场景中，何时使用部分复制，以及需要注意哪些问题。</p>
<h4 id="（1）第一次建立复制"><a href="#（1）第一次建立复制" class="headerlink" title="（1）第一次建立复制"></a>（1）第一次建立复制</h4><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;此时全量复制不可避免，但仍有几点需要注意：</p>
<ol>
<li><strong>如果主节点的数据量较大，应该尽量避开流量的高峰期，避免造成阻塞</strong>；</li>
<li><strong>如果有多个从节点需要建立对主节点的复制，可以考虑将几个从节点错开，避免主节点带宽占用过大</strong>。</li>
<li><strong>如果从节点过多，也可以调整主从复制的拓扑结构，由一主多从结构变为树状结构</strong>（中间的节点既是其主节点的从节点，也是其从节点的主节点）；但使用树状结构应该谨慎：虽然主节点的直接从节点减少，降低了主节点的负担，但是多层从节点的延迟增大，数据一致性变差；且结构复杂，维护相当困难。</li>
</ol>
<h4 id="（2）主节点重启"><a href="#（2）主节点重启" class="headerlink" title="（2）主节点重启"></a>（2）主节点重启</h4><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;主节点重启可以分为两种情况来讨论，一种是故障导致宕机，另一种则是有计划的重启。</p>
<h5 id="主节点宕机"><a href="#主节点宕机" class="headerlink" title="主节点宕机"></a>主节点宕机</h5><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;主节点宕机重启后，<code>runid</code>会发生变化，因此不能进行部分复制，只能全量复制。</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;实际上<strong>在主节点宕机的情况下，应进行故障转移处理，将其中的一个从节点升级为主节点，其他从节点从新的主节点进行复制；且故障转移应尽量的自动化</strong>。</p>
<h5 id="安全重启：debug-reload"><a href="#安全重启：debug-reload" class="headerlink" title="安全重启：debug reload"></a>安全重启：debug reload</h5><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;在一些场景下，可能希望对主节点进行重启，例如主节点内存碎片率过高，或者希望调整一些只能在启动时调整的参数。如果使用普通的手段重启主节点，会使得<code>runid</code>发生变化，可能导致不必要的全量复制。</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;为了解决这个问题，<code>Redis</code>提供了<code>debug reload</code>的重启方式：<strong>重启后，主节点的runid和offset都不受影响，</strong>避免了全量复制。</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;如下图所示，<code>debug reload</code>重启后<code>runid</code>和<code>offset</code>都未受影响：</p>
<p><img src="//blog.com/2019/07/13/Redis高可用-主从复制/1174710-20180628012018427-1532559550.png" alt="img"></p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;但<code>debug reload</code>是一柄双刃剑：<strong>它会清空当前内存中的数据，重新从RDB文件中加载，这个过程会导致主节点的阻塞，因此也需要谨慎</strong>。</p>
<h4 id="（3）从节点重启"><a href="#（3）从节点重启" class="headerlink" title="（3）从节点重启"></a>（3）从节点重启</h4><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;从节点宕机重启后，其保存的主节点的<code>runid</code>会丢失，因此即使再次执行<code>slaveof</code>，也无法进行部分复制。</p>
<h4 id="（4）网络中断"><a href="#（4）网络中断" class="headerlink" title="（4）网络中断"></a>（4）网络中断</h4><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;如果主从节点之间出现网络问题，造成短时间内网络中断，可以分为多种情况讨论。</p>
<p>第一种情况：网络问题时间极为短暂，只造成了短暂的丢包，主从节点都没有判定超时（未触发<code>repl-timeout</code>）；此时只需要通过<code>REPLCONF ACK</code>来补充丢失的数据即可。</p>
<p>第二种情况：网络问题时间很长，主从节点判断超时（触发了<code>repl-timeout</code>），且丢失的数据过多，超过了复制积压缓冲区所能存储的范围；此时主从节点无法进行部分复制，只能进行全量复制。为了尽可能避免这种情况的发生，应该<strong>根据实际情况适当调整复制积压缓冲区的大小；此外及时发现并修复网络中断，也可以减少全量复制</strong>。</p>
<p>第三种情况：介于前述两种情况之间，主从节点判断超时，且丢失的数据仍然都在复制积压缓冲区中；此时主从节点可以进行部分复制。</p>
<h3 id="5-复制相关的配置"><a href="#5-复制相关的配置" class="headerlink" title="5. 复制相关的配置"></a>5. 复制相关的配置</h3><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;这一节总结一下与复制有关的配置，说明这些配置的作用、起作用的阶段，以及配置方法等；通过了解这些配置，一方面加深对<code>Redis</code>复制的了解，另一方面掌握这些配置的方法，可以优化<code>Redis</code>的使用，少走坑。</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;配置大致可以分为主节点相关配置、从节点相关配置以及与主从节点都有关的配置，下面分别说明。</p>
<h4 id="（1）与主从节点都有关的配置"><a href="#（1）与主从节点都有关的配置" class="headerlink" title="（1）与主从节点都有关的配置"></a>（1）与主从节点都有关的配置</h4><p>首先介绍最特殊的配置，它决定了该节点是主节点还是从节点：</p>
<p>1)   <code>slaveof &lt;masterip&gt; &lt;masterport&gt;</code>：<code>Redis</code>启动时起作用；作用是建立复制关系，开启了该配置的<code>Redis</code>服务器在启动后成为从节点。该注释默认注释掉，即<code>Redis</code>服务器默认都是主节点。</p>
<p>2)   <code>repl-timeout 60</code>：与各个阶段主从节点连接超时判断有关。</p>
<h4 id="（2）主节点相关配置"><a href="#（2）主节点相关配置" class="headerlink" title="（2）主节点相关配置"></a>（2）主节点相关配置</h4><p>1)   <code>repl-diskless-sync no</code>：作用于全量复制阶段，控制主节点是否使用<code>diskless</code>复制（无盘复制）。所谓<code>diskless</code>复制，是指在全量复制时，主节点不再先把数据写入<code>RDB</code>文件，而是直接写入<code>slave</code>的<code>socket</code>中，整个过程中不涉及硬盘；<code>diskless</code>复制在磁盘<code>IO</code>很慢而网速很快时更有优势。需要注意的是，截至<code>Redis3.0</code>，<code>diskless</code>复制处于实验阶段，默认是关闭的。</p>
<p>2)   <code>repl-diskless-sync-delay 5</code>：该配置作用于全量复制阶段，当主节点使用<code>diskless</code>复制时，该配置决定主节点向从节点发送之前停顿的时间，单位是秒；只有当<code>diskless</code>复制打开时有效，默认<code>5s</code>。之所以设置停顿时间，是基于以下两个考虑：</p>
<p>​     (1) 向<code>slave</code>的<code>socket</code>的传输一旦开始，新连接的<code>slave</code>只能等待当前数据传输结束，才能开始新的数据传输 </p>
<p>​     (2)  多个从节点有较大的概率在短时间内建立主从复制</p>
<p>3)   <code>client-output-buffer-limit slave 256MB 64MB 60</code>：与全量复制阶段主节点的缓冲区大小有关。</p>
<p>4)   <code>repl-disable-tcp-nodelay no</code>：与命令传播阶段的延迟有关。</p>
<p>5)   <code>masterauth &lt;master-password&gt;</code>：与连接建立阶段的身份验证有关。</p>
<p>6)   <code>repl-ping-slave-period 10</code>：与命令传播阶段主从节点的超时判断有关。</p>
<p>7)  <code>repl-backlog-size 1mb</code>：复制积压缓冲区的大小。</p>
<p>8)  <code>repl-backlog-ttl 3600</code>：当主节点没有从节点时，复制积压缓冲区保留的时间，这样当断开的从节点重新连进来时，可以进行全量复制；默认3600s。如果设置为0，则永远不会释放复制积压缓冲区。</p>
<p>9) <code>min-slaves-to-write 3与min-slaves-max-lag 10</code>：规定了主节点的最小从节点数目，及对应的最大延迟。</p>
<h4 id="（3）从节点相关配置"><a href="#（3）从节点相关配置" class="headerlink" title="（3）从节点相关配置"></a>（3）从节点相关配置</h4><p>1)  <code>slave-serve-stale-data yes</code>：与从节点数据陈旧时是否响应客户端命令有关。</p>
<p>2)   <code>slave-read-only yes</code>：从节点是否只读；默认是只读的。由于从节点开启写操作容易导致主从节点的数据不一致，因此该配置尽量不要修改。</p>
<h3 id="6-单机内存大小限制"><a href="#6-单机内存大小限制" class="headerlink" title="6. 单机内存大小限制"></a>6. 单机内存大小限制</h3><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;在一文中，讲到了<code>fork</code>操作对<code>Redis</code>单机内存大小的限制。实际上在<code>Redis</code>的使用中，限制单机内存大小的因素非常之多，下面总结一下在主从复制中，单机内存过大可能造成的影响：</p>
<p>（1）<strong>切主</strong>：当主节点宕机时，一种常见的容灾策略是将其中一个从节点提升为主节点，并将其他从节点挂载到新的主节点上，此时这些从节点只能进行全量复制；如果<code>Redis</code>单机内存达到<code>10GB</code>，一个从节点的同步时间在几分钟的级别；如果从节点较多，恢复的速度会更慢。如果系统的读负载很高，而这段时间从节点无法提供服务，会对系统造成很大的压力。</p>
<p>（2）<strong>从库扩容</strong>：如果访问量突然增大，此时希望增加从节点分担读负载，如果数据量过大，从节点同步太慢，难以及时应对访问量的暴增。</p>
<p>（3）<strong>缓冲区溢出</strong>：（1）和（2）都是从节点可以正常同步的情形（虽然慢），但是如果数据量过大，导致全量复制阶段主节点的复制缓冲区溢出，从而导致复制中断，则主从节点的数据同步会全量复制-&gt;复制缓冲区溢出导致复制中断-&gt;重连-&gt;全量复制-&gt;复制缓冲区溢出导致复制中断……的循环。</p>
<p>（4）<strong>超时</strong>：如果数据量过大，全量复制阶段主节点<code>fork</code>+保存<code>RDB</code>文件耗时过大，从节点长时间接收不到数据触发超时，主从节点的数据同步同样可能陷入全量复制-&gt;超时导致复制中断-&gt;重连-&gt;全量复制-&gt;超时导致复制中断……的循环。</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;此外，<strong>主节点单机内存除了绝对量不能太大，其占用主机内存的比例也不应过大</strong>：最好只使用<code>50%-65%</code>的内存，留下<code>30%-45%</code>的内存用于执行<code>bgsave</code>命令和创建复制缓冲区等。</p>
<h3 id="7-info-Replication"><a href="#7-info-Replication" class="headerlink" title="7. info Replication"></a>7. info Replication</h3><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;在<code>Redis</code>客户端通过<code>info Replication</code>可以查看与复制相关的状态，对于了解主从节点的当前状态，以及解决出现的问题都会有帮助。</p>
<p>主节点：</p>
<p><img src="//blog.com/2019/07/13/Redis高可用-主从复制/1174710-20180628012051376-2011129261.png" alt="img"></p>
<p>从节点：</p>
<p><img src="//blog.com/2019/07/13/Redis高可用-主从复制/1174710-20180628012057112-2012438265.png" alt="img"></p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;对于从节点，上半部分展示的是其作为从节点的状态，从<code>connectd_slaves</code>开始，展示的是其作为潜在的主节点的状态。</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<code>info Replication</code>中展示的大部分内容在文章中都已经讲述，这里不再详述。</p>
<h1 id="七、总结"><a href="#七、总结" class="headerlink" title="七、总结"></a>七、总结</h1><p>下面回顾一下本文的主要内容：</p>
<p>1、主从复制的作用：宏观的了解主从复制是为了解决什么样的问题，即数据冗余、故障恢复、读负载均衡等。</p>
<p>2、主从复制的操作：即slaveof命令。</p>
<p>3、主从复制的原理：主从复制包括了连接建立阶段、数据同步阶段、命令传播阶段；其中数据同步阶段，有全量复制和部分复制两种数据同步方式；命令传播阶段，主从节点之间有PING和REPLCONF ACK命令互相进行心跳检测。</p>
<p>4、应用中的问题：包括读写分离的问题（数据不一致问题、数据过期问题、故障切换问题等）、复制超时问题、复制中断问题等，然后总结了主从复制相关的配置，其中repl-timeout、client-output-buffer-limit slave等对解决Redis主从复制中出现的问题可能会有帮助。</p>
<p>主从复制虽然解决或缓解了数据冗余、故障恢复、读负载均衡等问题，但其缺陷仍很明显：故障恢复无法自动化；写操作无法负载均衡；存储能力受到单机的限制；这些问题的解决，需要哨兵和集群的帮助，我将在后面的文章中介绍，欢迎关注。</p>
<h1 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h1><p>《Redis开发与运维》</p>
<p>《Redis设计与实现》</p>
<p>《Redis实战》</p>
<p><a href="http://mdba.cn/2015/03/16/redis复制中断问题-慢查询/" target="_blank" rel="noopener">http://mdba.cn/2015/03/16/redis复制中断问题-慢查询/</a></p>
<p><a href="https://redislabs.com/blog/top-redis-headaches-for-devops-replication-buffer/" target="_blank" rel="noopener">https://redislabs.com/blog/top-redis-headaches-for-devops-replication-buffer/</a></p>
<p><a href="http://mdba.cn/2015/03/17/redis主从复制（2）-replication-buffer与replication-backlog/" target="_blank" rel="noopener">http://mdba.cn/2015/03/17/redis主从复制（2）-replication-buffer与replication-backlog/</a></p>
<p><a href="https://github.com/antirez/redis/issues/918" target="_blank" rel="noopener">https://github.com/antirez/redis/issues/918</a></p>
<p><a href="https://blog.csdn.net/qbw2010/article/details/50496982" target="_blank" rel="noopener">https://blog.csdn.net/qbw2010/article/details/50496982</a></p>
<p><a href="https://mp.weixin.qq.com/s?__biz=MzIxMzEzMjM5NQ==&amp;mid=2651029484&amp;idx=1&amp;sn=5882f4c7c390a0a0e4f6dfd872e203b5&amp;chksm=8c4caae8bb3b23fe77909e307d45a071186f55069e5207602c61383eab573885615c1d835904&amp;mpshare=1&amp;scene=1&amp;srcid=0327SokqtxEY3WojWNDMHLYl#rd" target="_blank" rel="noopener">https://mp.weixin.qq.com/s?__biz=MzIxMzEzMjM5NQ==&amp;mid=2651029484&amp;idx=1&amp;sn=5882f4c7c390a0a0e4f6dfd872e203b5&amp;chksm=8c4caae8bb3b23fe77909e307d45a071186f55069e5207602c61383eab573885615c1d835904&amp;mpshare=1&amp;scene=1&amp;srcid=0327SokqtxEY3WojWNDMHLYl#rd</a></p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
  </section>

  
  <nav class="pagination">
    <a class="extend prev" rel="prev" href="/page/24/"><i class="fa fa-angle-left"></i></a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/24/">24</a><span class="page-number current">25</span><a class="page-number" href="/page/26/">26</a><span class="space">&hellip;</span><a class="page-number" href="/page/147/">147</a><a class="extend next" rel="next" href="/page/26/"><i class="fa fa-angle-right"></i></a>
  </nav>



          </div>
          


          

        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      

      <section class="site-overview-wrap sidebar-panel sidebar-panel-active">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope="" itemtype="http://schema.org/Person">
            
              <p class="site-author-name" itemprop="name">刘泽明</p>
              <p class="site-description motion-element" itemprop="description">做一个懂业务的程序员</p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">731</span>
                  <span class="site-state-item-name">日志</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-categories">
                <a href="/categories/index.html">
                  <span class="site-state-item-count">394</span>
                  <span class="site-state-item-name">分类</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-tags">
                <a href="/tags/index.html">
                  <span class="site-state-item-count">237</span>
                  <span class="site-state-item-name">标签</span>
                </a>
              </div>
            

          </nav>

          
            <div class="feed-link motion-element">
              <a href="/atom.xml" rel="alternate">
                <i class="fa fa-rss"></i>
                RSS
              </a>
            </div>
          

          

          
          

          
          

          

        </div>
      </section>

      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; 2017 &mdash; <span itemprop="copyrightYear">2019</span>
  <span class="with-love">
    <i class="fa fa-[object Object]"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">刘泽明</span>

  
</div>









        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  

  

  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  












  

  <script type="text/javascript">
    // Popup Window;
    var isfetched = false;
    var isXml = true;
    // Search DB path;
    var search_path = "search.xml";
    if (search_path.length === 0) {
      search_path = "search.xml";
    } else if (/json$/i.test(search_path)) {
      isXml = false;
    }
    var path = "/" + search_path;
    // monitor main search box;

    var onPopupClose = function (e) {
      $('.popup').hide();
      $('#local-search-input').val('');
      $('.search-result-list').remove();
      $('#no-result').remove();
      $(".local-search-pop-overlay").remove();
      $('body').css('overflow', '');
    }

    function proceedsearch() {
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay"></div>')
        .css('overflow', 'hidden');
      $('.search-popup-overlay').click(onPopupClose);
      $('.popup').toggle();
      var $localSearchInput = $('#local-search-input');
      $localSearchInput.attr("autocapitalize", "none");
      $localSearchInput.attr("autocorrect", "off");
      $localSearchInput.focus();
    }

    // search function;
    var searchFunc = function(path, search_id, content_id) {
      'use strict';

      // start loading animation
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay">' +
          '<div id="search-loading-icon">' +
          '<i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>' +
          '</div>' +
          '</div>')
        .css('overflow', 'hidden');
      $("#search-loading-icon").css('margin', '20% auto 0 auto').css('text-align', 'center');

      $.ajax({
        url: path,
        dataType: isXml ? "xml" : "json",
        async: true,
        success: function(res) {
          // get the contents from search data
          isfetched = true;
          $('.popup').detach().appendTo('.header-inner');
          var datas = isXml ? $("entry", res).map(function() {
            return {
              title: $("title", this).text(),
              content: $("content",this).text(),
              url: $("url" , this).text()
            };
          }).get() : res;
          var input = document.getElementById(search_id);
          var resultContent = document.getElementById(content_id);
          var inputEventFunction = function() {
            var searchText = input.value.trim().toLowerCase();
            var keywords = searchText.split(/[\s\-]+/);
            if (keywords.length > 1) {
              keywords.push(searchText);
            }
            var resultItems = [];
            if (searchText.length > 0) {
              // perform local searching
              datas.forEach(function(data) {
                var isMatch = false;
                var hitCount = 0;
                var searchTextCount = 0;
                var title = data.title.trim();
                var titleInLowerCase = title.toLowerCase();
                var content = data.content.trim().replace(/<[^>]+>/g,"");
                var contentInLowerCase = content.toLowerCase();
                var articleUrl = decodeURIComponent(data.url);
                var indexOfTitle = [];
                var indexOfContent = [];
                // only match articles with not empty titles
                if(title != '') {
                  keywords.forEach(function(keyword) {
                    function getIndexByWord(word, text, caseSensitive) {
                      var wordLen = word.length;
                      if (wordLen === 0) {
                        return [];
                      }
                      var startPosition = 0, position = [], index = [];
                      if (!caseSensitive) {
                        text = text.toLowerCase();
                        word = word.toLowerCase();
                      }
                      while ((position = text.indexOf(word, startPosition)) > -1) {
                        index.push({position: position, word: word});
                        startPosition = position + wordLen;
                      }
                      return index;
                    }

                    indexOfTitle = indexOfTitle.concat(getIndexByWord(keyword, titleInLowerCase, false));
                    indexOfContent = indexOfContent.concat(getIndexByWord(keyword, contentInLowerCase, false));
                  });
                  if (indexOfTitle.length > 0 || indexOfContent.length > 0) {
                    isMatch = true;
                    hitCount = indexOfTitle.length + indexOfContent.length;
                  }
                }

                // show search results

                if (isMatch) {
                  // sort index by position of keyword

                  [indexOfTitle, indexOfContent].forEach(function (index) {
                    index.sort(function (itemLeft, itemRight) {
                      if (itemRight.position !== itemLeft.position) {
                        return itemRight.position - itemLeft.position;
                      } else {
                        return itemLeft.word.length - itemRight.word.length;
                      }
                    });
                  });

                  // merge hits into slices

                  function mergeIntoSlice(text, start, end, index) {
                    var item = index[index.length - 1];
                    var position = item.position;
                    var word = item.word;
                    var hits = [];
                    var searchTextCountInSlice = 0;
                    while (position + word.length <= end && index.length != 0) {
                      if (word === searchText) {
                        searchTextCountInSlice++;
                      }
                      hits.push({position: position, length: word.length});
                      var wordEnd = position + word.length;

                      // move to next position of hit

                      index.pop();
                      while (index.length != 0) {
                        item = index[index.length - 1];
                        position = item.position;
                        word = item.word;
                        if (wordEnd > position) {
                          index.pop();
                        } else {
                          break;
                        }
                      }
                    }
                    searchTextCount += searchTextCountInSlice;
                    return {
                      hits: hits,
                      start: start,
                      end: end,
                      searchTextCount: searchTextCountInSlice
                    };
                  }

                  var slicesOfTitle = [];
                  if (indexOfTitle.length != 0) {
                    slicesOfTitle.push(mergeIntoSlice(title, 0, title.length, indexOfTitle));
                  }

                  var slicesOfContent = [];
                  while (indexOfContent.length != 0) {
                    var item = indexOfContent[indexOfContent.length - 1];
                    var position = item.position;
                    var word = item.word;
                    // cut out 100 characters
                    var start = position - 20;
                    var end = position + 80;
                    if(start < 0){
                      start = 0;
                    }
                    if (end < position + word.length) {
                      end = position + word.length;
                    }
                    if(end > content.length){
                      end = content.length;
                    }
                    slicesOfContent.push(mergeIntoSlice(content, start, end, indexOfContent));
                  }

                  // sort slices in content by search text's count and hits' count

                  slicesOfContent.sort(function (sliceLeft, sliceRight) {
                    if (sliceLeft.searchTextCount !== sliceRight.searchTextCount) {
                      return sliceRight.searchTextCount - sliceLeft.searchTextCount;
                    } else if (sliceLeft.hits.length !== sliceRight.hits.length) {
                      return sliceRight.hits.length - sliceLeft.hits.length;
                    } else {
                      return sliceLeft.start - sliceRight.start;
                    }
                  });

                  // select top N slices in content

                  var upperBound = parseInt('1');
                  if (upperBound >= 0) {
                    slicesOfContent = slicesOfContent.slice(0, upperBound);
                  }

                  // highlight title and content

                  function highlightKeyword(text, slice) {
                    var result = '';
                    var prevEnd = slice.start;
                    slice.hits.forEach(function (hit) {
                      result += text.substring(prevEnd, hit.position);
                      var end = hit.position + hit.length;
                      result += '<b class="search-keyword">' + text.substring(hit.position, end) + '</b>';
                      prevEnd = end;
                    });
                    result += text.substring(prevEnd, slice.end);
                    return result;
                  }

                  var resultItem = '';

                  if (slicesOfTitle.length != 0) {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + highlightKeyword(title, slicesOfTitle[0]) + "</a>";
                  } else {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + title + "</a>";
                  }

                  slicesOfContent.forEach(function (slice) {
                    resultItem += "<a href='" + articleUrl + "'>" +
                      "<p class=\"search-result\">" + highlightKeyword(content, slice) +
                      "...</p>" + "</a>";
                  });

                  resultItem += "</li>";
                  resultItems.push({
                    item: resultItem,
                    searchTextCount: searchTextCount,
                    hitCount: hitCount,
                    id: resultItems.length
                  });
                }
              })
            };
            if (keywords.length === 1 && keywords[0] === "") {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-search fa-5x" /></div>'
            } else if (resultItems.length === 0) {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-frown-o fa-5x" /></div>'
            } else {
              resultItems.sort(function (resultLeft, resultRight) {
                if (resultLeft.searchTextCount !== resultRight.searchTextCount) {
                  return resultRight.searchTextCount - resultLeft.searchTextCount;
                } else if (resultLeft.hitCount !== resultRight.hitCount) {
                  return resultRight.hitCount - resultLeft.hitCount;
                } else {
                  return resultRight.id - resultLeft.id;
                }
              });
              var searchResultList = '<ul class=\"search-result-list\">';
              resultItems.forEach(function (result) {
                searchResultList += result.item;
              })
              searchResultList += "</ul>";
              resultContent.innerHTML = searchResultList;
            }
          }

          if ('auto' === 'auto') {
            input.addEventListener('input', inputEventFunction);
          } else {
            $('.search-icon').click(inputEventFunction);
            input.addEventListener('keypress', function (event) {
              if (event.keyCode === 13) {
                inputEventFunction();
              }
            });
          }

          // remove loading animation
          $(".local-search-pop-overlay").remove();
          $('body').css('overflow', '');

          proceedsearch();
        }
      });
    }

    // handle and trigger popup window;
    $('.popup-trigger').click(function(e) {
      e.stopPropagation();
      if (isfetched === false) {
        searchFunc(path, 'local-search-input', 'local-search-result');
      } else {
        proceedsearch();
      };
    });

    $('.popup-btn-close').click(onPopupClose);
    $('.popup').click(function(e){
      e.stopPropagation();
    });
    $(document).on('keyup', function (event) {
      var shouldDismissSearchPopup = event.which === 27 &&
        $('.search-popup').is(':visible');
      if (shouldDismissSearchPopup) {
        onPopupClose();
      }
    });
  </script>





  

  

  

  
  

  

  

  

</body>
</html>
